>>> DEBUG: looking for: /opt/conda/lib/python3.10/site-packages/transformers/integrations/tensor_parallel.py
>>> WARNING: tensor_parallel.py not found ‚Äî skipping patch (Transformers version is new).
Flash SDPA: False
MemEff SDPA: True
Math SDPA: False
‚úÖ Loaded runwayml/stable-diffusion-v1-5 from cache
‚ö†Ô∏è UNet2DConditionModel.forward does not appear to be patched
‚ùå Could not check PeftModel: No module named 'peft'
‚ùå Could not check LoraModel: No module named 'peft'
‚ö†Ô∏è Not all classes are patched - training may encounter errors
Warning: Some classes are not patched - training may encounter errors
train_cubediff.py - cfg is {'dataset': '/home/jupyter/mluser/git/cube/data/dataspace/polyhaven_tiny_13_new_pt/cubediff_train.tar', 'val_dataset': '/home/jupyter/mluser/git/cube/data/dataspace/polyhaven_tiny_13_new_pt/cubediff_val.tar', 'batch_size': 2, 'eval_batch_size': 2, 'num_workers': 10, 'gradient_accum_steps': 4, 'learning_rate': 8e-05, 'mixed_precision': 'bf16', 'max_train_steps': 1, 'eval_cycle': 1, 'warmup_ratio': 0.03, 'plateau_ratio': 0, 'use_wandb': False, 'output_dir': 'outputs/cubediff_tiny_lora', 'seed': 1337, 'skip_weight_copy': False}
trainer.py - init - Rank 0: Accelerator state: Distributed environment: MULTI_GPU  Backend: nccl
Num processes: 1
Process index: 0
Local process index: 0
Device: cuda:0

Mixed precision type: bf16

trainer.py - init - Distributed type: MULTI_GPU
trainer.py - init - Using DeepSpeed: False
Rank 0: local_rank=0, device=cuda:0
Moving all components to cuda:0...
Loading VAE‚Ä¶
pipeline.py - CubeDiffPipeline - __init__ - replace_group_norms: replaced 52 GroupNorm layers with SynchronizedGroupNorm.
pipeline.py - CubeDiffPipeline - safe loading U-Net‚Ä¶
pipelien.py - CubeDiffPipeline - Wrapping & inflating U-Net (CubeDiffModel)‚Ä¶
architecture.py - CubeDiffModel - üîç All Attention modules in UNet:
    down_blocks.0.attentions.0.transformer_blocks.0.attn1 (self)
    down_blocks.0.attentions.0.transformer_blocks.0.attn2 (self)
    down_blocks.0.attentions.1.transformer_blocks.0.attn1 (self)
    down_blocks.0.attentions.1.transformer_blocks.0.attn2 (self)
    down_blocks.1.attentions.0.transformer_blocks.0.attn1 (self)
    down_blocks.1.attentions.0.transformer_blocks.0.attn2 (self)
    down_blocks.1.attentions.1.transformer_blocks.0.attn1 (self)
    down_blocks.1.attentions.1.transformer_blocks.0.attn2 (self)
    down_blocks.2.attentions.0.transformer_blocks.0.attn1 (self)
    down_blocks.2.attentions.0.transformer_blocks.0.attn2 (self)
    down_blocks.2.attentions.1.transformer_blocks.0.attn1 (self)
    down_blocks.2.attentions.1.transformer_blocks.0.attn2 (self)
    up_blocks.1.attentions.0.transformer_blocks.0.attn1 (self)
    up_blocks.1.attentions.0.transformer_blocks.0.attn2 (self)
    up_blocks.1.attentions.1.transformer_blocks.0.attn1 (self)
    up_blocks.1.attentions.1.transformer_blocks.0.attn2 (self)
    up_blocks.1.attentions.2.transformer_blocks.0.attn1 (self)
    up_blocks.1.attentions.2.transformer_blocks.0.attn2 (self)
    up_blocks.2.attentions.0.transformer_blocks.0.attn1 (self)
    up_blocks.2.attentions.0.transformer_blocks.0.attn2 (self)
    up_blocks.2.attentions.1.transformer_blocks.0.attn1 (self)
    up_blocks.2.attentions.1.transformer_blocks.0.attn2 (self)
    up_blocks.2.attentions.2.transformer_blocks.0.attn1 (self)
    up_blocks.2.attentions.2.transformer_blocks.0.attn2 (self)
    up_blocks.3.attentions.0.transformer_blocks.0.attn1 (self)
    up_blocks.3.attentions.0.transformer_blocks.0.attn2 (self)
    up_blocks.3.attentions.1.transformer_blocks.0.attn1 (self)
    up_blocks.3.attentions.1.transformer_blocks.0.attn2 (self)
    up_blocks.3.attentions.2.transformer_blocks.0.attn1 (self)
    up_blocks.3.attentions.2.transformer_blocks.0.attn2 (self)
    mid_block.attentions.0.transformer_blocks.0.attn1 (self)
    mid_block.attentions.0.transformer_blocks.0.attn2 (self)
architecture.py - CubeDiffModel - DEBUG: All trainable param names in base_unet:
architecture.py - CubeDiffModel - base_unet name is conv_in.weight
architecture.py - CubeDiffModel - base_unet name is conv_in.bias
architecture.py - CubeDiffModel - base_unet name is time_embedding.linear_1.weight
architecture.py - CubeDiffModel - base_unet name is time_embedding.linear_1.bias
architecture.py - CubeDiffModel - base_unet name is time_embedding.linear_2.weight
architecture.py - CubeDiffModel - base_unet name is time_embedding.linear_2.bias
architecture.py - CubeDiffModel - base_unet name is down_blocks.0.attentions.0.norm.weight
architecture.py - CubeDiffModel - base_unet name is down_blocks.0.attentions.0.norm.bias
architecture.py - CubeDiffModel - base_unet name is down_blocks.0.attentions.0.proj_in.weight
architecture.py - CubeDiffModel - base_unet name is down_blocks.0.attentions.0.proj_in.bias
architecture.py - CubeDiffModel - base_unet name is down_blocks.0.attentions.0.transformer_blocks.0.norm1.weight
architecture.py - CubeDiffModel - base_unet name is down_blocks.0.attentions.0.transformer_blocks.0.norm1.bias
architecture.py - CubeDiffModel - base_unet name is down_blocks.0.attentions.0.transformer_blocks.0.attn1.to_q.weight
architecture.py - CubeDiffModel - base_unet name is down_blocks.0.attentions.0.transformer_blocks.0.attn1.to_k.weight
architecture.py - CubeDiffModel - base_unet name is down_blocks.0.attentions.0.transformer_blocks.0.attn1.to_v.weight
architecture.py - CubeDiffModel - base_unet name is down_blocks.0.attentions.0.transformer_blocks.0.attn1.to_out.0.weight
architecture.py - CubeDiffModel - base_unet name is down_blocks.0.attentions.0.transformer_blocks.0.attn1.to_out.0.bias
architecture.py - CubeDiffModel - base_unet name is down_blocks.0.attentions.0.transformer_blocks.0.norm2.weight
architecture.py - CubeDiffModel - base_unet name is down_blocks.0.attentions.0.transformer_blocks.0.norm2.bias
architecture.py - CubeDiffModel - base_unet name is down_blocks.0.attentions.0.transformer_blocks.0.attn2.to_q.weight
architecture.py - CubeDiffModel - base_unet name is down_blocks.0.attentions.0.transformer_blocks.0.attn2.to_k.weight
architecture.py - CubeDiffModel - base_unet name is down_blocks.0.attentions.0.transformer_blocks.0.attn2.to_v.weight
architecture.py - CubeDiffModel - base_unet name is down_blocks.0.attentions.0.transformer_blocks.0.attn2.to_out.0.weight
architecture.py - CubeDiffModel - base_unet name is down_blocks.0.attentions.0.transformer_blocks.0.attn2.to_out.0.bias
architecture.py - CubeDiffModel - base_unet name is down_blocks.0.attentions.0.transformer_blocks.0.norm3.weight
architecture.py - CubeDiffModel - base_unet name is down_blocks.0.attentions.0.transformer_blocks.0.norm3.bias
architecture.py - CubeDiffModel - base_unet name is down_blocks.0.attentions.0.transformer_blocks.0.ff.net.0.proj.weight
architecture.py - CubeDiffModel - base_unet name is down_blocks.0.attentions.0.transformer_blocks.0.ff.net.0.proj.bias
architecture.py - CubeDiffModel - base_unet name is down_blocks.0.attentions.0.transformer_blocks.0.ff.net.2.weight
architecture.py - CubeDiffModel - base_unet name is down_blocks.0.attentions.0.transformer_blocks.0.ff.net.2.bias
architecture.py - CubeDiffModel - base_unet name is down_blocks.0.attentions.0.proj_out.weight
architecture.py - CubeDiffModel - base_unet name is down_blocks.0.attentions.0.proj_out.bias
architecture.py - CubeDiffModel - base_unet name is down_blocks.0.attentions.1.norm.weight
architecture.py - CubeDiffModel - base_unet name is down_blocks.0.attentions.1.norm.bias
architecture.py - CubeDiffModel - base_unet name is down_blocks.0.attentions.1.proj_in.weight
architecture.py - CubeDiffModel - base_unet name is down_blocks.0.attentions.1.proj_in.bias
architecture.py - CubeDiffModel - base_unet name is down_blocks.0.attentions.1.transformer_blocks.0.norm1.weight
architecture.py - CubeDiffModel - base_unet name is down_blocks.0.attentions.1.transformer_blocks.0.norm1.bias
architecture.py - CubeDiffModel - base_unet name is down_blocks.0.attentions.1.transformer_blocks.0.attn1.to_q.weight
architecture.py - CubeDiffModel - base_unet name is down_blocks.0.attentions.1.transformer_blocks.0.attn1.to_k.weight
architecture.py - CubeDiffModel - base_unet name is down_blocks.0.attentions.1.transformer_blocks.0.attn1.to_v.weight
architecture.py - CubeDiffModel - base_unet name is down_blocks.0.attentions.1.transformer_blocks.0.attn1.to_out.0.weight
architecture.py - CubeDiffModel - base_unet name is down_blocks.0.attentions.1.transformer_blocks.0.attn1.to_out.0.bias
architecture.py - CubeDiffModel - base_unet name is down_blocks.0.attentions.1.transformer_blocks.0.norm2.weight
architecture.py - CubeDiffModel - base_unet name is down_blocks.0.attentions.1.transformer_blocks.0.norm2.bias
architecture.py - CubeDiffModel - base_unet name is down_blocks.0.attentions.1.transformer_blocks.0.attn2.to_q.weight
architecture.py - CubeDiffModel - base_unet name is down_blocks.0.attentions.1.transformer_blocks.0.attn2.to_k.weight
architecture.py - CubeDiffModel - base_unet name is down_blocks.0.attentions.1.transformer_blocks.0.attn2.to_v.weight
architecture.py - CubeDiffModel - base_unet name is down_blocks.0.attentions.1.transformer_blocks.0.attn2.to_out.0.weight
architecture.py - CubeDiffModel - base_unet name is down_blocks.0.attentions.1.transformer_blocks.0.attn2.to_out.0.bias
architecture.py - CubeDiffModel - base_unet name is down_blocks.0.attentions.1.transformer_blocks.0.norm3.weight
architecture.py - CubeDiffModel - base_unet name is down_blocks.0.attentions.1.transformer_blocks.0.norm3.bias
architecture.py - CubeDiffModel - base_unet name is down_blocks.0.attentions.1.transformer_blocks.0.ff.net.0.proj.weight
architecture.py - CubeDiffModel - base_unet name is down_blocks.0.attentions.1.transformer_blocks.0.ff.net.0.proj.bias
architecture.py - CubeDiffModel - base_unet name is down_blocks.0.attentions.1.transformer_blocks.0.ff.net.2.weight
architecture.py - CubeDiffModel - base_unet name is down_blocks.0.attentions.1.transformer_blocks.0.ff.net.2.bias
architecture.py - CubeDiffModel - base_unet name is down_blocks.0.attentions.1.proj_out.weight
architecture.py - CubeDiffModel - base_unet name is down_blocks.0.attentions.1.proj_out.bias
architecture.py - CubeDiffModel - base_unet name is down_blocks.0.resnets.0.norm1.weight
architecture.py - CubeDiffModel - base_unet name is down_blocks.0.resnets.0.norm1.bias
architecture.py - CubeDiffModel - base_unet name is down_blocks.0.resnets.0.conv1.weight
architecture.py - CubeDiffModel - base_unet name is down_blocks.0.resnets.0.conv1.bias
architecture.py - CubeDiffModel - base_unet name is down_blocks.0.resnets.0.time_emb_proj.weight
architecture.py - CubeDiffModel - base_unet name is down_blocks.0.resnets.0.time_emb_proj.bias
architecture.py - CubeDiffModel - base_unet name is down_blocks.0.resnets.0.norm2.weight
architecture.py - CubeDiffModel - base_unet name is down_blocks.0.resnets.0.norm2.bias
architecture.py - CubeDiffModel - base_unet name is down_blocks.0.resnets.0.conv2.weight
architecture.py - CubeDiffModel - base_unet name is down_blocks.0.resnets.0.conv2.bias
architecture.py - CubeDiffModel - base_unet name is down_blocks.0.resnets.1.norm1.weight
architecture.py - CubeDiffModel - base_unet name is down_blocks.0.resnets.1.norm1.bias
architecture.py - CubeDiffModel - base_unet name is down_blocks.0.resnets.1.conv1.weight
architecture.py - CubeDiffModel - base_unet name is down_blocks.0.resnets.1.conv1.bias
architecture.py - CubeDiffModel - base_unet name is down_blocks.0.resnets.1.time_emb_proj.weight
architecture.py - CubeDiffModel - base_unet name is down_blocks.0.resnets.1.time_emb_proj.bias
architecture.py - CubeDiffModel - base_unet name is down_blocks.0.resnets.1.norm2.weight
architecture.py - CubeDiffModel - base_unet name is down_blocks.0.resnets.1.norm2.bias
architecture.py - CubeDiffModel - base_unet name is down_blocks.0.resnets.1.conv2.weight
architecture.py - CubeDiffModel - base_unet name is down_blocks.0.resnets.1.conv2.bias
architecture.py - CubeDiffModel - base_unet name is down_blocks.0.downsamplers.0.conv.weight
architecture.py - CubeDiffModel - base_unet name is down_blocks.0.downsamplers.0.conv.bias
architecture.py - CubeDiffModel - base_unet name is down_blocks.1.attentions.0.norm.weight
architecture.py - CubeDiffModel - base_unet name is down_blocks.1.attentions.0.norm.bias
architecture.py - CubeDiffModel - base_unet name is down_blocks.1.attentions.0.proj_in.weight
architecture.py - CubeDiffModel - base_unet name is down_blocks.1.attentions.0.proj_in.bias
architecture.py - CubeDiffModel - base_unet name is down_blocks.1.attentions.0.transformer_blocks.0.norm1.weight
architecture.py - CubeDiffModel - base_unet name is down_blocks.1.attentions.0.transformer_blocks.0.norm1.bias
architecture.py - CubeDiffModel - base_unet name is down_blocks.1.attentions.0.transformer_blocks.0.attn1.to_q.weight
architecture.py - CubeDiffModel - base_unet name is down_blocks.1.attentions.0.transformer_blocks.0.attn1.to_k.weight
architecture.py - CubeDiffModel - base_unet name is down_blocks.1.attentions.0.transformer_blocks.0.attn1.to_v.weight
architecture.py - CubeDiffModel - base_unet name is down_blocks.1.attentions.0.transformer_blocks.0.attn1.to_out.0.weight
architecture.py - CubeDiffModel - base_unet name is down_blocks.1.attentions.0.transformer_blocks.0.attn1.to_out.0.bias
architecture.py - CubeDiffModel - base_unet name is down_blocks.1.attentions.0.transformer_blocks.0.norm2.weight
architecture.py - CubeDiffModel - base_unet name is down_blocks.1.attentions.0.transformer_blocks.0.norm2.bias
architecture.py - CubeDiffModel - base_unet name is down_blocks.1.attentions.0.transformer_blocks.0.attn2.to_q.weight
architecture.py - CubeDiffModel - base_unet name is down_blocks.1.attentions.0.transformer_blocks.0.attn2.to_k.weight
architecture.py - CubeDiffModel - base_unet name is down_blocks.1.attentions.0.transformer_blocks.0.attn2.to_v.weight
architecture.py - CubeDiffModel - base_unet name is down_blocks.1.attentions.0.transformer_blocks.0.attn2.to_out.0.weight
architecture.py - CubeDiffModel - base_unet name is down_blocks.1.attentions.0.transformer_blocks.0.attn2.to_out.0.bias
architecture.py - CubeDiffModel - base_unet name is down_blocks.1.attentions.0.transformer_blocks.0.norm3.weight
architecture.py - CubeDiffModel - base_unet name is down_blocks.1.attentions.0.transformer_blocks.0.norm3.bias
architecture.py - CubeDiffModel - base_unet name is down_blocks.1.attentions.0.transformer_blocks.0.ff.net.0.proj.weight
architecture.py - CubeDiffModel - base_unet name is down_blocks.1.attentions.0.transformer_blocks.0.ff.net.0.proj.bias
architecture.py - CubeDiffModel - base_unet name is down_blocks.1.attentions.0.transformer_blocks.0.ff.net.2.weight
architecture.py - CubeDiffModel - base_unet name is down_blocks.1.attentions.0.transformer_blocks.0.ff.net.2.bias
architecture.py - CubeDiffModel - base_unet name is down_blocks.1.attentions.0.proj_out.weight
architecture.py - CubeDiffModel - base_unet name is down_blocks.1.attentions.0.proj_out.bias
architecture.py - CubeDiffModel - base_unet name is down_blocks.1.attentions.1.norm.weight
architecture.py - CubeDiffModel - base_unet name is down_blocks.1.attentions.1.norm.bias
architecture.py - CubeDiffModel - base_unet name is down_blocks.1.attentions.1.proj_in.weight
architecture.py - CubeDiffModel - base_unet name is down_blocks.1.attentions.1.proj_in.bias
architecture.py - CubeDiffModel - base_unet name is down_blocks.1.attentions.1.transformer_blocks.0.norm1.weight
architecture.py - CubeDiffModel - base_unet name is down_blocks.1.attentions.1.transformer_blocks.0.norm1.bias
architecture.py - CubeDiffModel - base_unet name is down_blocks.1.attentions.1.transformer_blocks.0.attn1.to_q.weight
architecture.py - CubeDiffModel - base_unet name is down_blocks.1.attentions.1.transformer_blocks.0.attn1.to_k.weight
architecture.py - CubeDiffModel - base_unet name is down_blocks.1.attentions.1.transformer_blocks.0.attn1.to_v.weight
architecture.py - CubeDiffModel - base_unet name is down_blocks.1.attentions.1.transformer_blocks.0.attn1.to_out.0.weight
architecture.py - CubeDiffModel - base_unet name is down_blocks.1.attentions.1.transformer_blocks.0.attn1.to_out.0.bias
architecture.py - CubeDiffModel - base_unet name is down_blocks.1.attentions.1.transformer_blocks.0.norm2.weight
architecture.py - CubeDiffModel - base_unet name is down_blocks.1.attentions.1.transformer_blocks.0.norm2.bias
architecture.py - CubeDiffModel - base_unet name is down_blocks.1.attentions.1.transformer_blocks.0.attn2.to_q.weight
architecture.py - CubeDiffModel - base_unet name is down_blocks.1.attentions.1.transformer_blocks.0.attn2.to_k.weight
architecture.py - CubeDiffModel - base_unet name is down_blocks.1.attentions.1.transformer_blocks.0.attn2.to_v.weight
architecture.py - CubeDiffModel - base_unet name is down_blocks.1.attentions.1.transformer_blocks.0.attn2.to_out.0.weight
architecture.py - CubeDiffModel - base_unet name is down_blocks.1.attentions.1.transformer_blocks.0.attn2.to_out.0.bias
architecture.py - CubeDiffModel - base_unet name is down_blocks.1.attentions.1.transformer_blocks.0.norm3.weight
architecture.py - CubeDiffModel - base_unet name is down_blocks.1.attentions.1.transformer_blocks.0.norm3.bias
architecture.py - CubeDiffModel - base_unet name is down_blocks.1.attentions.1.transformer_blocks.0.ff.net.0.proj.weight
architecture.py - CubeDiffModel - base_unet name is down_blocks.1.attentions.1.transformer_blocks.0.ff.net.0.proj.bias
architecture.py - CubeDiffModel - base_unet name is down_blocks.1.attentions.1.transformer_blocks.0.ff.net.2.weight
architecture.py - CubeDiffModel - base_unet name is down_blocks.1.attentions.1.transformer_blocks.0.ff.net.2.bias
architecture.py - CubeDiffModel - base_unet name is down_blocks.1.attentions.1.proj_out.weight
architecture.py - CubeDiffModel - base_unet name is down_blocks.1.attentions.1.proj_out.bias
architecture.py - CubeDiffModel - base_unet name is down_blocks.1.resnets.0.norm1.weight
architecture.py - CubeDiffModel - base_unet name is down_blocks.1.resnets.0.norm1.bias
architecture.py - CubeDiffModel - base_unet name is down_blocks.1.resnets.0.conv1.weight
architecture.py - CubeDiffModel - base_unet name is down_blocks.1.resnets.0.conv1.bias
architecture.py - CubeDiffModel - base_unet name is down_blocks.1.resnets.0.time_emb_proj.weight
architecture.py - CubeDiffModel - base_unet name is down_blocks.1.resnets.0.time_emb_proj.bias
architecture.py - CubeDiffModel - base_unet name is down_blocks.1.resnets.0.norm2.weight
architecture.py - CubeDiffModel - base_unet name is down_blocks.1.resnets.0.norm2.bias
architecture.py - CubeDiffModel - base_unet name is down_blocks.1.resnets.0.conv2.weight
architecture.py - CubeDiffModel - base_unet name is down_blocks.1.resnets.0.conv2.bias
architecture.py - CubeDiffModel - base_unet name is down_blocks.1.resnets.0.conv_shortcut.weight
architecture.py - CubeDiffModel - base_unet name is down_blocks.1.resnets.0.conv_shortcut.bias
architecture.py - CubeDiffModel - base_unet name is down_blocks.1.resnets.1.norm1.weight
architecture.py - CubeDiffModel - base_unet name is down_blocks.1.resnets.1.norm1.bias
architecture.py - CubeDiffModel - base_unet name is down_blocks.1.resnets.1.conv1.weight
architecture.py - CubeDiffModel - base_unet name is down_blocks.1.resnets.1.conv1.bias
architecture.py - CubeDiffModel - base_unet name is down_blocks.1.resnets.1.time_emb_proj.weight
architecture.py - CubeDiffModel - base_unet name is down_blocks.1.resnets.1.time_emb_proj.bias
architecture.py - CubeDiffModel - base_unet name is down_blocks.1.resnets.1.norm2.weight
architecture.py - CubeDiffModel - base_unet name is down_blocks.1.resnets.1.norm2.bias
architecture.py - CubeDiffModel - base_unet name is down_blocks.1.resnets.1.conv2.weight
architecture.py - CubeDiffModel - base_unet name is down_blocks.1.resnets.1.conv2.bias
architecture.py - CubeDiffModel - base_unet name is down_blocks.1.downsamplers.0.conv.weight
architecture.py - CubeDiffModel - base_unet name is down_blocks.1.downsamplers.0.conv.bias
architecture.py - CubeDiffModel - base_unet name is down_blocks.2.attentions.0.norm.weight
architecture.py - CubeDiffModel - base_unet name is down_blocks.2.attentions.0.norm.bias
architecture.py - CubeDiffModel - base_unet name is down_blocks.2.attentions.0.proj_in.weight
architecture.py - CubeDiffModel - base_unet name is down_blocks.2.attentions.0.proj_in.bias
architecture.py - CubeDiffModel - base_unet name is down_blocks.2.attentions.0.transformer_blocks.0.norm1.weight
architecture.py - CubeDiffModel - base_unet name is down_blocks.2.attentions.0.transformer_blocks.0.norm1.bias
architecture.py - CubeDiffModel - base_unet name is down_blocks.2.attentions.0.transformer_blocks.0.attn1.to_q.weight
architecture.py - CubeDiffModel - base_unet name is down_blocks.2.attentions.0.transformer_blocks.0.attn1.to_k.weight
architecture.py - CubeDiffModel - base_unet name is down_blocks.2.attentions.0.transformer_blocks.0.attn1.to_v.weight
architecture.py - CubeDiffModel - base_unet name is down_blocks.2.attentions.0.transformer_blocks.0.attn1.to_out.0.weight
architecture.py - CubeDiffModel - base_unet name is down_blocks.2.attentions.0.transformer_blocks.0.attn1.to_out.0.bias
architecture.py - CubeDiffModel - base_unet name is down_blocks.2.attentions.0.transformer_blocks.0.norm2.weight
architecture.py - CubeDiffModel - base_unet name is down_blocks.2.attentions.0.transformer_blocks.0.norm2.bias
architecture.py - CubeDiffModel - base_unet name is down_blocks.2.attentions.0.transformer_blocks.0.attn2.to_q.weight
architecture.py - CubeDiffModel - base_unet name is down_blocks.2.attentions.0.transformer_blocks.0.attn2.to_k.weight
architecture.py - CubeDiffModel - base_unet name is down_blocks.2.attentions.0.transformer_blocks.0.attn2.to_v.weight
architecture.py - CubeDiffModel - base_unet name is down_blocks.2.attentions.0.transformer_blocks.0.attn2.to_out.0.weight
architecture.py - CubeDiffModel - base_unet name is down_blocks.2.attentions.0.transformer_blocks.0.attn2.to_out.0.bias
architecture.py - CubeDiffModel - base_unet name is down_blocks.2.attentions.0.transformer_blocks.0.norm3.weight
architecture.py - CubeDiffModel - base_unet name is down_blocks.2.attentions.0.transformer_blocks.0.norm3.bias
architecture.py - CubeDiffModel - base_unet name is down_blocks.2.attentions.0.transformer_blocks.0.ff.net.0.proj.weight
architecture.py - CubeDiffModel - base_unet name is down_blocks.2.attentions.0.transformer_blocks.0.ff.net.0.proj.bias
architecture.py - CubeDiffModel - base_unet name is down_blocks.2.attentions.0.transformer_blocks.0.ff.net.2.weight
architecture.py - CubeDiffModel - base_unet name is down_blocks.2.attentions.0.transformer_blocks.0.ff.net.2.bias
architecture.py - CubeDiffModel - base_unet name is down_blocks.2.attentions.0.proj_out.weight
architecture.py - CubeDiffModel - base_unet name is down_blocks.2.attentions.0.proj_out.bias
architecture.py - CubeDiffModel - base_unet name is down_blocks.2.attentions.1.norm.weight
architecture.py - CubeDiffModel - base_unet name is down_blocks.2.attentions.1.norm.bias
architecture.py - CubeDiffModel - base_unet name is down_blocks.2.attentions.1.proj_in.weight
architecture.py - CubeDiffModel - base_unet name is down_blocks.2.attentions.1.proj_in.bias
architecture.py - CubeDiffModel - base_unet name is down_blocks.2.attentions.1.transformer_blocks.0.norm1.weight
architecture.py - CubeDiffModel - base_unet name is down_blocks.2.attentions.1.transformer_blocks.0.norm1.bias
architecture.py - CubeDiffModel - base_unet name is down_blocks.2.attentions.1.transformer_blocks.0.attn1.to_q.weight
architecture.py - CubeDiffModel - base_unet name is down_blocks.2.attentions.1.transformer_blocks.0.attn1.to_k.weight
architecture.py - CubeDiffModel - base_unet name is down_blocks.2.attentions.1.transformer_blocks.0.attn1.to_v.weight
architecture.py - CubeDiffModel - base_unet name is down_blocks.2.attentions.1.transformer_blocks.0.attn1.to_out.0.weight
architecture.py - CubeDiffModel - base_unet name is down_blocks.2.attentions.1.transformer_blocks.0.attn1.to_out.0.bias
architecture.py - CubeDiffModel - base_unet name is down_blocks.2.attentions.1.transformer_blocks.0.norm2.weight
architecture.py - CubeDiffModel - base_unet name is down_blocks.2.attentions.1.transformer_blocks.0.norm2.bias
architecture.py - CubeDiffModel - base_unet name is down_blocks.2.attentions.1.transformer_blocks.0.attn2.to_q.weight
architecture.py - CubeDiffModel - base_unet name is down_blocks.2.attentions.1.transformer_blocks.0.attn2.to_k.weight
architecture.py - CubeDiffModel - base_unet name is down_blocks.2.attentions.1.transformer_blocks.0.attn2.to_v.weight
architecture.py - CubeDiffModel - base_unet name is down_blocks.2.attentions.1.transformer_blocks.0.attn2.to_out.0.weight
architecture.py - CubeDiffModel - base_unet name is down_blocks.2.attentions.1.transformer_blocks.0.attn2.to_out.0.bias
architecture.py - CubeDiffModel - base_unet name is down_blocks.2.attentions.1.transformer_blocks.0.norm3.weight
architecture.py - CubeDiffModel - base_unet name is down_blocks.2.attentions.1.transformer_blocks.0.norm3.bias
architecture.py - CubeDiffModel - base_unet name is down_blocks.2.attentions.1.transformer_blocks.0.ff.net.0.proj.weight
architecture.py - CubeDiffModel - base_unet name is down_blocks.2.attentions.1.transformer_blocks.0.ff.net.0.proj.bias
architecture.py - CubeDiffModel - base_unet name is down_blocks.2.attentions.1.transformer_blocks.0.ff.net.2.weight
architecture.py - CubeDiffModel - base_unet name is down_blocks.2.attentions.1.transformer_blocks.0.ff.net.2.bias
architecture.py - CubeDiffModel - base_unet name is down_blocks.2.attentions.1.proj_out.weight
architecture.py - CubeDiffModel - base_unet name is down_blocks.2.attentions.1.proj_out.bias
architecture.py - CubeDiffModel - base_unet name is down_blocks.2.resnets.0.norm1.weight
architecture.py - CubeDiffModel - base_unet name is down_blocks.2.resnets.0.norm1.bias
architecture.py - CubeDiffModel - base_unet name is down_blocks.2.resnets.0.conv1.weight
architecture.py - CubeDiffModel - base_unet name is down_blocks.2.resnets.0.conv1.bias
architecture.py - CubeDiffModel - base_unet name is down_blocks.2.resnets.0.time_emb_proj.weight
architecture.py - CubeDiffModel - base_unet name is down_blocks.2.resnets.0.time_emb_proj.bias
architecture.py - CubeDiffModel - base_unet name is down_blocks.2.resnets.0.norm2.weight
architecture.py - CubeDiffModel - base_unet name is down_blocks.2.resnets.0.norm2.bias
architecture.py - CubeDiffModel - base_unet name is down_blocks.2.resnets.0.conv2.weight
architecture.py - CubeDiffModel - base_unet name is down_blocks.2.resnets.0.conv2.bias
architecture.py - CubeDiffModel - base_unet name is down_blocks.2.resnets.0.conv_shortcut.weight
architecture.py - CubeDiffModel - base_unet name is down_blocks.2.resnets.0.conv_shortcut.bias
architecture.py - CubeDiffModel - base_unet name is down_blocks.2.resnets.1.norm1.weight
architecture.py - CubeDiffModel - base_unet name is down_blocks.2.resnets.1.norm1.bias
architecture.py - CubeDiffModel - base_unet name is down_blocks.2.resnets.1.conv1.weight
architecture.py - CubeDiffModel - base_unet name is down_blocks.2.resnets.1.conv1.bias
architecture.py - CubeDiffModel - base_unet name is down_blocks.2.resnets.1.time_emb_proj.weight
architecture.py - CubeDiffModel - base_unet name is down_blocks.2.resnets.1.time_emb_proj.bias
architecture.py - CubeDiffModel - base_unet name is down_blocks.2.resnets.1.norm2.weight
architecture.py - CubeDiffModel - base_unet name is down_blocks.2.resnets.1.norm2.bias
architecture.py - CubeDiffModel - base_unet name is down_blocks.2.resnets.1.conv2.weight
architecture.py - CubeDiffModel - base_unet name is down_blocks.2.resnets.1.conv2.bias
architecture.py - CubeDiffModel - base_unet name is down_blocks.2.downsamplers.0.conv.weight
architecture.py - CubeDiffModel - base_unet name is down_blocks.2.downsamplers.0.conv.bias
architecture.py - CubeDiffModel - base_unet name is down_blocks.3.resnets.0.norm1.weight
architecture.py - CubeDiffModel - base_unet name is down_blocks.3.resnets.0.norm1.bias
architecture.py - CubeDiffModel - base_unet name is down_blocks.3.resnets.0.conv1.weight
architecture.py - CubeDiffModel - base_unet name is down_blocks.3.resnets.0.conv1.bias
architecture.py - CubeDiffModel - base_unet name is down_blocks.3.resnets.0.time_emb_proj.weight
architecture.py - CubeDiffModel - base_unet name is down_blocks.3.resnets.0.time_emb_proj.bias
architecture.py - CubeDiffModel - base_unet name is down_blocks.3.resnets.0.norm2.weight
architecture.py - CubeDiffModel - base_unet name is down_blocks.3.resnets.0.norm2.bias
architecture.py - CubeDiffModel - base_unet name is down_blocks.3.resnets.0.conv2.weight
architecture.py - CubeDiffModel - base_unet name is down_blocks.3.resnets.0.conv2.bias
architecture.py - CubeDiffModel - base_unet name is down_blocks.3.resnets.1.norm1.weight
architecture.py - CubeDiffModel - base_unet name is down_blocks.3.resnets.1.norm1.bias
architecture.py - CubeDiffModel - base_unet name is down_blocks.3.resnets.1.conv1.weight
architecture.py - CubeDiffModel - base_unet name is down_blocks.3.resnets.1.conv1.bias
architecture.py - CubeDiffModel - base_unet name is down_blocks.3.resnets.1.time_emb_proj.weight
architecture.py - CubeDiffModel - base_unet name is down_blocks.3.resnets.1.time_emb_proj.bias
architecture.py - CubeDiffModel - base_unet name is down_blocks.3.resnets.1.norm2.weight
architecture.py - CubeDiffModel - base_unet name is down_blocks.3.resnets.1.norm2.bias
architecture.py - CubeDiffModel - base_unet name is down_blocks.3.resnets.1.conv2.weight
architecture.py - CubeDiffModel - base_unet name is down_blocks.3.resnets.1.conv2.bias
architecture.py - CubeDiffModel - base_unet name is up_blocks.0.resnets.0.norm1.weight
architecture.py - CubeDiffModel - base_unet name is up_blocks.0.resnets.0.norm1.bias
architecture.py - CubeDiffModel - base_unet name is up_blocks.0.resnets.0.conv1.weight
architecture.py - CubeDiffModel - base_unet name is up_blocks.0.resnets.0.conv1.bias
architecture.py - CubeDiffModel - base_unet name is up_blocks.0.resnets.0.time_emb_proj.weight
architecture.py - CubeDiffModel - base_unet name is up_blocks.0.resnets.0.time_emb_proj.bias
architecture.py - CubeDiffModel - base_unet name is up_blocks.0.resnets.0.norm2.weight
architecture.py - CubeDiffModel - base_unet name is up_blocks.0.resnets.0.norm2.bias
architecture.py - CubeDiffModel - base_unet name is up_blocks.0.resnets.0.conv2.weight
architecture.py - CubeDiffModel - base_unet name is up_blocks.0.resnets.0.conv2.bias
architecture.py - CubeDiffModel - base_unet name is up_blocks.0.resnets.0.conv_shortcut.weight
architecture.py - CubeDiffModel - base_unet name is up_blocks.0.resnets.0.conv_shortcut.bias
architecture.py - CubeDiffModel - base_unet name is up_blocks.0.resnets.1.norm1.weight
architecture.py - CubeDiffModel - base_unet name is up_blocks.0.resnets.1.norm1.bias
architecture.py - CubeDiffModel - base_unet name is up_blocks.0.resnets.1.conv1.weight
architecture.py - CubeDiffModel - base_unet name is up_blocks.0.resnets.1.conv1.bias
architecture.py - CubeDiffModel - base_unet name is up_blocks.0.resnets.1.time_emb_proj.weight
architecture.py - CubeDiffModel - base_unet name is up_blocks.0.resnets.1.time_emb_proj.bias
architecture.py - CubeDiffModel - base_unet name is up_blocks.0.resnets.1.norm2.weight
architecture.py - CubeDiffModel - base_unet name is up_blocks.0.resnets.1.norm2.bias
architecture.py - CubeDiffModel - base_unet name is up_blocks.0.resnets.1.conv2.weight
architecture.py - CubeDiffModel - base_unet name is up_blocks.0.resnets.1.conv2.bias
architecture.py - CubeDiffModel - base_unet name is up_blocks.0.resnets.1.conv_shortcut.weight
architecture.py - CubeDiffModel - base_unet name is up_blocks.0.resnets.1.conv_shortcut.bias
architecture.py - CubeDiffModel - base_unet name is up_blocks.0.resnets.2.norm1.weight
architecture.py - CubeDiffModel - base_unet name is up_blocks.0.resnets.2.norm1.bias
architecture.py - CubeDiffModel - base_unet name is up_blocks.0.resnets.2.conv1.weight
architecture.py - CubeDiffModel - base_unet name is up_blocks.0.resnets.2.conv1.bias
architecture.py - CubeDiffModel - base_unet name is up_blocks.0.resnets.2.time_emb_proj.weight
architecture.py - CubeDiffModel - base_unet name is up_blocks.0.resnets.2.time_emb_proj.bias
architecture.py - CubeDiffModel - base_unet name is up_blocks.0.resnets.2.norm2.weight
architecture.py - CubeDiffModel - base_unet name is up_blocks.0.resnets.2.norm2.bias
architecture.py - CubeDiffModel - base_unet name is up_blocks.0.resnets.2.conv2.weight
architecture.py - CubeDiffModel - base_unet name is up_blocks.0.resnets.2.conv2.bias
architecture.py - CubeDiffModel - base_unet name is up_blocks.0.resnets.2.conv_shortcut.weight
architecture.py - CubeDiffModel - base_unet name is up_blocks.0.resnets.2.conv_shortcut.bias
architecture.py - CubeDiffModel - base_unet name is up_blocks.0.upsamplers.0.conv.weight
architecture.py - CubeDiffModel - base_unet name is up_blocks.0.upsamplers.0.conv.bias
architecture.py - CubeDiffModel - base_unet name is up_blocks.1.attentions.0.norm.weight
architecture.py - CubeDiffModel - base_unet name is up_blocks.1.attentions.0.norm.bias
architecture.py - CubeDiffModel - base_unet name is up_blocks.1.attentions.0.proj_in.weight
architecture.py - CubeDiffModel - base_unet name is up_blocks.1.attentions.0.proj_in.bias
architecture.py - CubeDiffModel - base_unet name is up_blocks.1.attentions.0.transformer_blocks.0.norm1.weight
architecture.py - CubeDiffModel - base_unet name is up_blocks.1.attentions.0.transformer_blocks.0.norm1.bias
architecture.py - CubeDiffModel - base_unet name is up_blocks.1.attentions.0.transformer_blocks.0.attn1.to_q.weight
architecture.py - CubeDiffModel - base_unet name is up_blocks.1.attentions.0.transformer_blocks.0.attn1.to_k.weight
architecture.py - CubeDiffModel - base_unet name is up_blocks.1.attentions.0.transformer_blocks.0.attn1.to_v.weight
architecture.py - CubeDiffModel - base_unet name is up_blocks.1.attentions.0.transformer_blocks.0.attn1.to_out.0.weight
architecture.py - CubeDiffModel - base_unet name is up_blocks.1.attentions.0.transformer_blocks.0.attn1.to_out.0.bias
architecture.py - CubeDiffModel - base_unet name is up_blocks.1.attentions.0.transformer_blocks.0.norm2.weight
architecture.py - CubeDiffModel - base_unet name is up_blocks.1.attentions.0.transformer_blocks.0.norm2.bias
architecture.py - CubeDiffModel - base_unet name is up_blocks.1.attentions.0.transformer_blocks.0.attn2.to_q.weight
architecture.py - CubeDiffModel - base_unet name is up_blocks.1.attentions.0.transformer_blocks.0.attn2.to_k.weight
architecture.py - CubeDiffModel - base_unet name is up_blocks.1.attentions.0.transformer_blocks.0.attn2.to_v.weight
architecture.py - CubeDiffModel - base_unet name is up_blocks.1.attentions.0.transformer_blocks.0.attn2.to_out.0.weight
architecture.py - CubeDiffModel - base_unet name is up_blocks.1.attentions.0.transformer_blocks.0.attn2.to_out.0.bias
architecture.py - CubeDiffModel - base_unet name is up_blocks.1.attentions.0.transformer_blocks.0.norm3.weight
architecture.py - CubeDiffModel - base_unet name is up_blocks.1.attentions.0.transformer_blocks.0.norm3.bias
architecture.py - CubeDiffModel - base_unet name is up_blocks.1.attentions.0.transformer_blocks.0.ff.net.0.proj.weight
architecture.py - CubeDiffModel - base_unet name is up_blocks.1.attentions.0.transformer_blocks.0.ff.net.0.proj.bias
architecture.py - CubeDiffModel - base_unet name is up_blocks.1.attentions.0.transformer_blocks.0.ff.net.2.weight
architecture.py - CubeDiffModel - base_unet name is up_blocks.1.attentions.0.transformer_blocks.0.ff.net.2.bias
architecture.py - CubeDiffModel - base_unet name is up_blocks.1.attentions.0.proj_out.weight
architecture.py - CubeDiffModel - base_unet name is up_blocks.1.attentions.0.proj_out.bias
architecture.py - CubeDiffModel - base_unet name is up_blocks.1.attentions.1.norm.weight
architecture.py - CubeDiffModel - base_unet name is up_blocks.1.attentions.1.norm.bias
architecture.py - CubeDiffModel - base_unet name is up_blocks.1.attentions.1.proj_in.weight
architecture.py - CubeDiffModel - base_unet name is up_blocks.1.attentions.1.proj_in.bias
architecture.py - CubeDiffModel - base_unet name is up_blocks.1.attentions.1.transformer_blocks.0.norm1.weight
architecture.py - CubeDiffModel - base_unet name is up_blocks.1.attentions.1.transformer_blocks.0.norm1.bias
architecture.py - CubeDiffModel - base_unet name is up_blocks.1.attentions.1.transformer_blocks.0.attn1.to_q.weight
architecture.py - CubeDiffModel - base_unet name is up_blocks.1.attentions.1.transformer_blocks.0.attn1.to_k.weight
architecture.py - CubeDiffModel - base_unet name is up_blocks.1.attentions.1.transformer_blocks.0.attn1.to_v.weight
architecture.py - CubeDiffModel - base_unet name is up_blocks.1.attentions.1.transformer_blocks.0.attn1.to_out.0.weight
architecture.py - CubeDiffModel - base_unet name is up_blocks.1.attentions.1.transformer_blocks.0.attn1.to_out.0.bias
architecture.py - CubeDiffModel - base_unet name is up_blocks.1.attentions.1.transformer_blocks.0.norm2.weight
architecture.py - CubeDiffModel - base_unet name is up_blocks.1.attentions.1.transformer_blocks.0.norm2.bias
architecture.py - CubeDiffModel - base_unet name is up_blocks.1.attentions.1.transformer_blocks.0.attn2.to_q.weight
architecture.py - CubeDiffModel - base_unet name is up_blocks.1.attentions.1.transformer_blocks.0.attn2.to_k.weight
architecture.py - CubeDiffModel - base_unet name is up_blocks.1.attentions.1.transformer_blocks.0.attn2.to_v.weight
architecture.py - CubeDiffModel - base_unet name is up_blocks.1.attentions.1.transformer_blocks.0.attn2.to_out.0.weight
architecture.py - CubeDiffModel - base_unet name is up_blocks.1.attentions.1.transformer_blocks.0.attn2.to_out.0.bias
architecture.py - CubeDiffModel - base_unet name is up_blocks.1.attentions.1.transformer_blocks.0.norm3.weight
architecture.py - CubeDiffModel - base_unet name is up_blocks.1.attentions.1.transformer_blocks.0.norm3.bias
architecture.py - CubeDiffModel - base_unet name is up_blocks.1.attentions.1.transformer_blocks.0.ff.net.0.proj.weight
architecture.py - CubeDiffModel - base_unet name is up_blocks.1.attentions.1.transformer_blocks.0.ff.net.0.proj.bias
architecture.py - CubeDiffModel - base_unet name is up_blocks.1.attentions.1.transformer_blocks.0.ff.net.2.weight
architecture.py - CubeDiffModel - base_unet name is up_blocks.1.attentions.1.transformer_blocks.0.ff.net.2.bias
architecture.py - CubeDiffModel - base_unet name is up_blocks.1.attentions.1.proj_out.weight
architecture.py - CubeDiffModel - base_unet name is up_blocks.1.attentions.1.proj_out.bias
architecture.py - CubeDiffModel - base_unet name is up_blocks.1.attentions.2.norm.weight
architecture.py - CubeDiffModel - base_unet name is up_blocks.1.attentions.2.norm.bias
architecture.py - CubeDiffModel - base_unet name is up_blocks.1.attentions.2.proj_in.weight
architecture.py - CubeDiffModel - base_unet name is up_blocks.1.attentions.2.proj_in.bias
architecture.py - CubeDiffModel - base_unet name is up_blocks.1.attentions.2.transformer_blocks.0.norm1.weight
architecture.py - CubeDiffModel - base_unet name is up_blocks.1.attentions.2.transformer_blocks.0.norm1.bias
architecture.py - CubeDiffModel - base_unet name is up_blocks.1.attentions.2.transformer_blocks.0.attn1.to_q.weight
architecture.py - CubeDiffModel - base_unet name is up_blocks.1.attentions.2.transformer_blocks.0.attn1.to_k.weight
architecture.py - CubeDiffModel - base_unet name is up_blocks.1.attentions.2.transformer_blocks.0.attn1.to_v.weight
architecture.py - CubeDiffModel - base_unet name is up_blocks.1.attentions.2.transformer_blocks.0.attn1.to_out.0.weight
architecture.py - CubeDiffModel - base_unet name is up_blocks.1.attentions.2.transformer_blocks.0.attn1.to_out.0.bias
architecture.py - CubeDiffModel - base_unet name is up_blocks.1.attentions.2.transformer_blocks.0.norm2.weight
architecture.py - CubeDiffModel - base_unet name is up_blocks.1.attentions.2.transformer_blocks.0.norm2.bias
architecture.py - CubeDiffModel - base_unet name is up_blocks.1.attentions.2.transformer_blocks.0.attn2.to_q.weight
architecture.py - CubeDiffModel - base_unet name is up_blocks.1.attentions.2.transformer_blocks.0.attn2.to_k.weight
architecture.py - CubeDiffModel - base_unet name is up_blocks.1.attentions.2.transformer_blocks.0.attn2.to_v.weight
architecture.py - CubeDiffModel - base_unet name is up_blocks.1.attentions.2.transformer_blocks.0.attn2.to_out.0.weight
architecture.py - CubeDiffModel - base_unet name is up_blocks.1.attentions.2.transformer_blocks.0.attn2.to_out.0.bias
architecture.py - CubeDiffModel - base_unet name is up_blocks.1.attentions.2.transformer_blocks.0.norm3.weight
architecture.py - CubeDiffModel - base_unet name is up_blocks.1.attentions.2.transformer_blocks.0.norm3.bias
architecture.py - CubeDiffModel - base_unet name is up_blocks.1.attentions.2.transformer_blocks.0.ff.net.0.proj.weight
architecture.py - CubeDiffModel - base_unet name is up_blocks.1.attentions.2.transformer_blocks.0.ff.net.0.proj.bias
architecture.py - CubeDiffModel - base_unet name is up_blocks.1.attentions.2.transformer_blocks.0.ff.net.2.weight
architecture.py - CubeDiffModel - base_unet name is up_blocks.1.attentions.2.transformer_blocks.0.ff.net.2.bias
architecture.py - CubeDiffModel - base_unet name is up_blocks.1.attentions.2.proj_out.weight
architecture.py - CubeDiffModel - base_unet name is up_blocks.1.attentions.2.proj_out.bias
architecture.py - CubeDiffModel - base_unet name is up_blocks.1.resnets.0.norm1.weight
architecture.py - CubeDiffModel - base_unet name is up_blocks.1.resnets.0.norm1.bias
architecture.py - CubeDiffModel - base_unet name is up_blocks.1.resnets.0.conv1.weight
architecture.py - CubeDiffModel - base_unet name is up_blocks.1.resnets.0.conv1.bias
architecture.py - CubeDiffModel - base_unet name is up_blocks.1.resnets.0.time_emb_proj.weight
architecture.py - CubeDiffModel - base_unet name is up_blocks.1.resnets.0.time_emb_proj.bias
architecture.py - CubeDiffModel - base_unet name is up_blocks.1.resnets.0.norm2.weight
architecture.py - CubeDiffModel - base_unet name is up_blocks.1.resnets.0.norm2.bias
architecture.py - CubeDiffModel - base_unet name is up_blocks.1.resnets.0.conv2.weight
architecture.py - CubeDiffModel - base_unet name is up_blocks.1.resnets.0.conv2.bias
architecture.py - CubeDiffModel - base_unet name is up_blocks.1.resnets.0.conv_shortcut.weight
architecture.py - CubeDiffModel - base_unet name is up_blocks.1.resnets.0.conv_shortcut.bias
architecture.py - CubeDiffModel - base_unet name is up_blocks.1.resnets.1.norm1.weight
architecture.py - CubeDiffModel - base_unet name is up_blocks.1.resnets.1.norm1.bias
architecture.py - CubeDiffModel - base_unet name is up_blocks.1.resnets.1.conv1.weight
architecture.py - CubeDiffModel - base_unet name is up_blocks.1.resnets.1.conv1.bias
architecture.py - CubeDiffModel - base_unet name is up_blocks.1.resnets.1.time_emb_proj.weight
architecture.py - CubeDiffModel - base_unet name is up_blocks.1.resnets.1.time_emb_proj.bias
architecture.py - CubeDiffModel - base_unet name is up_blocks.1.resnets.1.norm2.weight
architecture.py - CubeDiffModel - base_unet name is up_blocks.1.resnets.1.norm2.bias
architecture.py - CubeDiffModel - base_unet name is up_blocks.1.resnets.1.conv2.weight
architecture.py - CubeDiffModel - base_unet name is up_blocks.1.resnets.1.conv2.bias
architecture.py - CubeDiffModel - base_unet name is up_blocks.1.resnets.1.conv_shortcut.weight
architecture.py - CubeDiffModel - base_unet name is up_blocks.1.resnets.1.conv_shortcut.bias
architecture.py - CubeDiffModel - base_unet name is up_blocks.1.resnets.2.norm1.weight
architecture.py - CubeDiffModel - base_unet name is up_blocks.1.resnets.2.norm1.bias
architecture.py - CubeDiffModel - base_unet name is up_blocks.1.resnets.2.conv1.weight
architecture.py - CubeDiffModel - base_unet name is up_blocks.1.resnets.2.conv1.bias
architecture.py - CubeDiffModel - base_unet name is up_blocks.1.resnets.2.time_emb_proj.weight
architecture.py - CubeDiffModel - base_unet name is up_blocks.1.resnets.2.time_emb_proj.bias
architecture.py - CubeDiffModel - base_unet name is up_blocks.1.resnets.2.norm2.weight
architecture.py - CubeDiffModel - base_unet name is up_blocks.1.resnets.2.norm2.bias
architecture.py - CubeDiffModel - base_unet name is up_blocks.1.resnets.2.conv2.weight
architecture.py - CubeDiffModel - base_unet name is up_blocks.1.resnets.2.conv2.bias
architecture.py - CubeDiffModel - base_unet name is up_blocks.1.resnets.2.conv_shortcut.weight
architecture.py - CubeDiffModel - base_unet name is up_blocks.1.resnets.2.conv_shortcut.bias
architecture.py - CubeDiffModel - base_unet name is up_blocks.1.upsamplers.0.conv.weight
architecture.py - CubeDiffModel - base_unet name is up_blocks.1.upsamplers.0.conv.bias
architecture.py - CubeDiffModel - base_unet name is up_blocks.2.attentions.0.norm.weight
architecture.py - CubeDiffModel - base_unet name is up_blocks.2.attentions.0.norm.bias
architecture.py - CubeDiffModel - base_unet name is up_blocks.2.attentions.0.proj_in.weight
architecture.py - CubeDiffModel - base_unet name is up_blocks.2.attentions.0.proj_in.bias
architecture.py - CubeDiffModel - base_unet name is up_blocks.2.attentions.0.transformer_blocks.0.norm1.weight
architecture.py - CubeDiffModel - base_unet name is up_blocks.2.attentions.0.transformer_blocks.0.norm1.bias
architecture.py - CubeDiffModel - base_unet name is up_blocks.2.attentions.0.transformer_blocks.0.attn1.to_q.weight
architecture.py - CubeDiffModel - base_unet name is up_blocks.2.attentions.0.transformer_blocks.0.attn1.to_k.weight
architecture.py - CubeDiffModel - base_unet name is up_blocks.2.attentions.0.transformer_blocks.0.attn1.to_v.weight
architecture.py - CubeDiffModel - base_unet name is up_blocks.2.attentions.0.transformer_blocks.0.attn1.to_out.0.weight
architecture.py - CubeDiffModel - base_unet name is up_blocks.2.attentions.0.transformer_blocks.0.attn1.to_out.0.bias
architecture.py - CubeDiffModel - base_unet name is up_blocks.2.attentions.0.transformer_blocks.0.norm2.weight
architecture.py - CubeDiffModel - base_unet name is up_blocks.2.attentions.0.transformer_blocks.0.norm2.bias
architecture.py - CubeDiffModel - base_unet name is up_blocks.2.attentions.0.transformer_blocks.0.attn2.to_q.weight
architecture.py - CubeDiffModel - base_unet name is up_blocks.2.attentions.0.transformer_blocks.0.attn2.to_k.weight
architecture.py - CubeDiffModel - base_unet name is up_blocks.2.attentions.0.transformer_blocks.0.attn2.to_v.weight
architecture.py - CubeDiffModel - base_unet name is up_blocks.2.attentions.0.transformer_blocks.0.attn2.to_out.0.weight
architecture.py - CubeDiffModel - base_unet name is up_blocks.2.attentions.0.transformer_blocks.0.attn2.to_out.0.bias
architecture.py - CubeDiffModel - base_unet name is up_blocks.2.attentions.0.transformer_blocks.0.norm3.weight
architecture.py - CubeDiffModel - base_unet name is up_blocks.2.attentions.0.transformer_blocks.0.norm3.bias
architecture.py - CubeDiffModel - base_unet name is up_blocks.2.attentions.0.transformer_blocks.0.ff.net.0.proj.weight
architecture.py - CubeDiffModel - base_unet name is up_blocks.2.attentions.0.transformer_blocks.0.ff.net.0.proj.bias
architecture.py - CubeDiffModel - base_unet name is up_blocks.2.attentions.0.transformer_blocks.0.ff.net.2.weight
architecture.py - CubeDiffModel - base_unet name is up_blocks.2.attentions.0.transformer_blocks.0.ff.net.2.bias
architecture.py - CubeDiffModel - base_unet name is up_blocks.2.attentions.0.proj_out.weight
architecture.py - CubeDiffModel - base_unet name is up_blocks.2.attentions.0.proj_out.bias
architecture.py - CubeDiffModel - base_unet name is up_blocks.2.attentions.1.norm.weight
architecture.py - CubeDiffModel - base_unet name is up_blocks.2.attentions.1.norm.bias
architecture.py - CubeDiffModel - base_unet name is up_blocks.2.attentions.1.proj_in.weight
architecture.py - CubeDiffModel - base_unet name is up_blocks.2.attentions.1.proj_in.bias
architecture.py - CubeDiffModel - base_unet name is up_blocks.2.attentions.1.transformer_blocks.0.norm1.weight
architecture.py - CubeDiffModel - base_unet name is up_blocks.2.attentions.1.transformer_blocks.0.norm1.bias
architecture.py - CubeDiffModel - base_unet name is up_blocks.2.attentions.1.transformer_blocks.0.attn1.to_q.weight
architecture.py - CubeDiffModel - base_unet name is up_blocks.2.attentions.1.transformer_blocks.0.attn1.to_k.weight
architecture.py - CubeDiffModel - base_unet name is up_blocks.2.attentions.1.transformer_blocks.0.attn1.to_v.weight
architecture.py - CubeDiffModel - base_unet name is up_blocks.2.attentions.1.transformer_blocks.0.attn1.to_out.0.weight
architecture.py - CubeDiffModel - base_unet name is up_blocks.2.attentions.1.transformer_blocks.0.attn1.to_out.0.bias
architecture.py - CubeDiffModel - base_unet name is up_blocks.2.attentions.1.transformer_blocks.0.norm2.weight
architecture.py - CubeDiffModel - base_unet name is up_blocks.2.attentions.1.transformer_blocks.0.norm2.bias
architecture.py - CubeDiffModel - base_unet name is up_blocks.2.attentions.1.transformer_blocks.0.attn2.to_q.weight
architecture.py - CubeDiffModel - base_unet name is up_blocks.2.attentions.1.transformer_blocks.0.attn2.to_k.weight
architecture.py - CubeDiffModel - base_unet name is up_blocks.2.attentions.1.transformer_blocks.0.attn2.to_v.weight
architecture.py - CubeDiffModel - base_unet name is up_blocks.2.attentions.1.transformer_blocks.0.attn2.to_out.0.weight
architecture.py - CubeDiffModel - base_unet name is up_blocks.2.attentions.1.transformer_blocks.0.attn2.to_out.0.bias
architecture.py - CubeDiffModel - base_unet name is up_blocks.2.attentions.1.transformer_blocks.0.norm3.weight
architecture.py - CubeDiffModel - base_unet name is up_blocks.2.attentions.1.transformer_blocks.0.norm3.bias
architecture.py - CubeDiffModel - base_unet name is up_blocks.2.attentions.1.transformer_blocks.0.ff.net.0.proj.weight
architecture.py - CubeDiffModel - base_unet name is up_blocks.2.attentions.1.transformer_blocks.0.ff.net.0.proj.bias
architecture.py - CubeDiffModel - base_unet name is up_blocks.2.attentions.1.transformer_blocks.0.ff.net.2.weight
architecture.py - CubeDiffModel - base_unet name is up_blocks.2.attentions.1.transformer_blocks.0.ff.net.2.bias
architecture.py - CubeDiffModel - base_unet name is up_blocks.2.attentions.1.proj_out.weight
architecture.py - CubeDiffModel - base_unet name is up_blocks.2.attentions.1.proj_out.bias
architecture.py - CubeDiffModel - base_unet name is up_blocks.2.attentions.2.norm.weight
architecture.py - CubeDiffModel - base_unet name is up_blocks.2.attentions.2.norm.bias
architecture.py - CubeDiffModel - base_unet name is up_blocks.2.attentions.2.proj_in.weight
architecture.py - CubeDiffModel - base_unet name is up_blocks.2.attentions.2.proj_in.bias
architecture.py - CubeDiffModel - base_unet name is up_blocks.2.attentions.2.transformer_blocks.0.norm1.weight
architecture.py - CubeDiffModel - base_unet name is up_blocks.2.attentions.2.transformer_blocks.0.norm1.bias
architecture.py - CubeDiffModel - base_unet name is up_blocks.2.attentions.2.transformer_blocks.0.attn1.to_q.weight
architecture.py - CubeDiffModel - base_unet name is up_blocks.2.attentions.2.transformer_blocks.0.attn1.to_k.weight
architecture.py - CubeDiffModel - base_unet name is up_blocks.2.attentions.2.transformer_blocks.0.attn1.to_v.weight
architecture.py - CubeDiffModel - base_unet name is up_blocks.2.attentions.2.transformer_blocks.0.attn1.to_out.0.weight
architecture.py - CubeDiffModel - base_unet name is up_blocks.2.attentions.2.transformer_blocks.0.attn1.to_out.0.bias
architecture.py - CubeDiffModel - base_unet name is up_blocks.2.attentions.2.transformer_blocks.0.norm2.weight
architecture.py - CubeDiffModel - base_unet name is up_blocks.2.attentions.2.transformer_blocks.0.norm2.bias
architecture.py - CubeDiffModel - base_unet name is up_blocks.2.attentions.2.transformer_blocks.0.attn2.to_q.weight
architecture.py - CubeDiffModel - base_unet name is up_blocks.2.attentions.2.transformer_blocks.0.attn2.to_k.weight
architecture.py - CubeDiffModel - base_unet name is up_blocks.2.attentions.2.transformer_blocks.0.attn2.to_v.weight
architecture.py - CubeDiffModel - base_unet name is up_blocks.2.attentions.2.transformer_blocks.0.attn2.to_out.0.weight
architecture.py - CubeDiffModel - base_unet name is up_blocks.2.attentions.2.transformer_blocks.0.attn2.to_out.0.bias
architecture.py - CubeDiffModel - base_unet name is up_blocks.2.attentions.2.transformer_blocks.0.norm3.weight
architecture.py - CubeDiffModel - base_unet name is up_blocks.2.attentions.2.transformer_blocks.0.norm3.bias
architecture.py - CubeDiffModel - base_unet name is up_blocks.2.attentions.2.transformer_blocks.0.ff.net.0.proj.weight
architecture.py - CubeDiffModel - base_unet name is up_blocks.2.attentions.2.transformer_blocks.0.ff.net.0.proj.bias
architecture.py - CubeDiffModel - base_unet name is up_blocks.2.attentions.2.transformer_blocks.0.ff.net.2.weight
architecture.py - CubeDiffModel - base_unet name is up_blocks.2.attentions.2.transformer_blocks.0.ff.net.2.bias
architecture.py - CubeDiffModel - base_unet name is up_blocks.2.attentions.2.proj_out.weight
architecture.py - CubeDiffModel - base_unet name is up_blocks.2.attentions.2.proj_out.bias
architecture.py - CubeDiffModel - base_unet name is up_blocks.2.resnets.0.norm1.weight
architecture.py - CubeDiffModel - base_unet name is up_blocks.2.resnets.0.norm1.bias
architecture.py - CubeDiffModel - base_unet name is up_blocks.2.resnets.0.conv1.weight
architecture.py - CubeDiffModel - base_unet name is up_blocks.2.resnets.0.conv1.bias
architecture.py - CubeDiffModel - base_unet name is up_blocks.2.resnets.0.time_emb_proj.weight
architecture.py - CubeDiffModel - base_unet name is up_blocks.2.resnets.0.time_emb_proj.bias
architecture.py - CubeDiffModel - base_unet name is up_blocks.2.resnets.0.norm2.weight
architecture.py - CubeDiffModel - base_unet name is up_blocks.2.resnets.0.norm2.bias
architecture.py - CubeDiffModel - base_unet name is up_blocks.2.resnets.0.conv2.weight
architecture.py - CubeDiffModel - base_unet name is up_blocks.2.resnets.0.conv2.bias
architecture.py - CubeDiffModel - base_unet name is up_blocks.2.resnets.0.conv_shortcut.weight
architecture.py - CubeDiffModel - base_unet name is up_blocks.2.resnets.0.conv_shortcut.bias
architecture.py - CubeDiffModel - base_unet name is up_blocks.2.resnets.1.norm1.weight
architecture.py - CubeDiffModel - base_unet name is up_blocks.2.resnets.1.norm1.bias
architecture.py - CubeDiffModel - base_unet name is up_blocks.2.resnets.1.conv1.weight
architecture.py - CubeDiffModel - base_unet name is up_blocks.2.resnets.1.conv1.bias
architecture.py - CubeDiffModel - base_unet name is up_blocks.2.resnets.1.time_emb_proj.weight
architecture.py - CubeDiffModel - base_unet name is up_blocks.2.resnets.1.time_emb_proj.bias
architecture.py - CubeDiffModel - base_unet name is up_blocks.2.resnets.1.norm2.weight
architecture.py - CubeDiffModel - base_unet name is up_blocks.2.resnets.1.norm2.bias
architecture.py - CubeDiffModel - base_unet name is up_blocks.2.resnets.1.conv2.weight
architecture.py - CubeDiffModel - base_unet name is up_blocks.2.resnets.1.conv2.bias
architecture.py - CubeDiffModel - base_unet name is up_blocks.2.resnets.1.conv_shortcut.weight
architecture.py - CubeDiffModel - base_unet name is up_blocks.2.resnets.1.conv_shortcut.bias
architecture.py - CubeDiffModel - base_unet name is up_blocks.2.resnets.2.norm1.weight
architecture.py - CubeDiffModel - base_unet name is up_blocks.2.resnets.2.norm1.bias
architecture.py - CubeDiffModel - base_unet name is up_blocks.2.resnets.2.conv1.weight
architecture.py - CubeDiffModel - base_unet name is up_blocks.2.resnets.2.conv1.bias
architecture.py - CubeDiffModel - base_unet name is up_blocks.2.resnets.2.time_emb_proj.weight
architecture.py - CubeDiffModel - base_unet name is up_blocks.2.resnets.2.time_emb_proj.bias
architecture.py - CubeDiffModel - base_unet name is up_blocks.2.resnets.2.norm2.weight
architecture.py - CubeDiffModel - base_unet name is up_blocks.2.resnets.2.norm2.bias
architecture.py - CubeDiffModel - base_unet name is up_blocks.2.resnets.2.conv2.weight
architecture.py - CubeDiffModel - base_unet name is up_blocks.2.resnets.2.conv2.bias
architecture.py - CubeDiffModel - base_unet name is up_blocks.2.resnets.2.conv_shortcut.weight
architecture.py - CubeDiffModel - base_unet name is up_blocks.2.resnets.2.conv_shortcut.bias
architecture.py - CubeDiffModel - base_unet name is up_blocks.2.upsamplers.0.conv.weight
architecture.py - CubeDiffModel - base_unet name is up_blocks.2.upsamplers.0.conv.bias
architecture.py - CubeDiffModel - base_unet name is up_blocks.3.attentions.0.norm.weight
architecture.py - CubeDiffModel - base_unet name is up_blocks.3.attentions.0.norm.bias
architecture.py - CubeDiffModel - base_unet name is up_blocks.3.attentions.0.proj_in.weight
architecture.py - CubeDiffModel - base_unet name is up_blocks.3.attentions.0.proj_in.bias
architecture.py - CubeDiffModel - base_unet name is up_blocks.3.attentions.0.transformer_blocks.0.norm1.weight
architecture.py - CubeDiffModel - base_unet name is up_blocks.3.attentions.0.transformer_blocks.0.norm1.bias
architecture.py - CubeDiffModel - base_unet name is up_blocks.3.attentions.0.transformer_blocks.0.attn1.to_q.weight
architecture.py - CubeDiffModel - base_unet name is up_blocks.3.attentions.0.transformer_blocks.0.attn1.to_k.weight
architecture.py - CubeDiffModel - base_unet name is up_blocks.3.attentions.0.transformer_blocks.0.attn1.to_v.weight
architecture.py - CubeDiffModel - base_unet name is up_blocks.3.attentions.0.transformer_blocks.0.attn1.to_out.0.weight
architecture.py - CubeDiffModel - base_unet name is up_blocks.3.attentions.0.transformer_blocks.0.attn1.to_out.0.bias
architecture.py - CubeDiffModel - base_unet name is up_blocks.3.attentions.0.transformer_blocks.0.norm2.weight
architecture.py - CubeDiffModel - base_unet name is up_blocks.3.attentions.0.transformer_blocks.0.norm2.bias
architecture.py - CubeDiffModel - base_unet name is up_blocks.3.attentions.0.transformer_blocks.0.attn2.to_q.weight
architecture.py - CubeDiffModel - base_unet name is up_blocks.3.attentions.0.transformer_blocks.0.attn2.to_k.weight
architecture.py - CubeDiffModel - base_unet name is up_blocks.3.attentions.0.transformer_blocks.0.attn2.to_v.weight
architecture.py - CubeDiffModel - base_unet name is up_blocks.3.attentions.0.transformer_blocks.0.attn2.to_out.0.weight
architecture.py - CubeDiffModel - base_unet name is up_blocks.3.attentions.0.transformer_blocks.0.attn2.to_out.0.bias
architecture.py - CubeDiffModel - base_unet name is up_blocks.3.attentions.0.transformer_blocks.0.norm3.weight
architecture.py - CubeDiffModel - base_unet name is up_blocks.3.attentions.0.transformer_blocks.0.norm3.bias
architecture.py - CubeDiffModel - base_unet name is up_blocks.3.attentions.0.transformer_blocks.0.ff.net.0.proj.weight
architecture.py - CubeDiffModel - base_unet name is up_blocks.3.attentions.0.transformer_blocks.0.ff.net.0.proj.bias
architecture.py - CubeDiffModel - base_unet name is up_blocks.3.attentions.0.transformer_blocks.0.ff.net.2.weight
architecture.py - CubeDiffModel - base_unet name is up_blocks.3.attentions.0.transformer_blocks.0.ff.net.2.bias
architecture.py - CubeDiffModel - base_unet name is up_blocks.3.attentions.0.proj_out.weight
architecture.py - CubeDiffModel - base_unet name is up_blocks.3.attentions.0.proj_out.bias
architecture.py - CubeDiffModel - base_unet name is up_blocks.3.attentions.1.norm.weight
architecture.py - CubeDiffModel - base_unet name is up_blocks.3.attentions.1.norm.bias
architecture.py - CubeDiffModel - base_unet name is up_blocks.3.attentions.1.proj_in.weight
architecture.py - CubeDiffModel - base_unet name is up_blocks.3.attentions.1.proj_in.bias
architecture.py - CubeDiffModel - base_unet name is up_blocks.3.attentions.1.transformer_blocks.0.norm1.weight
architecture.py - CubeDiffModel - base_unet name is up_blocks.3.attentions.1.transformer_blocks.0.norm1.bias
architecture.py - CubeDiffModel - base_unet name is up_blocks.3.attentions.1.transformer_blocks.0.attn1.to_q.weight
architecture.py - CubeDiffModel - base_unet name is up_blocks.3.attentions.1.transformer_blocks.0.attn1.to_k.weight
architecture.py - CubeDiffModel - base_unet name is up_blocks.3.attentions.1.transformer_blocks.0.attn1.to_v.weight
architecture.py - CubeDiffModel - base_unet name is up_blocks.3.attentions.1.transformer_blocks.0.attn1.to_out.0.weight
architecture.py - CubeDiffModel - base_unet name is up_blocks.3.attentions.1.transformer_blocks.0.attn1.to_out.0.bias
architecture.py - CubeDiffModel - base_unet name is up_blocks.3.attentions.1.transformer_blocks.0.norm2.weight
architecture.py - CubeDiffModel - base_unet name is up_blocks.3.attentions.1.transformer_blocks.0.norm2.bias
architecture.py - CubeDiffModel - base_unet name is up_blocks.3.attentions.1.transformer_blocks.0.attn2.to_q.weight
architecture.py - CubeDiffModel - base_unet name is up_blocks.3.attentions.1.transformer_blocks.0.attn2.to_k.weight
architecture.py - CubeDiffModel - base_unet name is up_blocks.3.attentions.1.transformer_blocks.0.attn2.to_v.weight
architecture.py - CubeDiffModel - base_unet name is up_blocks.3.attentions.1.transformer_blocks.0.attn2.to_out.0.weight
architecture.py - CubeDiffModel - base_unet name is up_blocks.3.attentions.1.transformer_blocks.0.attn2.to_out.0.bias
architecture.py - CubeDiffModel - base_unet name is up_blocks.3.attentions.1.transformer_blocks.0.norm3.weight
architecture.py - CubeDiffModel - base_unet name is up_blocks.3.attentions.1.transformer_blocks.0.norm3.bias
architecture.py - CubeDiffModel - base_unet name is up_blocks.3.attentions.1.transformer_blocks.0.ff.net.0.proj.weight
architecture.py - CubeDiffModel - base_unet name is up_blocks.3.attentions.1.transformer_blocks.0.ff.net.0.proj.bias
architecture.py - CubeDiffModel - base_unet name is up_blocks.3.attentions.1.transformer_blocks.0.ff.net.2.weight
architecture.py - CubeDiffModel - base_unet name is up_blocks.3.attentions.1.transformer_blocks.0.ff.net.2.bias
architecture.py - CubeDiffModel - base_unet name is up_blocks.3.attentions.1.proj_out.weight
architecture.py - CubeDiffModel - base_unet name is up_blocks.3.attentions.1.proj_out.bias
architecture.py - CubeDiffModel - base_unet name is up_blocks.3.attentions.2.norm.weight
architecture.py - CubeDiffModel - base_unet name is up_blocks.3.attentions.2.norm.bias
architecture.py - CubeDiffModel - base_unet name is up_blocks.3.attentions.2.proj_in.weight
architecture.py - CubeDiffModel - base_unet name is up_blocks.3.attentions.2.proj_in.bias
architecture.py - CubeDiffModel - base_unet name is up_blocks.3.attentions.2.transformer_blocks.0.norm1.weight
architecture.py - CubeDiffModel - base_unet name is up_blocks.3.attentions.2.transformer_blocks.0.norm1.bias
architecture.py - CubeDiffModel - base_unet name is up_blocks.3.attentions.2.transformer_blocks.0.attn1.to_q.weight
architecture.py - CubeDiffModel - base_unet name is up_blocks.3.attentions.2.transformer_blocks.0.attn1.to_k.weight
architecture.py - CubeDiffModel - base_unet name is up_blocks.3.attentions.2.transformer_blocks.0.attn1.to_v.weight
architecture.py - CubeDiffModel - base_unet name is up_blocks.3.attentions.2.transformer_blocks.0.attn1.to_out.0.weight
architecture.py - CubeDiffModel - base_unet name is up_blocks.3.attentions.2.transformer_blocks.0.attn1.to_out.0.bias
architecture.py - CubeDiffModel - base_unet name is up_blocks.3.attentions.2.transformer_blocks.0.norm2.weight
architecture.py - CubeDiffModel - base_unet name is up_blocks.3.attentions.2.transformer_blocks.0.norm2.bias
architecture.py - CubeDiffModel - base_unet name is up_blocks.3.attentions.2.transformer_blocks.0.attn2.to_q.weight
architecture.py - CubeDiffModel - base_unet name is up_blocks.3.attentions.2.transformer_blocks.0.attn2.to_k.weight
architecture.py - CubeDiffModel - base_unet name is up_blocks.3.attentions.2.transformer_blocks.0.attn2.to_v.weight
architecture.py - CubeDiffModel - base_unet name is up_blocks.3.attentions.2.transformer_blocks.0.attn2.to_out.0.weight
architecture.py - CubeDiffModel - base_unet name is up_blocks.3.attentions.2.transformer_blocks.0.attn2.to_out.0.bias
architecture.py - CubeDiffModel - base_unet name is up_blocks.3.attentions.2.transformer_blocks.0.norm3.weight
architecture.py - CubeDiffModel - base_unet name is up_blocks.3.attentions.2.transformer_blocks.0.norm3.bias
architecture.py - CubeDiffModel - base_unet name is up_blocks.3.attentions.2.transformer_blocks.0.ff.net.0.proj.weight
architecture.py - CubeDiffModel - base_unet name is up_blocks.3.attentions.2.transformer_blocks.0.ff.net.0.proj.bias
architecture.py - CubeDiffModel - base_unet name is up_blocks.3.attentions.2.transformer_blocks.0.ff.net.2.weight
architecture.py - CubeDiffModel - base_unet name is up_blocks.3.attentions.2.transformer_blocks.0.ff.net.2.bias
architecture.py - CubeDiffModel - base_unet name is up_blocks.3.attentions.2.proj_out.weight
architecture.py - CubeDiffModel - base_unet name is up_blocks.3.attentions.2.proj_out.bias
architecture.py - CubeDiffModel - base_unet name is up_blocks.3.resnets.0.norm1.weight
architecture.py - CubeDiffModel - base_unet name is up_blocks.3.resnets.0.norm1.bias
architecture.py - CubeDiffModel - base_unet name is up_blocks.3.resnets.0.conv1.weight
architecture.py - CubeDiffModel - base_unet name is up_blocks.3.resnets.0.conv1.bias
architecture.py - CubeDiffModel - base_unet name is up_blocks.3.resnets.0.time_emb_proj.weight
architecture.py - CubeDiffModel - base_unet name is up_blocks.3.resnets.0.time_emb_proj.bias
architecture.py - CubeDiffModel - base_unet name is up_blocks.3.resnets.0.norm2.weight
architecture.py - CubeDiffModel - base_unet name is up_blocks.3.resnets.0.norm2.bias
architecture.py - CubeDiffModel - base_unet name is up_blocks.3.resnets.0.conv2.weight
architecture.py - CubeDiffModel - base_unet name is up_blocks.3.resnets.0.conv2.bias
architecture.py - CubeDiffModel - base_unet name is up_blocks.3.resnets.0.conv_shortcut.weight
architecture.py - CubeDiffModel - base_unet name is up_blocks.3.resnets.0.conv_shortcut.bias
architecture.py - CubeDiffModel - base_unet name is up_blocks.3.resnets.1.norm1.weight
architecture.py - CubeDiffModel - base_unet name is up_blocks.3.resnets.1.norm1.bias
architecture.py - CubeDiffModel - base_unet name is up_blocks.3.resnets.1.conv1.weight
architecture.py - CubeDiffModel - base_unet name is up_blocks.3.resnets.1.conv1.bias
architecture.py - CubeDiffModel - base_unet name is up_blocks.3.resnets.1.time_emb_proj.weight
architecture.py - CubeDiffModel - base_unet name is up_blocks.3.resnets.1.time_emb_proj.bias
architecture.py - CubeDiffModel - base_unet name is up_blocks.3.resnets.1.norm2.weight
architecture.py - CubeDiffModel - base_unet name is up_blocks.3.resnets.1.norm2.bias
architecture.py - CubeDiffModel - base_unet name is up_blocks.3.resnets.1.conv2.weight
architecture.py - CubeDiffModel - base_unet name is up_blocks.3.resnets.1.conv2.bias
architecture.py - CubeDiffModel - base_unet name is up_blocks.3.resnets.1.conv_shortcut.weight
architecture.py - CubeDiffModel - base_unet name is up_blocks.3.resnets.1.conv_shortcut.bias
architecture.py - CubeDiffModel - base_unet name is up_blocks.3.resnets.2.norm1.weight
architecture.py - CubeDiffModel - base_unet name is up_blocks.3.resnets.2.norm1.bias
architecture.py - CubeDiffModel - base_unet name is up_blocks.3.resnets.2.conv1.weight
architecture.py - CubeDiffModel - base_unet name is up_blocks.3.resnets.2.conv1.bias
architecture.py - CubeDiffModel - base_unet name is up_blocks.3.resnets.2.time_emb_proj.weight
architecture.py - CubeDiffModel - base_unet name is up_blocks.3.resnets.2.time_emb_proj.bias
architecture.py - CubeDiffModel - base_unet name is up_blocks.3.resnets.2.norm2.weight
architecture.py - CubeDiffModel - base_unet name is up_blocks.3.resnets.2.norm2.bias
architecture.py - CubeDiffModel - base_unet name is up_blocks.3.resnets.2.conv2.weight
architecture.py - CubeDiffModel - base_unet name is up_blocks.3.resnets.2.conv2.bias
architecture.py - CubeDiffModel - base_unet name is up_blocks.3.resnets.2.conv_shortcut.weight
architecture.py - CubeDiffModel - base_unet name is up_blocks.3.resnets.2.conv_shortcut.bias
architecture.py - CubeDiffModel - base_unet name is mid_block.attentions.0.norm.weight
architecture.py - CubeDiffModel - base_unet name is mid_block.attentions.0.norm.bias
architecture.py - CubeDiffModel - base_unet name is mid_block.attentions.0.proj_in.weight
architecture.py - CubeDiffModel - base_unet name is mid_block.attentions.0.proj_in.bias
architecture.py - CubeDiffModel - base_unet name is mid_block.attentions.0.transformer_blocks.0.norm1.weight
architecture.py - CubeDiffModel - base_unet name is mid_block.attentions.0.transformer_blocks.0.norm1.bias
architecture.py - CubeDiffModel - base_unet name is mid_block.attentions.0.transformer_blocks.0.attn1.to_q.weight
architecture.py - CubeDiffModel - base_unet name is mid_block.attentions.0.transformer_blocks.0.attn1.to_k.weight
architecture.py - CubeDiffModel - base_unet name is mid_block.attentions.0.transformer_blocks.0.attn1.to_v.weight
architecture.py - CubeDiffModel - base_unet name is mid_block.attentions.0.transformer_blocks.0.attn1.to_out.0.weight
architecture.py - CubeDiffModel - base_unet name is mid_block.attentions.0.transformer_blocks.0.attn1.to_out.0.bias
architecture.py - CubeDiffModel - base_unet name is mid_block.attentions.0.transformer_blocks.0.norm2.weight
architecture.py - CubeDiffModel - base_unet name is mid_block.attentions.0.transformer_blocks.0.norm2.bias
architecture.py - CubeDiffModel - base_unet name is mid_block.attentions.0.transformer_blocks.0.attn2.to_q.weight
architecture.py - CubeDiffModel - base_unet name is mid_block.attentions.0.transformer_blocks.0.attn2.to_k.weight
architecture.py - CubeDiffModel - base_unet name is mid_block.attentions.0.transformer_blocks.0.attn2.to_v.weight
architecture.py - CubeDiffModel - base_unet name is mid_block.attentions.0.transformer_blocks.0.attn2.to_out.0.weight
architecture.py - CubeDiffModel - base_unet name is mid_block.attentions.0.transformer_blocks.0.attn2.to_out.0.bias
architecture.py - CubeDiffModel - base_unet name is mid_block.attentions.0.transformer_blocks.0.norm3.weight
architecture.py - CubeDiffModel - base_unet name is mid_block.attentions.0.transformer_blocks.0.norm3.bias
architecture.py - CubeDiffModel - base_unet name is mid_block.attentions.0.transformer_blocks.0.ff.net.0.proj.weight
architecture.py - CubeDiffModel - base_unet name is mid_block.attentions.0.transformer_blocks.0.ff.net.0.proj.bias
architecture.py - CubeDiffModel - base_unet name is mid_block.attentions.0.transformer_blocks.0.ff.net.2.weight
architecture.py - CubeDiffModel - base_unet name is mid_block.attentions.0.transformer_blocks.0.ff.net.2.bias
architecture.py - CubeDiffModel - base_unet name is mid_block.attentions.0.proj_out.weight
architecture.py - CubeDiffModel - base_unet name is mid_block.attentions.0.proj_out.bias
architecture.py - CubeDiffModel - base_unet name is mid_block.resnets.0.norm1.weight
architecture.py - CubeDiffModel - base_unet name is mid_block.resnets.0.norm1.bias
architecture.py - CubeDiffModel - base_unet name is mid_block.resnets.0.conv1.weight
architecture.py - CubeDiffModel - base_unet name is mid_block.resnets.0.conv1.bias
architecture.py - CubeDiffModel - base_unet name is mid_block.resnets.0.time_emb_proj.weight
architecture.py - CubeDiffModel - base_unet name is mid_block.resnets.0.time_emb_proj.bias
architecture.py - CubeDiffModel - base_unet name is mid_block.resnets.0.norm2.weight
architecture.py - CubeDiffModel - base_unet name is mid_block.resnets.0.norm2.bias
architecture.py - CubeDiffModel - base_unet name is mid_block.resnets.0.conv2.weight
architecture.py - CubeDiffModel - base_unet name is mid_block.resnets.0.conv2.bias
architecture.py - CubeDiffModel - base_unet name is mid_block.resnets.1.norm1.weight
architecture.py - CubeDiffModel - base_unet name is mid_block.resnets.1.norm1.bias
architecture.py - CubeDiffModel - base_unet name is mid_block.resnets.1.conv1.weight
architecture.py - CubeDiffModel - base_unet name is mid_block.resnets.1.conv1.bias
architecture.py - CubeDiffModel - base_unet name is mid_block.resnets.1.time_emb_proj.weight
architecture.py - CubeDiffModel - base_unet name is mid_block.resnets.1.time_emb_proj.bias
architecture.py - CubeDiffModel - base_unet name is mid_block.resnets.1.norm2.weight
architecture.py - CubeDiffModel - base_unet name is mid_block.resnets.1.norm2.bias
architecture.py - CubeDiffModel - base_unet name is mid_block.resnets.1.conv2.weight
architecture.py - CubeDiffModel - base_unet name is mid_block.resnets.1.conv2.bias
architecture.py - CubeDiffModel - base_unet name is conv_norm_out.weight
architecture.py - CubeDiffModel - base_unet name is conv_norm_out.bias
architecture.py - CubeDiffModel - base_unet name is conv_out.weight
architecture.py - CubeDiffModel - base_unet name is conv_out.bias
architecture.py - CubeDiffModel - Inflating down_blocks.0.attentions.0.transformer_blocks.0.attn1
attention.py - skip_copy is False - ‚úÖ Inflating attention @ Attention,               copying pretrained q‚Äêweight shape (320, 320)
attention.py - Detected self-attention: query_dim=320, ctx_dim=320
architecture.py - CubeDiffModel - Replaced base_unet down_blocks.0.attentions.0.transformer_blocks.0.attn1 with inflated_layer
architecture.py - CubeDiffModel - Inflating down_blocks.0.attentions.0.transformer_blocks.0.attn2
attention.py - skip_copy is False - ‚úÖ Inflating attention @ Attention,               copying pretrained q‚Äêweight shape (320, 320)
attention.py - Detected cross-attention: query_dim=320, ctx_dim=768
architecture.py - CubeDiffModel - Replaced base_unet down_blocks.0.attentions.0.transformer_blocks.0.attn2 with inflated_layer
architecture.py - CubeDiffModel - Inflating down_blocks.0.attentions.1.transformer_blocks.0.attn1
attention.py - skip_copy is False - ‚úÖ Inflating attention @ Attention,               copying pretrained q‚Äêweight shape (320, 320)
attention.py - Detected self-attention: query_dim=320, ctx_dim=320
architecture.py - CubeDiffModel - Replaced base_unet down_blocks.0.attentions.1.transformer_blocks.0.attn1 with inflated_layer
architecture.py - CubeDiffModel - Inflating down_blocks.0.attentions.1.transformer_blocks.0.attn2
attention.py - skip_copy is False - ‚úÖ Inflating attention @ Attention,               copying pretrained q‚Äêweight shape (320, 320)
attention.py - Detected cross-attention: query_dim=320, ctx_dim=768
architecture.py - CubeDiffModel - Replaced base_unet down_blocks.0.attentions.1.transformer_blocks.0.attn2 with inflated_layer
architecture.py - CubeDiffModel - Inflating down_blocks.1.attentions.0.transformer_blocks.0.attn1
attention.py - skip_copy is False - ‚úÖ Inflating attention @ Attention,               copying pretrained q‚Äêweight shape (640, 640)
attention.py - Detected self-attention: query_dim=640, ctx_dim=640
architecture.py - CubeDiffModel - Replaced base_unet down_blocks.1.attentions.0.transformer_blocks.0.attn1 with inflated_layer
architecture.py - CubeDiffModel - Inflating down_blocks.1.attentions.0.transformer_blocks.0.attn2
attention.py - skip_copy is False - ‚úÖ Inflating attention @ Attention,               copying pretrained q‚Äêweight shape (640, 640)
attention.py - Detected cross-attention: query_dim=640, ctx_dim=768
architecture.py - CubeDiffModel - Replaced base_unet down_blocks.1.attentions.0.transformer_blocks.0.attn2 with inflated_layer
architecture.py - CubeDiffModel - Inflating down_blocks.1.attentions.1.transformer_blocks.0.attn1
attention.py - skip_copy is False - ‚úÖ Inflating attention @ Attention,               copying pretrained q‚Äêweight shape (640, 640)
attention.py - Detected self-attention: query_dim=640, ctx_dim=640
architecture.py - CubeDiffModel - Replaced base_unet down_blocks.1.attentions.1.transformer_blocks.0.attn1 with inflated_layer
architecture.py - CubeDiffModel - Inflating down_blocks.1.attentions.1.transformer_blocks.0.attn2
attention.py - skip_copy is False - ‚úÖ Inflating attention @ Attention,               copying pretrained q‚Äêweight shape (640, 640)
attention.py - Detected cross-attention: query_dim=640, ctx_dim=768
architecture.py - CubeDiffModel - Replaced base_unet down_blocks.1.attentions.1.transformer_blocks.0.attn2 with inflated_layer
architecture.py - CubeDiffModel - Inflating down_blocks.2.attentions.0.transformer_blocks.0.attn1
attention.py - skip_copy is False - ‚úÖ Inflating attention @ Attention,               copying pretrained q‚Äêweight shape (1280, 1280)
attention.py - Detected self-attention: query_dim=1280, ctx_dim=1280
architecture.py - CubeDiffModel - Replaced base_unet down_blocks.2.attentions.0.transformer_blocks.0.attn1 with inflated_layer
architecture.py - CubeDiffModel - Inflating down_blocks.2.attentions.0.transformer_blocks.0.attn2
attention.py - skip_copy is False - ‚úÖ Inflating attention @ Attention,               copying pretrained q‚Äêweight shape (1280, 1280)
attention.py - Detected cross-attention: query_dim=1280, ctx_dim=768
architecture.py - CubeDiffModel - Replaced base_unet down_blocks.2.attentions.0.transformer_blocks.0.attn2 with inflated_layer
architecture.py - CubeDiffModel - Inflating down_blocks.2.attentions.1.transformer_blocks.0.attn1
attention.py - skip_copy is False - ‚úÖ Inflating attention @ Attention,               copying pretrained q‚Äêweight shape (1280, 1280)
attention.py - Detected self-attention: query_dim=1280, ctx_dim=1280
architecture.py - CubeDiffModel - Replaced base_unet down_blocks.2.attentions.1.transformer_blocks.0.attn1 with inflated_layer
architecture.py - CubeDiffModel - Inflating down_blocks.2.attentions.1.transformer_blocks.0.attn2
attention.py - skip_copy is False - ‚úÖ Inflating attention @ Attention,               copying pretrained q‚Äêweight shape (1280, 1280)
attention.py - Detected cross-attention: query_dim=1280, ctx_dim=768
architecture.py - CubeDiffModel - Replaced base_unet down_blocks.2.attentions.1.transformer_blocks.0.attn2 with inflated_layer
architecture.py - CubeDiffModel - Inflating up_blocks.1.attentions.0.transformer_blocks.0.attn1
attention.py - skip_copy is False - ‚úÖ Inflating attention @ Attention,               copying pretrained q‚Äêweight shape (1280, 1280)
attention.py - Detected self-attention: query_dim=1280, ctx_dim=1280
architecture.py - CubeDiffModel - Replaced base_unet up_blocks.1.attentions.0.transformer_blocks.0.attn1 with inflated_layer
architecture.py - CubeDiffModel - Inflating up_blocks.1.attentions.0.transformer_blocks.0.attn2
attention.py - skip_copy is False - ‚úÖ Inflating attention @ Attention,               copying pretrained q‚Äêweight shape (1280, 1280)
attention.py - Detected cross-attention: query_dim=1280, ctx_dim=768
architecture.py - CubeDiffModel - Replaced base_unet up_blocks.1.attentions.0.transformer_blocks.0.attn2 with inflated_layer
architecture.py - CubeDiffModel - Inflating up_blocks.1.attentions.1.transformer_blocks.0.attn1
attention.py - skip_copy is False - ‚úÖ Inflating attention @ Attention,               copying pretrained q‚Äêweight shape (1280, 1280)
attention.py - Detected self-attention: query_dim=1280, ctx_dim=1280
architecture.py - CubeDiffModel - Replaced base_unet up_blocks.1.attentions.1.transformer_blocks.0.attn1 with inflated_layer
architecture.py - CubeDiffModel - Inflating up_blocks.1.attentions.1.transformer_blocks.0.attn2
attention.py - skip_copy is False - ‚úÖ Inflating attention @ Attention,               copying pretrained q‚Äêweight shape (1280, 1280)
attention.py - Detected cross-attention: query_dim=1280, ctx_dim=768
architecture.py - CubeDiffModel - Replaced base_unet up_blocks.1.attentions.1.transformer_blocks.0.attn2 with inflated_layer
architecture.py - CubeDiffModel - Inflating up_blocks.1.attentions.2.transformer_blocks.0.attn1
attention.py - skip_copy is False - ‚úÖ Inflating attention @ Attention,               copying pretrained q‚Äêweight shape (1280, 1280)
attention.py - Detected self-attention: query_dim=1280, ctx_dim=1280
architecture.py - CubeDiffModel - Replaced base_unet up_blocks.1.attentions.2.transformer_blocks.0.attn1 with inflated_layer
architecture.py - CubeDiffModel - Inflating up_blocks.1.attentions.2.transformer_blocks.0.attn2
attention.py - skip_copy is False - ‚úÖ Inflating attention @ Attention,               copying pretrained q‚Äêweight shape (1280, 1280)
attention.py - Detected cross-attention: query_dim=1280, ctx_dim=768
architecture.py - CubeDiffModel - Replaced base_unet up_blocks.1.attentions.2.transformer_blocks.0.attn2 with inflated_layer
architecture.py - CubeDiffModel - Inflating up_blocks.2.attentions.0.transformer_blocks.0.attn1
attention.py - skip_copy is False - ‚úÖ Inflating attention @ Attention,               copying pretrained q‚Äêweight shape (640, 640)
attention.py - Detected self-attention: query_dim=640, ctx_dim=640
architecture.py - CubeDiffModel - Replaced base_unet up_blocks.2.attentions.0.transformer_blocks.0.attn1 with inflated_layer
architecture.py - CubeDiffModel - Inflating up_blocks.2.attentions.0.transformer_blocks.0.attn2
attention.py - skip_copy is False - ‚úÖ Inflating attention @ Attention,               copying pretrained q‚Äêweight shape (640, 640)
attention.py - Detected cross-attention: query_dim=640, ctx_dim=768
architecture.py - CubeDiffModel - Replaced base_unet up_blocks.2.attentions.0.transformer_blocks.0.attn2 with inflated_layer
architecture.py - CubeDiffModel - Inflating up_blocks.2.attentions.1.transformer_blocks.0.attn1
attention.py - skip_copy is False - ‚úÖ Inflating attention @ Attention,               copying pretrained q‚Äêweight shape (640, 640)
attention.py - Detected self-attention: query_dim=640, ctx_dim=640
architecture.py - CubeDiffModel - Replaced base_unet up_blocks.2.attentions.1.transformer_blocks.0.attn1 with inflated_layer
architecture.py - CubeDiffModel - Inflating up_blocks.2.attentions.1.transformer_blocks.0.attn2
attention.py - skip_copy is False - ‚úÖ Inflating attention @ Attention,               copying pretrained q‚Äêweight shape (640, 640)
attention.py - Detected cross-attention: query_dim=640, ctx_dim=768
architecture.py - CubeDiffModel - Replaced base_unet up_blocks.2.attentions.1.transformer_blocks.0.attn2 with inflated_layer
architecture.py - CubeDiffModel - Inflating up_blocks.2.attentions.2.transformer_blocks.0.attn1
attention.py - skip_copy is False - ‚úÖ Inflating attention @ Attention,               copying pretrained q‚Äêweight shape (640, 640)
attention.py - Detected self-attention: query_dim=640, ctx_dim=640
architecture.py - CubeDiffModel - Replaced base_unet up_blocks.2.attentions.2.transformer_blocks.0.attn1 with inflated_layer
architecture.py - CubeDiffModel - Inflating up_blocks.2.attentions.2.transformer_blocks.0.attn2
attention.py - skip_copy is False - ‚úÖ Inflating attention @ Attention,               copying pretrained q‚Äêweight shape (640, 640)
attention.py - Detected cross-attention: query_dim=640, ctx_dim=768
architecture.py - CubeDiffModel - Replaced base_unet up_blocks.2.attentions.2.transformer_blocks.0.attn2 with inflated_layer
architecture.py - CubeDiffModel - Inflating up_blocks.3.attentions.0.transformer_blocks.0.attn1
attention.py - skip_copy is False - ‚úÖ Inflating attention @ Attention,               copying pretrained q‚Äêweight shape (320, 320)
attention.py - Detected self-attention: query_dim=320, ctx_dim=320
architecture.py - CubeDiffModel - Replaced base_unet up_blocks.3.attentions.0.transformer_blocks.0.attn1 with inflated_layer
architecture.py - CubeDiffModel - Inflating up_blocks.3.attentions.0.transformer_blocks.0.attn2
attention.py - skip_copy is False - ‚úÖ Inflating attention @ Attention,               copying pretrained q‚Äêweight shape (320, 320)
attention.py - Detected cross-attention: query_dim=320, ctx_dim=768
architecture.py - CubeDiffModel - Replaced base_unet up_blocks.3.attentions.0.transformer_blocks.0.attn2 with inflated_layer
architecture.py - CubeDiffModel - Inflating up_blocks.3.attentions.1.transformer_blocks.0.attn1
attention.py - skip_copy is False - ‚úÖ Inflating attention @ Attention,               copying pretrained q‚Äêweight shape (320, 320)
attention.py - Detected self-attention: query_dim=320, ctx_dim=320
architecture.py - CubeDiffModel - Replaced base_unet up_blocks.3.attentions.1.transformer_blocks.0.attn1 with inflated_layer
architecture.py - CubeDiffModel - Inflating up_blocks.3.attentions.1.transformer_blocks.0.attn2
attention.py - skip_copy is False - ‚úÖ Inflating attention @ Attention,               copying pretrained q‚Äêweight shape (320, 320)
attention.py - Detected cross-attention: query_dim=320, ctx_dim=768
architecture.py - CubeDiffModel - Replaced base_unet up_blocks.3.attentions.1.transformer_blocks.0.attn2 with inflated_layer
architecture.py - CubeDiffModel - Inflating up_blocks.3.attentions.2.transformer_blocks.0.attn1
attention.py - skip_copy is False - ‚úÖ Inflating attention @ Attention,               copying pretrained q‚Äêweight shape (320, 320)
attention.py - Detected self-attention: query_dim=320, ctx_dim=320
architecture.py - CubeDiffModel - Replaced base_unet up_blocks.3.attentions.2.transformer_blocks.0.attn1 with inflated_layer
architecture.py - CubeDiffModel - Inflating up_blocks.3.attentions.2.transformer_blocks.0.attn2
attention.py - skip_copy is False - ‚úÖ Inflating attention @ Attention,               copying pretrained q‚Äêweight shape (320, 320)
attention.py - Detected cross-attention: query_dim=320, ctx_dim=768
architecture.py - CubeDiffModel - Replaced base_unet up_blocks.3.attentions.2.transformer_blocks.0.attn2 with inflated_layer
architecture.py - CubeDiffModel - Inflating mid_block.attentions.0.transformer_blocks.0.attn1
attention.py - skip_copy is False - ‚úÖ Inflating attention @ Attention,               copying pretrained q‚Äêweight shape (1280, 1280)
attention.py - Detected self-attention: query_dim=1280, ctx_dim=1280
architecture.py - CubeDiffModel - Replaced base_unet mid_block.attentions.0.transformer_blocks.0.attn1 with inflated_layer
architecture.py - CubeDiffModel - Inflating mid_block.attentions.0.transformer_blocks.0.attn2
attention.py - skip_copy is False - ‚úÖ Inflating attention @ Attention,               copying pretrained q‚Äêweight shape (1280, 1280)
attention.py - Detected cross-attention: query_dim=1280, ctx_dim=768
architecture.py - CubeDiffModel - Replaced base_unet mid_block.attentions.0.transformer_blocks.0.attn2 with inflated_layer
architecture.py - CubeDiffModel - ___init__ - [DEBUG] total_inflated_params: 93536640 (93.5366 million)
architecture.py - CubeDiffModel - Inflated layer module: down_blocks.0.attentions.0.transformer_blocks.0.attn1
architecture.py - CubeDiffModel - Inflated layer module: down_blocks.0.attentions.0.transformer_blocks.0.attn2
architecture.py - CubeDiffModel - Inflated layer module: down_blocks.0.attentions.1.transformer_blocks.0.attn1
architecture.py - CubeDiffModel - Inflated layer module: down_blocks.0.attentions.1.transformer_blocks.0.attn2
architecture.py - CubeDiffModel - Inflated layer module: down_blocks.1.attentions.0.transformer_blocks.0.attn1
architecture.py - CubeDiffModel - Inflated layer module: down_blocks.1.attentions.0.transformer_blocks.0.attn2
architecture.py - CubeDiffModel - Inflated layer module: down_blocks.1.attentions.1.transformer_blocks.0.attn1
architecture.py - CubeDiffModel - Inflated layer module: down_blocks.1.attentions.1.transformer_blocks.0.attn2
architecture.py - CubeDiffModel - Inflated layer module: down_blocks.2.attentions.0.transformer_blocks.0.attn1
architecture.py - CubeDiffModel - Inflated layer module: down_blocks.2.attentions.0.transformer_blocks.0.attn2
architecture.py - CubeDiffModel - Inflated layer module: down_blocks.2.attentions.1.transformer_blocks.0.attn1
architecture.py - CubeDiffModel - Inflated layer module: down_blocks.2.attentions.1.transformer_blocks.0.attn2
architecture.py - CubeDiffModel - Inflated layer module: up_blocks.1.attentions.0.transformer_blocks.0.attn1
architecture.py - CubeDiffModel - Inflated layer module: up_blocks.1.attentions.0.transformer_blocks.0.attn2
architecture.py - CubeDiffModel - Inflated layer module: up_blocks.1.attentions.1.transformer_blocks.0.attn1
architecture.py - CubeDiffModel - Inflated layer module: up_blocks.1.attentions.1.transformer_blocks.0.attn2
architecture.py - CubeDiffModel - Inflated layer module: up_blocks.1.attentions.2.transformer_blocks.0.attn1
architecture.py - CubeDiffModel - Inflated layer module: up_blocks.1.attentions.2.transformer_blocks.0.attn2
architecture.py - CubeDiffModel - Inflated layer module: up_blocks.2.attentions.0.transformer_blocks.0.attn1
architecture.py - CubeDiffModel - Inflated layer module: up_blocks.2.attentions.0.transformer_blocks.0.attn2
architecture.py - CubeDiffModel - Inflated layer module: up_blocks.2.attentions.1.transformer_blocks.0.attn1
architecture.py - CubeDiffModel - Inflated layer module: up_blocks.2.attentions.1.transformer_blocks.0.attn2
architecture.py - CubeDiffModel - Inflated layer module: up_blocks.2.attentions.2.transformer_blocks.0.attn1
architecture.py - CubeDiffModel - Inflated layer module: up_blocks.2.attentions.2.transformer_blocks.0.attn2
architecture.py - CubeDiffModel - Inflated layer module: up_blocks.3.attentions.0.transformer_blocks.0.attn1
architecture.py - CubeDiffModel - Inflated layer module: up_blocks.3.attentions.0.transformer_blocks.0.attn2
architecture.py - CubeDiffModel - Inflated layer module: up_blocks.3.attentions.1.transformer_blocks.0.attn1
architecture.py - CubeDiffModel - Inflated layer module: up_blocks.3.attentions.1.transformer_blocks.0.attn2
architecture.py - CubeDiffModel - Inflated layer module: up_blocks.3.attentions.2.transformer_blocks.0.attn1
architecture.py - CubeDiffModel - Inflated layer module: up_blocks.3.attentions.2.transformer_blocks.0.attn2
architecture.py - CubeDiffModel - Inflated layer module: mid_block.attentions.0.transformer_blocks.0.attn1
architecture.py - CubeDiffModel - Inflated layer module: mid_block.attentions.0.transformer_blocks.0.attn2

=== Inflated-Attention Layers ===
archtecture.py - __init__  - unet -  ‚úÖ down_blocks.0.attentions.0.transformer_blocks.0.attn1: InflatedAttention(
  (to_q): Linear(in_features=320, out_features=320, bias=False)
  (to_k): Linear(in_features=320, out_features=320, bias=False)
  (to_v): Linear(in_features=320, out_features=320, bias=False)
  (to_out): Sequential(
    (0): Linear(in_features=320, out_features=320, bias=True)
    (1): Dropout(p=0.0, inplace=False)
  )
)
archtecture.py - __init__  - unet -  ‚úÖ down_blocks.0.attentions.0.transformer_blocks.0.attn2: InflatedAttention(
  (to_q): Linear(in_features=320, out_features=320, bias=False)
  (to_k): Linear(in_features=768, out_features=320, bias=False)
  (to_v): Linear(in_features=768, out_features=320, bias=False)
  (to_out): Sequential(
    (0): Linear(in_features=320, out_features=320, bias=True)
    (1): Dropout(p=0.0, inplace=False)
  )
)
archtecture.py - __init__  - unet -  ‚úÖ down_blocks.0.attentions.1.transformer_blocks.0.attn1: InflatedAttention(
  (to_q): Linear(in_features=320, out_features=320, bias=False)
  (to_k): Linear(in_features=320, out_features=320, bias=False)
  (to_v): Linear(in_features=320, out_features=320, bias=False)
  (to_out): Sequential(
    (0): Linear(in_features=320, out_features=320, bias=True)
    (1): Dropout(p=0.0, inplace=False)
  )
)
archtecture.py - __init__  - unet -  ‚úÖ down_blocks.0.attentions.1.transformer_blocks.0.attn2: InflatedAttention(
  (to_q): Linear(in_features=320, out_features=320, bias=False)
  (to_k): Linear(in_features=768, out_features=320, bias=False)
  (to_v): Linear(in_features=768, out_features=320, bias=False)
  (to_out): Sequential(
    (0): Linear(in_features=320, out_features=320, bias=True)
    (1): Dropout(p=0.0, inplace=False)
  )
)
archtecture.py - __init__  - unet -  ‚úÖ down_blocks.1.attentions.0.transformer_blocks.0.attn1: InflatedAttention(
  (to_q): Linear(in_features=640, out_features=640, bias=False)
  (to_k): Linear(in_features=640, out_features=640, bias=False)
  (to_v): Linear(in_features=640, out_features=640, bias=False)
  (to_out): Sequential(
    (0): Linear(in_features=640, out_features=640, bias=True)
    (1): Dropout(p=0.0, inplace=False)
  )
)
archtecture.py - __init__  - unet -  ‚úÖ down_blocks.1.attentions.0.transformer_blocks.0.attn2: InflatedAttention(
  (to_q): Linear(in_features=640, out_features=640, bias=False)
  (to_k): Linear(in_features=768, out_features=640, bias=False)
  (to_v): Linear(in_features=768, out_features=640, bias=False)
  (to_out): Sequential(
    (0): Linear(in_features=640, out_features=640, bias=True)
    (1): Dropout(p=0.0, inplace=False)
  )
)
archtecture.py - __init__  - unet -  ‚úÖ down_blocks.1.attentions.1.transformer_blocks.0.attn1: InflatedAttention(
  (to_q): Linear(in_features=640, out_features=640, bias=False)
  (to_k): Linear(in_features=640, out_features=640, bias=False)
  (to_v): Linear(in_features=640, out_features=640, bias=False)
  (to_out): Sequential(
    (0): Linear(in_features=640, out_features=640, bias=True)
    (1): Dropout(p=0.0, inplace=False)
  )
)
archtecture.py - __init__  - unet -  ‚úÖ down_blocks.1.attentions.1.transformer_blocks.0.attn2: InflatedAttention(
  (to_q): Linear(in_features=640, out_features=640, bias=False)
  (to_k): Linear(in_features=768, out_features=640, bias=False)
  (to_v): Linear(in_features=768, out_features=640, bias=False)
  (to_out): Sequential(
    (0): Linear(in_features=640, out_features=640, bias=True)
    (1): Dropout(p=0.0, inplace=False)
  )
)
archtecture.py - __init__  - unet -  ‚úÖ down_blocks.2.attentions.0.transformer_blocks.0.attn1: InflatedAttention(
  (to_q): Linear(in_features=1280, out_features=1280, bias=False)
  (to_k): Linear(in_features=1280, out_features=1280, bias=False)
  (to_v): Linear(in_features=1280, out_features=1280, bias=False)
  (to_out): Sequential(
    (0): Linear(in_features=1280, out_features=1280, bias=True)
    (1): Dropout(p=0.0, inplace=False)
  )
)
archtecture.py - __init__  - unet -  ‚úÖ down_blocks.2.attentions.0.transformer_blocks.0.attn2: InflatedAttention(
  (to_q): Linear(in_features=1280, out_features=1280, bias=False)
  (to_k): Linear(in_features=768, out_features=1280, bias=False)
  (to_v): Linear(in_features=768, out_features=1280, bias=False)
  (to_out): Sequential(
    (0): Linear(in_features=1280, out_features=1280, bias=True)
    (1): Dropout(p=0.0, inplace=False)
  )
)
archtecture.py - __init__  - unet -  ‚úÖ down_blocks.2.attentions.1.transformer_blocks.0.attn1: InflatedAttention(
  (to_q): Linear(in_features=1280, out_features=1280, bias=False)
  (to_k): Linear(in_features=1280, out_features=1280, bias=False)
  (to_v): Linear(in_features=1280, out_features=1280, bias=False)
  (to_out): Sequential(
    (0): Linear(in_features=1280, out_features=1280, bias=True)
    (1): Dropout(p=0.0, inplace=False)
  )
)
archtecture.py - __init__  - unet -  ‚úÖ down_blocks.2.attentions.1.transformer_blocks.0.attn2: InflatedAttention(
  (to_q): Linear(in_features=1280, out_features=1280, bias=False)
  (to_k): Linear(in_features=768, out_features=1280, bias=False)
  (to_v): Linear(in_features=768, out_features=1280, bias=False)
  (to_out): Sequential(
    (0): Linear(in_features=1280, out_features=1280, bias=True)
    (1): Dropout(p=0.0, inplace=False)
  )
)
archtecture.py - __init__  - unet -  ‚úÖ up_blocks.1.attentions.0.transformer_blocks.0.attn1: InflatedAttention(
  (to_q): Linear(in_features=1280, out_features=1280, bias=False)
  (to_k): Linear(in_features=1280, out_features=1280, bias=False)
  (to_v): Linear(in_features=1280, out_features=1280, bias=False)
  (to_out): Sequential(
    (0): Linear(in_features=1280, out_features=1280, bias=True)
    (1): Dropout(p=0.0, inplace=False)
  )
)
archtecture.py - __init__  - unet -  ‚úÖ up_blocks.1.attentions.0.transformer_blocks.0.attn2: InflatedAttention(
  (to_q): Linear(in_features=1280, out_features=1280, bias=False)
  (to_k): Linear(in_features=768, out_features=1280, bias=False)
  (to_v): Linear(in_features=768, out_features=1280, bias=False)
  (to_out): Sequential(
    (0): Linear(in_features=1280, out_features=1280, bias=True)
    (1): Dropout(p=0.0, inplace=False)
  )
)
archtecture.py - __init__  - unet -  ‚úÖ up_blocks.1.attentions.1.transformer_blocks.0.attn1: InflatedAttention(
  (to_q): Linear(in_features=1280, out_features=1280, bias=False)
  (to_k): Linear(in_features=1280, out_features=1280, bias=False)
  (to_v): Linear(in_features=1280, out_features=1280, bias=False)
  (to_out): Sequential(
    (0): Linear(in_features=1280, out_features=1280, bias=True)
    (1): Dropout(p=0.0, inplace=False)
  )
)
archtecture.py - __init__  - unet -  ‚úÖ up_blocks.1.attentions.1.transformer_blocks.0.attn2: InflatedAttention(
  (to_q): Linear(in_features=1280, out_features=1280, bias=False)
  (to_k): Linear(in_features=768, out_features=1280, bias=False)
  (to_v): Linear(in_features=768, out_features=1280, bias=False)
  (to_out): Sequential(
    (0): Linear(in_features=1280, out_features=1280, bias=True)
    (1): Dropout(p=0.0, inplace=False)
  )
)
archtecture.py - __init__  - unet -  ‚úÖ up_blocks.1.attentions.2.transformer_blocks.0.attn1: InflatedAttention(
  (to_q): Linear(in_features=1280, out_features=1280, bias=False)
  (to_k): Linear(in_features=1280, out_features=1280, bias=False)
  (to_v): Linear(in_features=1280, out_features=1280, bias=False)
  (to_out): Sequential(
    (0): Linear(in_features=1280, out_features=1280, bias=True)
    (1): Dropout(p=0.0, inplace=False)
  )
)
archtecture.py - __init__  - unet -  ‚úÖ up_blocks.1.attentions.2.transformer_blocks.0.attn2: InflatedAttention(
  (to_q): Linear(in_features=1280, out_features=1280, bias=False)
  (to_k): Linear(in_features=768, out_features=1280, bias=False)
  (to_v): Linear(in_features=768, out_features=1280, bias=False)
  (to_out): Sequential(
    (0): Linear(in_features=1280, out_features=1280, bias=True)
    (1): Dropout(p=0.0, inplace=False)
  )
)
archtecture.py - __init__  - unet -  ‚úÖ up_blocks.2.attentions.0.transformer_blocks.0.attn1: InflatedAttention(
  (to_q): Linear(in_features=640, out_features=640, bias=False)
  (to_k): Linear(in_features=640, out_features=640, bias=False)
  (to_v): Linear(in_features=640, out_features=640, bias=False)
  (to_out): Sequential(
    (0): Linear(in_features=640, out_features=640, bias=True)
    (1): Dropout(p=0.0, inplace=False)
  )
)
archtecture.py - __init__  - unet -  ‚úÖ up_blocks.2.attentions.0.transformer_blocks.0.attn2: InflatedAttention(
  (to_q): Linear(in_features=640, out_features=640, bias=False)
  (to_k): Linear(in_features=768, out_features=640, bias=False)
  (to_v): Linear(in_features=768, out_features=640, bias=False)
  (to_out): Sequential(
    (0): Linear(in_features=640, out_features=640, bias=True)
    (1): Dropout(p=0.0, inplace=False)
  )
)
archtecture.py - __init__  - unet -  ‚úÖ up_blocks.2.attentions.1.transformer_blocks.0.attn1: InflatedAttention(
  (to_q): Linear(in_features=640, out_features=640, bias=False)
  (to_k): Linear(in_features=640, out_features=640, bias=False)
  (to_v): Linear(in_features=640, out_features=640, bias=False)
  (to_out): Sequential(
    (0): Linear(in_features=640, out_features=640, bias=True)
    (1): Dropout(p=0.0, inplace=False)
  )
)
archtecture.py - __init__  - unet -  ‚úÖ up_blocks.2.attentions.1.transformer_blocks.0.attn2: InflatedAttention(
  (to_q): Linear(in_features=640, out_features=640, bias=False)
  (to_k): Linear(in_features=768, out_features=640, bias=False)
  (to_v): Linear(in_features=768, out_features=640, bias=False)
  (to_out): Sequential(
    (0): Linear(in_features=640, out_features=640, bias=True)
    (1): Dropout(p=0.0, inplace=False)
  )
)
archtecture.py - __init__  - unet -  ‚úÖ up_blocks.2.attentions.2.transformer_blocks.0.attn1: InflatedAttention(
  (to_q): Linear(in_features=640, out_features=640, bias=False)
  (to_k): Linear(in_features=640, out_features=640, bias=False)
  (to_v): Linear(in_features=640, out_features=640, bias=False)
  (to_out): Sequential(
    (0): Linear(in_features=640, out_features=640, bias=True)
    (1): Dropout(p=0.0, inplace=False)
  )
)
archtecture.py - __init__  - unet -  ‚úÖ up_blocks.2.attentions.2.transformer_blocks.0.attn2: InflatedAttention(
  (to_q): Linear(in_features=640, out_features=640, bias=False)
  (to_k): Linear(in_features=768, out_features=640, bias=False)
  (to_v): Linear(in_features=768, out_features=640, bias=False)
  (to_out): Sequential(
    (0): Linear(in_features=640, out_features=640, bias=True)
    (1): Dropout(p=0.0, inplace=False)
  )
)
archtecture.py - __init__  - unet -  ‚úÖ up_blocks.3.attentions.0.transformer_blocks.0.attn1: InflatedAttention(
  (to_q): Linear(in_features=320, out_features=320, bias=False)
  (to_k): Linear(in_features=320, out_features=320, bias=False)
  (to_v): Linear(in_features=320, out_features=320, bias=False)
  (to_out): Sequential(
    (0): Linear(in_features=320, out_features=320, bias=True)
    (1): Dropout(p=0.0, inplace=False)
  )
)
archtecture.py - __init__  - unet -  ‚úÖ up_blocks.3.attentions.0.transformer_blocks.0.attn2: InflatedAttention(
  (to_q): Linear(in_features=320, out_features=320, bias=False)
  (to_k): Linear(in_features=768, out_features=320, bias=False)
  (to_v): Linear(in_features=768, out_features=320, bias=False)
  (to_out): Sequential(
    (0): Linear(in_features=320, out_features=320, bias=True)
    (1): Dropout(p=0.0, inplace=False)
  )
)
archtecture.py - __init__  - unet -  ‚úÖ up_blocks.3.attentions.1.transformer_blocks.0.attn1: InflatedAttention(
  (to_q): Linear(in_features=320, out_features=320, bias=False)
  (to_k): Linear(in_features=320, out_features=320, bias=False)
  (to_v): Linear(in_features=320, out_features=320, bias=False)
  (to_out): Sequential(
    (0): Linear(in_features=320, out_features=320, bias=True)
    (1): Dropout(p=0.0, inplace=False)
  )
)
archtecture.py - __init__  - unet -  ‚úÖ up_blocks.3.attentions.1.transformer_blocks.0.attn2: InflatedAttention(
  (to_q): Linear(in_features=320, out_features=320, bias=False)
  (to_k): Linear(in_features=768, out_features=320, bias=False)
  (to_v): Linear(in_features=768, out_features=320, bias=False)
  (to_out): Sequential(
    (0): Linear(in_features=320, out_features=320, bias=True)
    (1): Dropout(p=0.0, inplace=False)
  )
)
archtecture.py - __init__  - unet -  ‚úÖ up_blocks.3.attentions.2.transformer_blocks.0.attn1: InflatedAttention(
  (to_q): Linear(in_features=320, out_features=320, bias=False)
  (to_k): Linear(in_features=320, out_features=320, bias=False)
  (to_v): Linear(in_features=320, out_features=320, bias=False)
  (to_out): Sequential(
    (0): Linear(in_features=320, out_features=320, bias=True)
    (1): Dropout(p=0.0, inplace=False)
  )
)
archtecture.py - __init__  - unet -  ‚úÖ up_blocks.3.attentions.2.transformer_blocks.0.attn2: InflatedAttention(
  (to_q): Linear(in_features=320, out_features=320, bias=False)
  (to_k): Linear(in_features=768, out_features=320, bias=False)
  (to_v): Linear(in_features=768, out_features=320, bias=False)
  (to_out): Sequential(
    (0): Linear(in_features=320, out_features=320, bias=True)
    (1): Dropout(p=0.0, inplace=False)
  )
)
archtecture.py - __init__  - unet -  ‚úÖ mid_block.attentions.0.transformer_blocks.0.attn1: InflatedAttention(
  (to_q): Linear(in_features=1280, out_features=1280, bias=False)
  (to_k): Linear(in_features=1280, out_features=1280, bias=False)
  (to_v): Linear(in_features=1280, out_features=1280, bias=False)
  (to_out): Sequential(
    (0): Linear(in_features=1280, out_features=1280, bias=True)
    (1): Dropout(p=0.0, inplace=False)
  )
)
archtecture.py - __init__  - unet -  ‚úÖ mid_block.attentions.0.transformer_blocks.0.attn2: InflatedAttention(
  (to_q): Linear(in_features=1280, out_features=1280, bias=False)
  (to_k): Linear(in_features=768, out_features=1280, bias=False)
  (to_v): Linear(in_features=768, out_features=1280, bias=False)
  (to_out): Sequential(
    (0): Linear(in_features=1280, out_features=1280, bias=True)
    (1): Dropout(p=0.0, inplace=False)
  )
)
=== End inflated list ===

architecture.py - CubeDiffModel - Replacing GroupNorms in UNet for color consistency
architecture.py - CubeDiffModel - init - Replaced 61 GroupNorm layers with SGN in UNet
Loading CLIP tokenizer & text encoder‚Ä¶
Loading CLIP tokenizer done
Loading CLIP text encode done
Loading Scheduler done
‚úÖ CubeDiffPipeline initialized.
Explicitly moved positional_encoding to cuda:0
Running warmup forward pass...
trainer.py - CubeDiffTrainer - setup_model - UNet model parameters set to requires_grad for inflated-attn layers only

[TRAINABLE] trainer.py - setup_model - inflated module InflatedAttention at InflatedAttention(
  (to_q): Linear(in_features=320, out_features=320, bias=False)
  (to_k): Linear(in_features=320, out_features=320, bias=False)
  (to_v): Linear(in_features=320, out_features=320, bias=False)
  (to_out): Sequential(
    (0): Linear(in_features=320, out_features=320, bias=True)
    (1): Dropout(p=0.0, inplace=False)
  )
) with is_inflated = True
[TRAINABLE] trainer.py - setup_model - inflated layer down_blocks.0.attentions.0.transformer_blocks.0.attn1.to_q.weight, requires_grad = True
[TRAINABLE] trainer.py - setup_model - inflated layer down_blocks.0.attentions.0.transformer_blocks.0.attn1.to_k.weight, requires_grad = True
[TRAINABLE] trainer.py - setup_model - inflated layer down_blocks.0.attentions.0.transformer_blocks.0.attn1.to_v.weight, requires_grad = True
[TRAINABLE] trainer.py - setup_model - inflated layer down_blocks.0.attentions.0.transformer_blocks.0.attn1.to_out.0.weight, requires_grad = True
[TRAINABLE] trainer.py - setup_model - inflated layer down_blocks.0.attentions.0.transformer_blocks.0.attn1.to_out.0.bias, requires_grad = True
[TRAINABLE] trainer.py - setup_model - inflated module InflatedAttention at InflatedAttention(
  (to_q): Linear(in_features=320, out_features=320, bias=False)
  (to_k): Linear(in_features=768, out_features=320, bias=False)
  (to_v): Linear(in_features=768, out_features=320, bias=False)
  (to_out): Sequential(
    (0): Linear(in_features=320, out_features=320, bias=True)
    (1): Dropout(p=0.0, inplace=False)
  )
) with is_inflated = True
[TRAINABLE] trainer.py - setup_model - inflated layer down_blocks.0.attentions.0.transformer_blocks.0.attn2.to_q.weight, requires_grad = True
[TRAINABLE] trainer.py - setup_model - inflated layer down_blocks.0.attentions.0.transformer_blocks.0.attn2.to_k.weight, requires_grad = True
[TRAINABLE] trainer.py - setup_model - inflated layer down_blocks.0.attentions.0.transformer_blocks.0.attn2.to_v.weight, requires_grad = True
[TRAINABLE] trainer.py - setup_model - inflated layer down_blocks.0.attentions.0.transformer_blocks.0.attn2.to_out.0.weight, requires_grad = True
[TRAINABLE] trainer.py - setup_model - inflated layer down_blocks.0.attentions.0.transformer_blocks.0.attn2.to_out.0.bias, requires_grad = True
[TRAINABLE] trainer.py - setup_model - inflated module InflatedAttention at InflatedAttention(
  (to_q): Linear(in_features=320, out_features=320, bias=False)
  (to_k): Linear(in_features=320, out_features=320, bias=False)
  (to_v): Linear(in_features=320, out_features=320, bias=False)
  (to_out): Sequential(
    (0): Linear(in_features=320, out_features=320, bias=True)
    (1): Dropout(p=0.0, inplace=False)
  )
) with is_inflated = True
[TRAINABLE] trainer.py - setup_model - inflated layer down_blocks.0.attentions.1.transformer_blocks.0.attn1.to_q.weight, requires_grad = True
[TRAINABLE] trainer.py - setup_model - inflated layer down_blocks.0.attentions.1.transformer_blocks.0.attn1.to_k.weight, requires_grad = True
[TRAINABLE] trainer.py - setup_model - inflated layer down_blocks.0.attentions.1.transformer_blocks.0.attn1.to_v.weight, requires_grad = True
[TRAINABLE] trainer.py - setup_model - inflated layer down_blocks.0.attentions.1.transformer_blocks.0.attn1.to_out.0.weight, requires_grad = True
[TRAINABLE] trainer.py - setup_model - inflated layer down_blocks.0.attentions.1.transformer_blocks.0.attn1.to_out.0.bias, requires_grad = True
[TRAINABLE] trainer.py - setup_model - inflated module InflatedAttention at InflatedAttention(
  (to_q): Linear(in_features=320, out_features=320, bias=False)
  (to_k): Linear(in_features=768, out_features=320, bias=False)
  (to_v): Linear(in_features=768, out_features=320, bias=False)
  (to_out): Sequential(
    (0): Linear(in_features=320, out_features=320, bias=True)
    (1): Dropout(p=0.0, inplace=False)
  )
) with is_inflated = True
[TRAINABLE] trainer.py - setup_model - inflated layer down_blocks.0.attentions.1.transformer_blocks.0.attn2.to_q.weight, requires_grad = True
[TRAINABLE] trainer.py - setup_model - inflated layer down_blocks.0.attentions.1.transformer_blocks.0.attn2.to_k.weight, requires_grad = True
[TRAINABLE] trainer.py - setup_model - inflated layer down_blocks.0.attentions.1.transformer_blocks.0.attn2.to_v.weight, requires_grad = True
[TRAINABLE] trainer.py - setup_model - inflated layer down_blocks.0.attentions.1.transformer_blocks.0.attn2.to_out.0.weight, requires_grad = True
[TRAINABLE] trainer.py - setup_model - inflated layer down_blocks.0.attentions.1.transformer_blocks.0.attn2.to_out.0.bias, requires_grad = True
[TRAINABLE] trainer.py - setup_model - inflated module InflatedAttention at InflatedAttention(
  (to_q): Linear(in_features=640, out_features=640, bias=False)
  (to_k): Linear(in_features=640, out_features=640, bias=False)
  (to_v): Linear(in_features=640, out_features=640, bias=False)
  (to_out): Sequential(
    (0): Linear(in_features=640, out_features=640, bias=True)
    (1): Dropout(p=0.0, inplace=False)
  )
) with is_inflated = True
[TRAINABLE] trainer.py - setup_model - inflated layer down_blocks.1.attentions.0.transformer_blocks.0.attn1.to_q.weight, requires_grad = True
[TRAINABLE] trainer.py - setup_model - inflated layer down_blocks.1.attentions.0.transformer_blocks.0.attn1.to_k.weight, requires_grad = True
[TRAINABLE] trainer.py - setup_model - inflated layer down_blocks.1.attentions.0.transformer_blocks.0.attn1.to_v.weight, requires_grad = True
[TRAINABLE] trainer.py - setup_model - inflated layer down_blocks.1.attentions.0.transformer_blocks.0.attn1.to_out.0.weight, requires_grad = True
[TRAINABLE] trainer.py - setup_model - inflated layer down_blocks.1.attentions.0.transformer_blocks.0.attn1.to_out.0.bias, requires_grad = True
[TRAINABLE] trainer.py - setup_model - inflated module InflatedAttention at InflatedAttention(
  (to_q): Linear(in_features=640, out_features=640, bias=False)
  (to_k): Linear(in_features=768, out_features=640, bias=False)
  (to_v): Linear(in_features=768, out_features=640, bias=False)
  (to_out): Sequential(
    (0): Linear(in_features=640, out_features=640, bias=True)
    (1): Dropout(p=0.0, inplace=False)
  )
) with is_inflated = True
[TRAINABLE] trainer.py - setup_model - inflated layer down_blocks.1.attentions.0.transformer_blocks.0.attn2.to_q.weight, requires_grad = True
[TRAINABLE] trainer.py - setup_model - inflated layer down_blocks.1.attentions.0.transformer_blocks.0.attn2.to_k.weight, requires_grad = True
[TRAINABLE] trainer.py - setup_model - inflated layer down_blocks.1.attentions.0.transformer_blocks.0.attn2.to_v.weight, requires_grad = True
[TRAINABLE] trainer.py - setup_model - inflated layer down_blocks.1.attentions.0.transformer_blocks.0.attn2.to_out.0.weight, requires_grad = True
[TRAINABLE] trainer.py - setup_model - inflated layer down_blocks.1.attentions.0.transformer_blocks.0.attn2.to_out.0.bias, requires_grad = True
[TRAINABLE] trainer.py - setup_model - inflated module InflatedAttention at InflatedAttention(
  (to_q): Linear(in_features=640, out_features=640, bias=False)
  (to_k): Linear(in_features=640, out_features=640, bias=False)
  (to_v): Linear(in_features=640, out_features=640, bias=False)
  (to_out): Sequential(
    (0): Linear(in_features=640, out_features=640, bias=True)
    (1): Dropout(p=0.0, inplace=False)
  )
) with is_inflated = True
[TRAINABLE] trainer.py - setup_model - inflated layer down_blocks.1.attentions.1.transformer_blocks.0.attn1.to_q.weight, requires_grad = True
[TRAINABLE] trainer.py - setup_model - inflated layer down_blocks.1.attentions.1.transformer_blocks.0.attn1.to_k.weight, requires_grad = True
[TRAINABLE] trainer.py - setup_model - inflated layer down_blocks.1.attentions.1.transformer_blocks.0.attn1.to_v.weight, requires_grad = True
[TRAINABLE] trainer.py - setup_model - inflated layer down_blocks.1.attentions.1.transformer_blocks.0.attn1.to_out.0.weight, requires_grad = True
[TRAINABLE] trainer.py - setup_model - inflated layer down_blocks.1.attentions.1.transformer_blocks.0.attn1.to_out.0.bias, requires_grad = True
[TRAINABLE] trainer.py - setup_model - inflated module InflatedAttention at InflatedAttention(
  (to_q): Linear(in_features=640, out_features=640, bias=False)
  (to_k): Linear(in_features=768, out_features=640, bias=False)
  (to_v): Linear(in_features=768, out_features=640, bias=False)
  (to_out): Sequential(
    (0): Linear(in_features=640, out_features=640, bias=True)
    (1): Dropout(p=0.0, inplace=False)
  )
) with is_inflated = True
[TRAINABLE] trainer.py - setup_model - inflated layer down_blocks.1.attentions.1.transformer_blocks.0.attn2.to_q.weight, requires_grad = True
[TRAINABLE] trainer.py - setup_model - inflated layer down_blocks.1.attentions.1.transformer_blocks.0.attn2.to_k.weight, requires_grad = True
[TRAINABLE] trainer.py - setup_model - inflated layer down_blocks.1.attentions.1.transformer_blocks.0.attn2.to_v.weight, requires_grad = True
[TRAINABLE] trainer.py - setup_model - inflated layer down_blocks.1.attentions.1.transformer_blocks.0.attn2.to_out.0.weight, requires_grad = True
[TRAINABLE] trainer.py - setup_model - inflated layer down_blocks.1.attentions.1.transformer_blocks.0.attn2.to_out.0.bias, requires_grad = True
[TRAINABLE] trainer.py - setup_model - inflated module InflatedAttention at InflatedAttention(
  (to_q): Linear(in_features=1280, out_features=1280, bias=False)
  (to_k): Linear(in_features=1280, out_features=1280, bias=False)
  (to_v): Linear(in_features=1280, out_features=1280, bias=False)
  (to_out): Sequential(
    (0): Linear(in_features=1280, out_features=1280, bias=True)
    (1): Dropout(p=0.0, inplace=False)
  )
) with is_inflated = True
[TRAINABLE] trainer.py - setup_model - inflated layer down_blocks.2.attentions.0.transformer_blocks.0.attn1.to_q.weight, requires_grad = True
[TRAINABLE] trainer.py - setup_model - inflated layer down_blocks.2.attentions.0.transformer_blocks.0.attn1.to_k.weight, requires_grad = True
[TRAINABLE] trainer.py - setup_model - inflated layer down_blocks.2.attentions.0.transformer_blocks.0.attn1.to_v.weight, requires_grad = True
[TRAINABLE] trainer.py - setup_model - inflated layer down_blocks.2.attentions.0.transformer_blocks.0.attn1.to_out.0.weight, requires_grad = True
[TRAINABLE] trainer.py - setup_model - inflated layer down_blocks.2.attentions.0.transformer_blocks.0.attn1.to_out.0.bias, requires_grad = True
[TRAINABLE] trainer.py - setup_model - inflated module InflatedAttention at InflatedAttention(
  (to_q): Linear(in_features=1280, out_features=1280, bias=False)
  (to_k): Linear(in_features=768, out_features=1280, bias=False)
  (to_v): Linear(in_features=768, out_features=1280, bias=False)
  (to_out): Sequential(
    (0): Linear(in_features=1280, out_features=1280, bias=True)
    (1): Dropout(p=0.0, inplace=False)
  )
) with is_inflated = True
[TRAINABLE] trainer.py - setup_model - inflated layer down_blocks.2.attentions.0.transformer_blocks.0.attn2.to_q.weight, requires_grad = True
[TRAINABLE] trainer.py - setup_model - inflated layer down_blocks.2.attentions.0.transformer_blocks.0.attn2.to_k.weight, requires_grad = True
[TRAINABLE] trainer.py - setup_model - inflated layer down_blocks.2.attentions.0.transformer_blocks.0.attn2.to_v.weight, requires_grad = True
[TRAINABLE] trainer.py - setup_model - inflated layer down_blocks.2.attentions.0.transformer_blocks.0.attn2.to_out.0.weight, requires_grad = True
[TRAINABLE] trainer.py - setup_model - inflated layer down_blocks.2.attentions.0.transformer_blocks.0.attn2.to_out.0.bias, requires_grad = True
[TRAINABLE] trainer.py - setup_model - inflated module InflatedAttention at InflatedAttention(
  (to_q): Linear(in_features=1280, out_features=1280, bias=False)
  (to_k): Linear(in_features=1280, out_features=1280, bias=False)
  (to_v): Linear(in_features=1280, out_features=1280, bias=False)
  (to_out): Sequential(
    (0): Linear(in_features=1280, out_features=1280, bias=True)
    (1): Dropout(p=0.0, inplace=False)
  )
) with is_inflated = True
[TRAINABLE] trainer.py - setup_model - inflated layer down_blocks.2.attentions.1.transformer_blocks.0.attn1.to_q.weight, requires_grad = True
[TRAINABLE] trainer.py - setup_model - inflated layer down_blocks.2.attentions.1.transformer_blocks.0.attn1.to_k.weight, requires_grad = True
[TRAINABLE] trainer.py - setup_model - inflated layer down_blocks.2.attentions.1.transformer_blocks.0.attn1.to_v.weight, requires_grad = True
[TRAINABLE] trainer.py - setup_model - inflated layer down_blocks.2.attentions.1.transformer_blocks.0.attn1.to_out.0.weight, requires_grad = True
[TRAINABLE] trainer.py - setup_model - inflated layer down_blocks.2.attentions.1.transformer_blocks.0.attn1.to_out.0.bias, requires_grad = True
[TRAINABLE] trainer.py - setup_model - inflated module InflatedAttention at InflatedAttention(
  (to_q): Linear(in_features=1280, out_features=1280, bias=False)
  (to_k): Linear(in_features=768, out_features=1280, bias=False)
  (to_v): Linear(in_features=768, out_features=1280, bias=False)
  (to_out): Sequential(
    (0): Linear(in_features=1280, out_features=1280, bias=True)
    (1): Dropout(p=0.0, inplace=False)
  )
) with is_inflated = True
[TRAINABLE] trainer.py - setup_model - inflated layer down_blocks.2.attentions.1.transformer_blocks.0.attn2.to_q.weight, requires_grad = True
[TRAINABLE] trainer.py - setup_model - inflated layer down_blocks.2.attentions.1.transformer_blocks.0.attn2.to_k.weight, requires_grad = True
[TRAINABLE] trainer.py - setup_model - inflated layer down_blocks.2.attentions.1.transformer_blocks.0.attn2.to_v.weight, requires_grad = True
[TRAINABLE] trainer.py - setup_model - inflated layer down_blocks.2.attentions.1.transformer_blocks.0.attn2.to_out.0.weight, requires_grad = True
[TRAINABLE] trainer.py - setup_model - inflated layer down_blocks.2.attentions.1.transformer_blocks.0.attn2.to_out.0.bias, requires_grad = True
[TRAINABLE] trainer.py - setup_model - inflated module InflatedAttention at InflatedAttention(
  (to_q): Linear(in_features=1280, out_features=1280, bias=False)
  (to_k): Linear(in_features=1280, out_features=1280, bias=False)
  (to_v): Linear(in_features=1280, out_features=1280, bias=False)
  (to_out): Sequential(
    (0): Linear(in_features=1280, out_features=1280, bias=True)
    (1): Dropout(p=0.0, inplace=False)
  )
) with is_inflated = True
[TRAINABLE] trainer.py - setup_model - inflated layer up_blocks.1.attentions.0.transformer_blocks.0.attn1.to_q.weight, requires_grad = True
[TRAINABLE] trainer.py - setup_model - inflated layer up_blocks.1.attentions.0.transformer_blocks.0.attn1.to_k.weight, requires_grad = True
[TRAINABLE] trainer.py - setup_model - inflated layer up_blocks.1.attentions.0.transformer_blocks.0.attn1.to_v.weight, requires_grad = True
[TRAINABLE] trainer.py - setup_model - inflated layer up_blocks.1.attentions.0.transformer_blocks.0.attn1.to_out.0.weight, requires_grad = True
[TRAINABLE] trainer.py - setup_model - inflated layer up_blocks.1.attentions.0.transformer_blocks.0.attn1.to_out.0.bias, requires_grad = True
[TRAINABLE] trainer.py - setup_model - inflated module InflatedAttention at InflatedAttention(
  (to_q): Linear(in_features=1280, out_features=1280, bias=False)
  (to_k): Linear(in_features=768, out_features=1280, bias=False)
  (to_v): Linear(in_features=768, out_features=1280, bias=False)
  (to_out): Sequential(
    (0): Linear(in_features=1280, out_features=1280, bias=True)
    (1): Dropout(p=0.0, inplace=False)
  )
) with is_inflated = True
[TRAINABLE] trainer.py - setup_model - inflated layer up_blocks.1.attentions.0.transformer_blocks.0.attn2.to_q.weight, requires_grad = True
[TRAINABLE] trainer.py - setup_model - inflated layer up_blocks.1.attentions.0.transformer_blocks.0.attn2.to_k.weight, requires_grad = True
[TRAINABLE] trainer.py - setup_model - inflated layer up_blocks.1.attentions.0.transformer_blocks.0.attn2.to_v.weight, requires_grad = True
[TRAINABLE] trainer.py - setup_model - inflated layer up_blocks.1.attentions.0.transformer_blocks.0.attn2.to_out.0.weight, requires_grad = True
[TRAINABLE] trainer.py - setup_model - inflated layer up_blocks.1.attentions.0.transformer_blocks.0.attn2.to_out.0.bias, requires_grad = True
[TRAINABLE] trainer.py - setup_model - inflated module InflatedAttention at InflatedAttention(
  (to_q): Linear(in_features=1280, out_features=1280, bias=False)
  (to_k): Linear(in_features=1280, out_features=1280, bias=False)
  (to_v): Linear(in_features=1280, out_features=1280, bias=False)
  (to_out): Sequential(
    (0): Linear(in_features=1280, out_features=1280, bias=True)
    (1): Dropout(p=0.0, inplace=False)
  )
) with is_inflated = True
[TRAINABLE] trainer.py - setup_model - inflated layer up_blocks.1.attentions.1.transformer_blocks.0.attn1.to_q.weight, requires_grad = True
[TRAINABLE] trainer.py - setup_model - inflated layer up_blocks.1.attentions.1.transformer_blocks.0.attn1.to_k.weight, requires_grad = True
[TRAINABLE] trainer.py - setup_model - inflated layer up_blocks.1.attentions.1.transformer_blocks.0.attn1.to_v.weight, requires_grad = True
[TRAINABLE] trainer.py - setup_model - inflated layer up_blocks.1.attentions.1.transformer_blocks.0.attn1.to_out.0.weight, requires_grad = True
[TRAINABLE] trainer.py - setup_model - inflated layer up_blocks.1.attentions.1.transformer_blocks.0.attn1.to_out.0.bias, requires_grad = True
[TRAINABLE] trainer.py - setup_model - inflated module InflatedAttention at InflatedAttention(
  (to_q): Linear(in_features=1280, out_features=1280, bias=False)
  (to_k): Linear(in_features=768, out_features=1280, bias=False)
  (to_v): Linear(in_features=768, out_features=1280, bias=False)
  (to_out): Sequential(
    (0): Linear(in_features=1280, out_features=1280, bias=True)
    (1): Dropout(p=0.0, inplace=False)
  )
) with is_inflated = True
[TRAINABLE] trainer.py - setup_model - inflated layer up_blocks.1.attentions.1.transformer_blocks.0.attn2.to_q.weight, requires_grad = True
[TRAINABLE] trainer.py - setup_model - inflated layer up_blocks.1.attentions.1.transformer_blocks.0.attn2.to_k.weight, requires_grad = True
[TRAINABLE] trainer.py - setup_model - inflated layer up_blocks.1.attentions.1.transformer_blocks.0.attn2.to_v.weight, requires_grad = True
[TRAINABLE] trainer.py - setup_model - inflated layer up_blocks.1.attentions.1.transformer_blocks.0.attn2.to_out.0.weight, requires_grad = True
[TRAINABLE] trainer.py - setup_model - inflated layer up_blocks.1.attentions.1.transformer_blocks.0.attn2.to_out.0.bias, requires_grad = True
[TRAINABLE] trainer.py - setup_model - inflated module InflatedAttention at InflatedAttention(
  (to_q): Linear(in_features=1280, out_features=1280, bias=False)
  (to_k): Linear(in_features=1280, out_features=1280, bias=False)
  (to_v): Linear(in_features=1280, out_features=1280, bias=False)
  (to_out): Sequential(
    (0): Linear(in_features=1280, out_features=1280, bias=True)
    (1): Dropout(p=0.0, inplace=False)
  )
) with is_inflated = True
[TRAINABLE] trainer.py - setup_model - inflated layer up_blocks.1.attentions.2.transformer_blocks.0.attn1.to_q.weight, requires_grad = True
[TRAINABLE] trainer.py - setup_model - inflated layer up_blocks.1.attentions.2.transformer_blocks.0.attn1.to_k.weight, requires_grad = True
[TRAINABLE] trainer.py - setup_model - inflated layer up_blocks.1.attentions.2.transformer_blocks.0.attn1.to_v.weight, requires_grad = True
[TRAINABLE] trainer.py - setup_model - inflated layer up_blocks.1.attentions.2.transformer_blocks.0.attn1.to_out.0.weight, requires_grad = True
[TRAINABLE] trainer.py - setup_model - inflated layer up_blocks.1.attentions.2.transformer_blocks.0.attn1.to_out.0.bias, requires_grad = True
[TRAINABLE] trainer.py - setup_model - inflated module InflatedAttention at InflatedAttention(
  (to_q): Linear(in_features=1280, out_features=1280, bias=False)
  (to_k): Linear(in_features=768, out_features=1280, bias=False)
  (to_v): Linear(in_features=768, out_features=1280, bias=False)
  (to_out): Sequential(
    (0): Linear(in_features=1280, out_features=1280, bias=True)
    (1): Dropout(p=0.0, inplace=False)
  )
) with is_inflated = True
[TRAINABLE] trainer.py - setup_model - inflated layer up_blocks.1.attentions.2.transformer_blocks.0.attn2.to_q.weight, requires_grad = True
[TRAINABLE] trainer.py - setup_model - inflated layer up_blocks.1.attentions.2.transformer_blocks.0.attn2.to_k.weight, requires_grad = True
[TRAINABLE] trainer.py - setup_model - inflated layer up_blocks.1.attentions.2.transformer_blocks.0.attn2.to_v.weight, requires_grad = True
[TRAINABLE] trainer.py - setup_model - inflated layer up_blocks.1.attentions.2.transformer_blocks.0.attn2.to_out.0.weight, requires_grad = True
[TRAINABLE] trainer.py - setup_model - inflated layer up_blocks.1.attentions.2.transformer_blocks.0.attn2.to_out.0.bias, requires_grad = True
[TRAINABLE] trainer.py - setup_model - inflated module InflatedAttention at InflatedAttention(
  (to_q): Linear(in_features=640, out_features=640, bias=False)
  (to_k): Linear(in_features=640, out_features=640, bias=False)
  (to_v): Linear(in_features=640, out_features=640, bias=False)
  (to_out): Sequential(
    (0): Linear(in_features=640, out_features=640, bias=True)
    (1): Dropout(p=0.0, inplace=False)
  )
) with is_inflated = True
[TRAINABLE] trainer.py - setup_model - inflated layer up_blocks.2.attentions.0.transformer_blocks.0.attn1.to_q.weight, requires_grad = True
[TRAINABLE] trainer.py - setup_model - inflated layer up_blocks.2.attentions.0.transformer_blocks.0.attn1.to_k.weight, requires_grad = True
[TRAINABLE] trainer.py - setup_model - inflated layer up_blocks.2.attentions.0.transformer_blocks.0.attn1.to_v.weight, requires_grad = True
[TRAINABLE] trainer.py - setup_model - inflated layer up_blocks.2.attentions.0.transformer_blocks.0.attn1.to_out.0.weight, requires_grad = True
[TRAINABLE] trainer.py - setup_model - inflated layer up_blocks.2.attentions.0.transformer_blocks.0.attn1.to_out.0.bias, requires_grad = True
[TRAINABLE] trainer.py - setup_model - inflated module InflatedAttention at InflatedAttention(
  (to_q): Linear(in_features=640, out_features=640, bias=False)
  (to_k): Linear(in_features=768, out_features=640, bias=False)
  (to_v): Linear(in_features=768, out_features=640, bias=False)
  (to_out): Sequential(
    (0): Linear(in_features=640, out_features=640, bias=True)
    (1): Dropout(p=0.0, inplace=False)
  )
) with is_inflated = True
[TRAINABLE] trainer.py - setup_model - inflated layer up_blocks.2.attentions.0.transformer_blocks.0.attn2.to_q.weight, requires_grad = True
[TRAINABLE] trainer.py - setup_model - inflated layer up_blocks.2.attentions.0.transformer_blocks.0.attn2.to_k.weight, requires_grad = True
[TRAINABLE] trainer.py - setup_model - inflated layer up_blocks.2.attentions.0.transformer_blocks.0.attn2.to_v.weight, requires_grad = True
[TRAINABLE] trainer.py - setup_model - inflated layer up_blocks.2.attentions.0.transformer_blocks.0.attn2.to_out.0.weight, requires_grad = True
[TRAINABLE] trainer.py - setup_model - inflated layer up_blocks.2.attentions.0.transformer_blocks.0.attn2.to_out.0.bias, requires_grad = True
[TRAINABLE] trainer.py - setup_model - inflated module InflatedAttention at InflatedAttention(
  (to_q): Linear(in_features=640, out_features=640, bias=False)
  (to_k): Linear(in_features=640, out_features=640, bias=False)
  (to_v): Linear(in_features=640, out_features=640, bias=False)
  (to_out): Sequential(
    (0): Linear(in_features=640, out_features=640, bias=True)
    (1): Dropout(p=0.0, inplace=False)
  )
) with is_inflated = True
[TRAINABLE] trainer.py - setup_model - inflated layer up_blocks.2.attentions.1.transformer_blocks.0.attn1.to_q.weight, requires_grad = True
[TRAINABLE] trainer.py - setup_model - inflated layer up_blocks.2.attentions.1.transformer_blocks.0.attn1.to_k.weight, requires_grad = True
[TRAINABLE] trainer.py - setup_model - inflated layer up_blocks.2.attentions.1.transformer_blocks.0.attn1.to_v.weight, requires_grad = True
[TRAINABLE] trainer.py - setup_model - inflated layer up_blocks.2.attentions.1.transformer_blocks.0.attn1.to_out.0.weight, requires_grad = True
[TRAINABLE] trainer.py - setup_model - inflated layer up_blocks.2.attentions.1.transformer_blocks.0.attn1.to_out.0.bias, requires_grad = True
[TRAINABLE] trainer.py - setup_model - inflated module InflatedAttention at InflatedAttention(
  (to_q): Linear(in_features=640, out_features=640, bias=False)
  (to_k): Linear(in_features=768, out_features=640, bias=False)
  (to_v): Linear(in_features=768, out_features=640, bias=False)
  (to_out): Sequential(
    (0): Linear(in_features=640, out_features=640, bias=True)
    (1): Dropout(p=0.0, inplace=False)
  )
) with is_inflated = True
[TRAINABLE] trainer.py - setup_model - inflated layer up_blocks.2.attentions.1.transformer_blocks.0.attn2.to_q.weight, requires_grad = True
[TRAINABLE] trainer.py - setup_model - inflated layer up_blocks.2.attentions.1.transformer_blocks.0.attn2.to_k.weight, requires_grad = True
[TRAINABLE] trainer.py - setup_model - inflated layer up_blocks.2.attentions.1.transformer_blocks.0.attn2.to_v.weight, requires_grad = True
[TRAINABLE] trainer.py - setup_model - inflated layer up_blocks.2.attentions.1.transformer_blocks.0.attn2.to_out.0.weight, requires_grad = True
[TRAINABLE] trainer.py - setup_model - inflated layer up_blocks.2.attentions.1.transformer_blocks.0.attn2.to_out.0.bias, requires_grad = True
[TRAINABLE] trainer.py - setup_model - inflated module InflatedAttention at InflatedAttention(
  (to_q): Linear(in_features=640, out_features=640, bias=False)
  (to_k): Linear(in_features=640, out_features=640, bias=False)
  (to_v): Linear(in_features=640, out_features=640, bias=False)
  (to_out): Sequential(
    (0): Linear(in_features=640, out_features=640, bias=True)
    (1): Dropout(p=0.0, inplace=False)
  )
) with is_inflated = True
[TRAINABLE] trainer.py - setup_model - inflated layer up_blocks.2.attentions.2.transformer_blocks.0.attn1.to_q.weight, requires_grad = True
[TRAINABLE] trainer.py - setup_model - inflated layer up_blocks.2.attentions.2.transformer_blocks.0.attn1.to_k.weight, requires_grad = True
[TRAINABLE] trainer.py - setup_model - inflated layer up_blocks.2.attentions.2.transformer_blocks.0.attn1.to_v.weight, requires_grad = True
[TRAINABLE] trainer.py - setup_model - inflated layer up_blocks.2.attentions.2.transformer_blocks.0.attn1.to_out.0.weight, requires_grad = True
[TRAINABLE] trainer.py - setup_model - inflated layer up_blocks.2.attentions.2.transformer_blocks.0.attn1.to_out.0.bias, requires_grad = True
[TRAINABLE] trainer.py - setup_model - inflated module InflatedAttention at InflatedAttention(
  (to_q): Linear(in_features=640, out_features=640, bias=False)
  (to_k): Linear(in_features=768, out_features=640, bias=False)
  (to_v): Linear(in_features=768, out_features=640, bias=False)
  (to_out): Sequential(
    (0): Linear(in_features=640, out_features=640, bias=True)
    (1): Dropout(p=0.0, inplace=False)
  )
) with is_inflated = True
[TRAINABLE] trainer.py - setup_model - inflated layer up_blocks.2.attentions.2.transformer_blocks.0.attn2.to_q.weight, requires_grad = True
[TRAINABLE] trainer.py - setup_model - inflated layer up_blocks.2.attentions.2.transformer_blocks.0.attn2.to_k.weight, requires_grad = True
[TRAINABLE] trainer.py - setup_model - inflated layer up_blocks.2.attentions.2.transformer_blocks.0.attn2.to_v.weight, requires_grad = True
[TRAINABLE] trainer.py - setup_model - inflated layer up_blocks.2.attentions.2.transformer_blocks.0.attn2.to_out.0.weight, requires_grad = True
[TRAINABLE] trainer.py - setup_model - inflated layer up_blocks.2.attentions.2.transformer_blocks.0.attn2.to_out.0.bias, requires_grad = True
[TRAINABLE] trainer.py - setup_model - inflated module InflatedAttention at InflatedAttention(
  (to_q): Linear(in_features=320, out_features=320, bias=False)
  (to_k): Linear(in_features=320, out_features=320, bias=False)
  (to_v): Linear(in_features=320, out_features=320, bias=False)
  (to_out): Sequential(
    (0): Linear(in_features=320, out_features=320, bias=True)
    (1): Dropout(p=0.0, inplace=False)
  )
) with is_inflated = True
[TRAINABLE] trainer.py - setup_model - inflated layer up_blocks.3.attentions.0.transformer_blocks.0.attn1.to_q.weight, requires_grad = True
[TRAINABLE] trainer.py - setup_model - inflated layer up_blocks.3.attentions.0.transformer_blocks.0.attn1.to_k.weight, requires_grad = True
[TRAINABLE] trainer.py - setup_model - inflated layer up_blocks.3.attentions.0.transformer_blocks.0.attn1.to_v.weight, requires_grad = True
[TRAINABLE] trainer.py - setup_model - inflated layer up_blocks.3.attentions.0.transformer_blocks.0.attn1.to_out.0.weight, requires_grad = True
[TRAINABLE] trainer.py - setup_model - inflated layer up_blocks.3.attentions.0.transformer_blocks.0.attn1.to_out.0.bias, requires_grad = True
[TRAINABLE] trainer.py - setup_model - inflated module InflatedAttention at InflatedAttention(
  (to_q): Linear(in_features=320, out_features=320, bias=False)
  (to_k): Linear(in_features=768, out_features=320, bias=False)
  (to_v): Linear(in_features=768, out_features=320, bias=False)
  (to_out): Sequential(
    (0): Linear(in_features=320, out_features=320, bias=True)
    (1): Dropout(p=0.0, inplace=False)
  )
) with is_inflated = True
[TRAINABLE] trainer.py - setup_model - inflated layer up_blocks.3.attentions.0.transformer_blocks.0.attn2.to_q.weight, requires_grad = True
[TRAINABLE] trainer.py - setup_model - inflated layer up_blocks.3.attentions.0.transformer_blocks.0.attn2.to_k.weight, requires_grad = True
[TRAINABLE] trainer.py - setup_model - inflated layer up_blocks.3.attentions.0.transformer_blocks.0.attn2.to_v.weight, requires_grad = True
[TRAINABLE] trainer.py - setup_model - inflated layer up_blocks.3.attentions.0.transformer_blocks.0.attn2.to_out.0.weight, requires_grad = True
[TRAINABLE] trainer.py - setup_model - inflated layer up_blocks.3.attentions.0.transformer_blocks.0.attn2.to_out.0.bias, requires_grad = True
[TRAINABLE] trainer.py - setup_model - inflated module InflatedAttention at InflatedAttention(
  (to_q): Linear(in_features=320, out_features=320, bias=False)
  (to_k): Linear(in_features=320, out_features=320, bias=False)
  (to_v): Linear(in_features=320, out_features=320, bias=False)
  (to_out): Sequential(
    (0): Linear(in_features=320, out_features=320, bias=True)
    (1): Dropout(p=0.0, inplace=False)
  )
) with is_inflated = True
[TRAINABLE] trainer.py - setup_model - inflated layer up_blocks.3.attentions.1.transformer_blocks.0.attn1.to_q.weight, requires_grad = True
[TRAINABLE] trainer.py - setup_model - inflated layer up_blocks.3.attentions.1.transformer_blocks.0.attn1.to_k.weight, requires_grad = True
[TRAINABLE] trainer.py - setup_model - inflated layer up_blocks.3.attentions.1.transformer_blocks.0.attn1.to_v.weight, requires_grad = True
[TRAINABLE] trainer.py - setup_model - inflated layer up_blocks.3.attentions.1.transformer_blocks.0.attn1.to_out.0.weight, requires_grad = True
[TRAINABLE] trainer.py - setup_model - inflated layer up_blocks.3.attentions.1.transformer_blocks.0.attn1.to_out.0.bias, requires_grad = True
[TRAINABLE] trainer.py - setup_model - inflated module InflatedAttention at InflatedAttention(
  (to_q): Linear(in_features=320, out_features=320, bias=False)
  (to_k): Linear(in_features=768, out_features=320, bias=False)
  (to_v): Linear(in_features=768, out_features=320, bias=False)
  (to_out): Sequential(
    (0): Linear(in_features=320, out_features=320, bias=True)
    (1): Dropout(p=0.0, inplace=False)
  )
) with is_inflated = True
[TRAINABLE] trainer.py - setup_model - inflated layer up_blocks.3.attentions.1.transformer_blocks.0.attn2.to_q.weight, requires_grad = True
[TRAINABLE] trainer.py - setup_model - inflated layer up_blocks.3.attentions.1.transformer_blocks.0.attn2.to_k.weight, requires_grad = True
[TRAINABLE] trainer.py - setup_model - inflated layer up_blocks.3.attentions.1.transformer_blocks.0.attn2.to_v.weight, requires_grad = True
[TRAINABLE] trainer.py - setup_model - inflated layer up_blocks.3.attentions.1.transformer_blocks.0.attn2.to_out.0.weight, requires_grad = True
[TRAINABLE] trainer.py - setup_model - inflated layer up_blocks.3.attentions.1.transformer_blocks.0.attn2.to_out.0.bias, requires_grad = True
[TRAINABLE] trainer.py - setup_model - inflated module InflatedAttention at InflatedAttention(
  (to_q): Linear(in_features=320, out_features=320, bias=False)
  (to_k): Linear(in_features=320, out_features=320, bias=False)
  (to_v): Linear(in_features=320, out_features=320, bias=False)
  (to_out): Sequential(
    (0): Linear(in_features=320, out_features=320, bias=True)
    (1): Dropout(p=0.0, inplace=False)
  )
) with is_inflated = True
[TRAINABLE] trainer.py - setup_model - inflated layer up_blocks.3.attentions.2.transformer_blocks.0.attn1.to_q.weight, requires_grad = True
[TRAINABLE] trainer.py - setup_model - inflated layer up_blocks.3.attentions.2.transformer_blocks.0.attn1.to_k.weight, requires_grad = True
[TRAINABLE] trainer.py - setup_model - inflated layer up_blocks.3.attentions.2.transformer_blocks.0.attn1.to_v.weight, requires_grad = True
[TRAINABLE] trainer.py - setup_model - inflated layer up_blocks.3.attentions.2.transformer_blocks.0.attn1.to_out.0.weight, requires_grad = True
[TRAINABLE] trainer.py - setup_model - inflated layer up_blocks.3.attentions.2.transformer_blocks.0.attn1.to_out.0.bias, requires_grad = True
[TRAINABLE] trainer.py - setup_model - inflated module InflatedAttention at InflatedAttention(
  (to_q): Linear(in_features=320, out_features=320, bias=False)
  (to_k): Linear(in_features=768, out_features=320, bias=False)
  (to_v): Linear(in_features=768, out_features=320, bias=False)
  (to_out): Sequential(
    (0): Linear(in_features=320, out_features=320, bias=True)
    (1): Dropout(p=0.0, inplace=False)
  )
) with is_inflated = True
[TRAINABLE] trainer.py - setup_model - inflated layer up_blocks.3.attentions.2.transformer_blocks.0.attn2.to_q.weight, requires_grad = True
[TRAINABLE] trainer.py - setup_model - inflated layer up_blocks.3.attentions.2.transformer_blocks.0.attn2.to_k.weight, requires_grad = True
[TRAINABLE] trainer.py - setup_model - inflated layer up_blocks.3.attentions.2.transformer_blocks.0.attn2.to_v.weight, requires_grad = True
[TRAINABLE] trainer.py - setup_model - inflated layer up_blocks.3.attentions.2.transformer_blocks.0.attn2.to_out.0.weight, requires_grad = True
[TRAINABLE] trainer.py - setup_model - inflated layer up_blocks.3.attentions.2.transformer_blocks.0.attn2.to_out.0.bias, requires_grad = True
[TRAINABLE] trainer.py - setup_model - inflated module InflatedAttention at InflatedAttention(
  (to_q): Linear(in_features=1280, out_features=1280, bias=False)
  (to_k): Linear(in_features=1280, out_features=1280, bias=False)
  (to_v): Linear(in_features=1280, out_features=1280, bias=False)
  (to_out): Sequential(
    (0): Linear(in_features=1280, out_features=1280, bias=True)
    (1): Dropout(p=0.0, inplace=False)
  )
) with is_inflated = True
[TRAINABLE] trainer.py - setup_model - inflated layer mid_block.attentions.0.transformer_blocks.0.attn1.to_q.weight, requires_grad = True
[TRAINABLE] trainer.py - setup_model - inflated layer mid_block.attentions.0.transformer_blocks.0.attn1.to_k.weight, requires_grad = True
[TRAINABLE] trainer.py - setup_model - inflated layer mid_block.attentions.0.transformer_blocks.0.attn1.to_v.weight, requires_grad = True
[TRAINABLE] trainer.py - setup_model - inflated layer mid_block.attentions.0.transformer_blocks.0.attn1.to_out.0.weight, requires_grad = True
[TRAINABLE] trainer.py - setup_model - inflated layer mid_block.attentions.0.transformer_blocks.0.attn1.to_out.0.bias, requires_grad = True
[TRAINABLE] trainer.py - setup_model - inflated module InflatedAttention at InflatedAttention(
  (to_q): Linear(in_features=1280, out_features=1280, bias=False)
  (to_k): Linear(in_features=768, out_features=1280, bias=False)
  (to_v): Linear(in_features=768, out_features=1280, bias=False)
  (to_out): Sequential(
    (0): Linear(in_features=1280, out_features=1280, bias=True)
    (1): Dropout(p=0.0, inplace=False)
  )
) with is_inflated = True
[TRAINABLE] trainer.py - setup_model - inflated layer mid_block.attentions.0.transformer_blocks.0.attn2.to_q.weight, requires_grad = True
[TRAINABLE] trainer.py - setup_model - inflated layer mid_block.attentions.0.transformer_blocks.0.attn2.to_k.weight, requires_grad = True
[TRAINABLE] trainer.py - setup_model - inflated layer mid_block.attentions.0.transformer_blocks.0.attn2.to_v.weight, requires_grad = True
[TRAINABLE] trainer.py - setup_model - inflated layer mid_block.attentions.0.transformer_blocks.0.attn2.to_out.0.weight, requires_grad = True
[TRAINABLE] trainer.py - setup_model - inflated layer mid_block.attentions.0.transformer_blocks.0.attn2.to_out.0.bias, requires_grad = True
>>> trainer.py - setup_model - 160 parameters un-frozen for fine-tuning (inflated modules)
üëâ trainer.py - CubeDiffTrainer - setup_model - UNet model parameters set to requires_grad for Full-rank tuning: 93.54M / 859.6M params
trainer.py - CubeDiffTrainer- CubeDiff Model components cast to torch.bfloat16
trainer.py - CubeDiffTrainer - CubeDiff Model enabled gradient checkpointing

trainer.py - CubeDiffTrainer - CubeDiff Model enabled xformers

>>> Training 170 tensors, totalling 93,537,040 params
trainer.py - CubeDiffTrainer - setup_model done - Total params 859.6M ‚Äî trainable 93.54M
üöÄ Compiling model with optimized settings for L4 GPUs...
‚ö†Ô∏è Model compilation failed: Either mode or options can be specified, but both can't be specified at the same time.
Training will continue without compilation...
trainer.py - CubeDiffTrainer - setup_model - torch.compile done, cost 0.0001 seconds
trainer.py - CubeDiffTrainer - init - setup_model runwayml/stable-diffusion-v1-5 done

trainer.py - CubeDiffTrainer - init - vgg16 loading done
train.py - Try loading CLIPProcessor from local cache snapshots : /home/jupyter/.cache/huggingface/hub/models--openai--clip-vit-base-patch32/snapshots
trainer.py - Try loading CLIPProcessor from local cache subfolder: /home/jupyter/.cache/huggingface/hub/models--openai--clip-vit-base-patch32/snapshots/3d74acf9a28c67741b2f4f2ea7635f0aaf6f0268
trainer.py - ‚úÖ Loaded CLIPProcessor from local cache
trainer.py - ‚úÖ Loaded CLIPModel from local cache (safetensors) use_safetensors as True
  ‚ñ∂ Train samples: 630
  ‚ñ∂ Val   samples: 70
trainer.py - CubeDiffTrainer - __init__ - train data -                 self.train_size is 630, max_train_steps (total_updates) is 1,                 epochs_per_trn_datasize is 79, total_steps is 1,                  self.total_required_samples is 8,                 world_size is 1, batch_size is 2, micro_batch_size is 2,                 batch_num_per_rank is 315, samples_per_rank is 630,                 accum_steps is 4, accum_batch_size_per_rank is 8,                 sample_size_per_update_step (actual batch size for gradients update) is 8, eval_cycle is 1

train_cubediff.py - trainer is <training.trainer.CubeDiffTrainer object at 0x7f7b92bb82e0>
trainer.py - CubeDiffTrainer - Building dataloaders with config: {'dataset': '/home/jupyter/mluser/git/cube/data/dataspace/polyhaven_tiny_13_new_pt/cubediff_train.tar', 'val_dataset': '/home/jupyter/mluser/git/cube/data/dataspace/polyhaven_tiny_13_new_pt/cubediff_val.tar', 'batch_size': 2, 'eval_batch_size': 2, 'num_workers': 10, 'gradient_accum_steps': 4, 'learning_rate': 8e-05, 'mixed_precision': 'bf16', 'max_train_steps': 1, 'eval_cycle': 1, 'warmup_ratio': 0.03, 'plateau_ratio': 0, 'use_wandb': False, 'output_dir': 'outputs/cubediff_tiny_lora', 'seed': 1337, 'skip_weight_copy': False}
[latent_webdataset] rank=0 loading 630 raw samples
latent_webdataset.py - üìù [rank=0] loading abandoned_factory_canteen_01.abandoned_factory_canteen_01 from raw samples with raw size 3 bytes
latent_webdataset.py - [rank {rank}] Loading encoder_hidden_states and attention_mask from .emb format
latent_webdataset.py - üìù [rank=0] loading abandoned_bakery.abandoned_bakery from raw samples with raw size 3 bytes
latent_webdataset.py - [rank {rank}] Loading encoder_hidden_states and attention_mask from .emb format
latent_webdataset.py - üìù [rank=0] loading abandoned_games_room_01.abandoned_games_room_01 from raw samples with raw size 3 bytes
latent_webdataset.py - [rank {rank}] Loading encoder_hidden_states and attention_mask from .emb format
latent_webdataset.py - üìù [rank=0] loading abandoned_construction.abandoned_construction from raw samples with raw size 3 bytes
latent_webdataset.py - [rank {rank}] Loading encoder_hidden_states and attention_mask from .emb format
latent_webdataset.py - üìù [rank=0] loading abandoned_church.abandoned_church from raw samples with raw size 3 bytes
latent_webdataset.py - [rank {rank}] Loading encoder_hidden_states and attention_mask from .emb format
latent_webdataset.py - üìù [rank=0] loading abandoned_hall_01.abandoned_hall_01 from raw samples with raw size 3 bytes
latent_webdataset.py - [rank {rank}] Loading encoder_hidden_states and attention_mask from .emb format
latent_webdataset.py - üìù [rank=0] loading abandoned_factory_canteen_02.abandoned_factory_canteen_02 from raw samples with raw size 3 bytes
latent_webdataset.py - [rank {rank}] Loading encoder_hidden_states and attention_mask from .emb format
latent_webdataset.py - üìù [rank=0] loading abandoned_greenhouse.abandoned_greenhouse from raw samples with raw size 3 bytes
latent_webdataset.py - [rank {rank}] Loading encoder_hidden_states and attention_mask from .emb format
latent_webdataset.py - üìù [rank=0] loading abandoned_hopper_terminal_01.abandoned_hopper_terminal_01 from raw samples with raw size 3 bytes
latent_webdataset.py - [rank {rank}] Loading encoder_hidden_states and attention_mask from .emb format
latent_webdataset.py - üìù [rank=0] loading abandoned_hopper_terminal_02.abandoned_hopper_terminal_02 from raw samples with raw size 3 bytes
latent_webdataset.py - [rank {rank}] Loading encoder_hidden_states and attention_mask from .emb format
latent_webdataset.py - üìù [rank=0] loading abandoned_hopper_terminal_03.abandoned_hopper_terminal_03 from raw samples with raw size 3 bytes
latent_webdataset.py - [rank {rank}] Loading encoder_hidden_states and attention_mask from .emb format
latent_webdataset.py - üìù [rank=0] loading abandoned_hopper_terminal_04.abandoned_hopper_terminal_04 from raw samples with raw size 3 bytes
latent_webdataset.py - [rank {rank}] Loading encoder_hidden_states and attention_mask from .emb format
latent_webdataset.py - üìù [rank=0] loading abandoned_parking.abandoned_parking from raw samples with raw size 3 bytes
latent_webdataset.py - [rank {rank}] Loading encoder_hidden_states and attention_mask from .emb format
latent_webdataset.py - üìù [rank=0] loading abandoned_pathway.abandoned_pathway from raw samples with raw size 3 bytes
latent_webdataset.py - [rank {rank}] Loading encoder_hidden_states and attention_mask from .emb format
latent_webdataset.py - üìù [rank=0] loading abandoned_slipway.abandoned_slipway from raw samples with raw size 3 bytes
latent_webdataset.py - [rank {rank}] Loading encoder_hidden_states and attention_mask from .emb format
latent_webdataset.py - üìù [rank=0] loading abandoned_tank_farm_01.abandoned_tank_farm_01 from raw samples with raw size 3 bytes
latent_webdataset.py - [rank {rank}] Loading encoder_hidden_states and attention_mask from .emb format
latent_webdataset.py - üìù [rank=0] loading abandoned_tank_farm_02.abandoned_tank_farm_02 from raw samples with raw size 3 bytes
latent_webdataset.py - [rank {rank}] Loading encoder_hidden_states and attention_mask from .emb format
latent_webdataset.py - üìù [rank=0] loading abandoned_tank_farm_03.abandoned_tank_farm_03 from raw samples with raw size 3 bytes
latent_webdataset.py - [rank {rank}] Loading encoder_hidden_states and attention_mask from .emb format
latent_webdataset.py - üìù [rank=0] loading abandoned_tank_farm_04.abandoned_tank_farm_04 from raw samples with raw size 3 bytes
latent_webdataset.py - [rank {rank}] Loading encoder_hidden_states and attention_mask from .emb format
latent_webdataset.py - üìù [rank=0] loading abandoned_tank_farm_05.abandoned_tank_farm_05 from raw samples with raw size 3 bytes
latent_webdataset.py - [rank {rank}] Loading encoder_hidden_states and attention_mask from .emb format
latent_webdataset.py - üìù [rank=0] loading abandoned_tiled_room.abandoned_tiled_room from raw samples with raw size 3 bytes
latent_webdataset.py - [rank {rank}] Loading encoder_hidden_states and attention_mask from .emb format
latent_webdataset.py - üìù [rank=0] loading abandoned_waterworks.abandoned_waterworks from raw samples with raw size 3 bytes
latent_webdataset.py - [rank {rank}] Loading encoder_hidden_states and attention_mask from .emb format
latent_webdataset.py - üìù [rank=0] loading abandoned_workshop.abandoned_workshop from raw samples with raw size 3 bytes
latent_webdataset.py - [rank {rank}] Loading encoder_hidden_states and attention_mask from .emb format
latent_webdataset.py - üìù [rank=0] loading abandoned_workshop_02.abandoned_workshop_02 from raw samples with raw size 3 bytes
latent_webdataset.py - [rank {rank}] Loading encoder_hidden_states and attention_mask from .emb format
latent_webdataset.py - üìù [rank=0] loading adams_place_bridge.adams_place_bridge from raw samples with raw size 3 bytes
latent_webdataset.py - [rank {rank}] Loading encoder_hidden_states and attention_mask from .emb format
latent_webdataset.py - üìù [rank=0] loading aft_lounge.aft_lounge from raw samples with raw size 3 bytes
latent_webdataset.py - [rank {rank}] Loading encoder_hidden_states and attention_mask from .emb format
latent_webdataset.py - üìù [rank=0] loading ahornsteig.ahornsteig from raw samples with raw size 3 bytes
latent_webdataset.py - [rank {rank}] Loading encoder_hidden_states and attention_mask from .emb format
latent_webdataset.py - üìù [rank=0] loading aircraft_workshop_01.aircraft_workshop_01 from raw samples with raw size 3 bytes
latent_webdataset.py - [rank {rank}] Loading encoder_hidden_states and attention_mask from .emb format
latent_webdataset.py - üìù [rank=0] loading alps_field.alps_field from raw samples with raw size 3 bytes
latent_webdataset.py - [rank {rank}] Loading encoder_hidden_states and attention_mask from .emb format
latent_webdataset.py - üìù [rank=0] loading altanka.altanka from raw samples with raw size 3 bytes
latent_webdataset.py - [rank {rank}] Loading encoder_hidden_states and attention_mask from .emb format
latent_webdataset.py - üìù [rank=0] loading amphitheatre_zanzibar_fort.amphitheatre_zanzibar_fort from raw samples with raw size 3 bytes
latent_webdataset.py - [rank {rank}] Loading encoder_hidden_states and attention_mask from .emb format
latent_webdataset.py - üìù [rank=0] loading anniversary_lounge.anniversary_lounge from raw samples with raw size 3 bytes
latent_webdataset.py - [rank {rank}] Loading encoder_hidden_states and attention_mask from .emb format
latent_webdataset.py - üìù [rank=0] loading approaching_storm.approaching_storm from raw samples with raw size 3 bytes
latent_webdataset.py - [rank {rank}] Loading encoder_hidden_states and attention_mask from .emb format
latent_webdataset.py - üìù [rank=0] loading arboretum.arboretum from raw samples with raw size 3 bytes
latent_webdataset.py - [rank {rank}] Loading encoder_hidden_states and attention_mask from .emb format
latent_webdataset.py - üìù [rank=0] loading aristea_wreck.aristea_wreck from raw samples with raw size 3 bytes
latent_webdataset.py - [rank {rank}] Loading encoder_hidden_states and attention_mask from .emb format
latent_webdataset.py - üìù [rank=0] loading aristea_wreck_puresky.aristea_wreck_puresky from raw samples with raw size 3 bytes
latent_webdataset.py - [rank {rank}] Loading encoder_hidden_states and attention_mask from .emb format
latent_webdataset.py - üìù [rank=0] loading art_studio.art_studio from raw samples with raw size 3 bytes
latent_webdataset.py - [rank {rank}] Loading encoder_hidden_states and attention_mask from .emb format
latent_webdataset.py - üìù [rank=0] loading artist_workshop.artist_workshop from raw samples with raw size 3 bytes
latent_webdataset.py - [rank {rank}] Loading encoder_hidden_states and attention_mask from .emb format
latent_webdataset.py - üìù [rank=0] loading auto_service.auto_service from raw samples with raw size 3 bytes
latent_webdataset.py - [rank {rank}] Loading encoder_hidden_states and attention_mask from .emb format
latent_webdataset.py - üìù [rank=0] loading autumn_crossing.autumn_crossing from raw samples with raw size 3 bytes
latent_webdataset.py - [rank {rank}] Loading encoder_hidden_states and attention_mask from .emb format
latent_webdataset.py - üìù [rank=0] loading autumn_field_puresky.autumn_field_puresky from raw samples with raw size 3 bytes
latent_webdataset.py - [rank {rank}] Loading encoder_hidden_states and attention_mask from .emb format
latent_webdataset.py - üìù [rank=0] loading autumn_forest_01.autumn_forest_01 from raw samples with raw size 3 bytes
latent_webdataset.py - [rank {rank}] Loading encoder_hidden_states and attention_mask from .emb format
latent_webdataset.py - üìù [rank=0] loading autumn_forest_02.autumn_forest_02 from raw samples with raw size 3 bytes
latent_webdataset.py - [rank {rank}] Loading encoder_hidden_states and attention_mask from .emb format
latent_webdataset.py - üìù [rank=0] loading autumn_forest_03.autumn_forest_03 from raw samples with raw size 3 bytes
latent_webdataset.py - [rank {rank}] Loading encoder_hidden_states and attention_mask from .emb format
latent_webdataset.py - üìù [rank=0] loading autumn_forest_04.autumn_forest_04 from raw samples with raw size 3 bytes
latent_webdataset.py - [rank {rank}] Loading encoder_hidden_states and attention_mask from .emb format
latent_webdataset.py - üìù [rank=0] loading autumn_ground.autumn_ground from raw samples with raw size 3 bytes
latent_webdataset.py - [rank {rank}] Loading encoder_hidden_states and attention_mask from .emb format
latent_webdataset.py - üìù [rank=0] loading autumn_hockey.autumn_hockey from raw samples with raw size 3 bytes
latent_webdataset.py - [rank {rank}] Loading encoder_hidden_states and attention_mask from .emb format
latent_webdataset.py - üìù [rank=0] loading autumn_meadow.autumn_meadow from raw samples with raw size 3 bytes
latent_webdataset.py - [rank {rank}] Loading encoder_hidden_states and attention_mask from .emb format
latent_webdataset.py - üìù [rank=0] loading autumn_park.autumn_park from raw samples with raw size 3 bytes
latent_webdataset.py - [rank {rank}] Loading encoder_hidden_states and attention_mask from .emb format
latent_webdataset.py - üìù [rank=0] loading autumn_road.autumn_road from raw samples with raw size 3 bytes
latent_webdataset.py - [rank {rank}] Loading encoder_hidden_states and attention_mask from .emb format
latent_webdataset.py - üìù [rank=0] loading aviation_museum.aviation_museum from raw samples with raw size 3 bytes
latent_webdataset.py - [rank {rank}] Loading encoder_hidden_states and attention_mask from .emb format
latent_webdataset.py - üìù [rank=0] loading aviation_museum_hill.aviation_museum_hill from raw samples with raw size 3 bytes
latent_webdataset.py - [rank {rank}] Loading encoder_hidden_states and attention_mask from .emb format
latent_webdataset.py - üìù [rank=0] loading balcony.balcony from raw samples with raw size 3 bytes
latent_webdataset.py - [rank {rank}] Loading encoder_hidden_states and attention_mask from .emb format
latent_webdataset.py - üìù [rank=0] loading ballawley_park.ballawley_park from raw samples with raw size 3 bytes
latent_webdataset.py - [rank {rank}] Loading encoder_hidden_states and attention_mask from .emb format
latent_webdataset.py - üìù [rank=0] loading ballroom.ballroom from raw samples with raw size 3 bytes
latent_webdataset.py - [rank {rank}] Loading encoder_hidden_states and attention_mask from .emb format
latent_webdataset.py - üìù [rank=0] loading bambanani_sunset.bambanani_sunset from raw samples with raw size 3 bytes
latent_webdataset.py - [rank {rank}] Loading encoder_hidden_states and attention_mask from .emb format
latent_webdataset.py - üìù [rank=0] loading bank_vault.bank_vault from raw samples with raw size 3 bytes
latent_webdataset.py - [rank {rank}] Loading encoder_hidden_states and attention_mask from .emb format
latent_webdataset.py - üìù [rank=0] loading basement_boxing_ring.basement_boxing_ring from raw samples with raw size 3 bytes
latent_webdataset.py - [rank {rank}] Loading encoder_hidden_states and attention_mask from .emb format
latent_webdataset.py - üìù [rank=0] loading bathroom.bathroom from raw samples with raw size 3 bytes
latent_webdataset.py - [rank {rank}] Loading encoder_hidden_states and attention_mask from .emb format
latent_webdataset.py - üìù [rank=0] loading beach_cloudy_bridge.beach_cloudy_bridge from raw samples with raw size 3 bytes
latent_webdataset.py - [rank {rank}] Loading encoder_hidden_states and attention_mask from .emb format
latent_webdataset.py - üìù [rank=0] loading beach_parking.beach_parking from raw samples with raw size 3 bytes
latent_webdataset.py - [rank {rank}] Loading encoder_hidden_states and attention_mask from .emb format
latent_webdataset.py - üìù [rank=0] loading belfast_farmhouse.belfast_farmhouse from raw samples with raw size 3 bytes
latent_webdataset.py - [rank {rank}] Loading encoder_hidden_states and attention_mask from .emb format
latent_webdataset.py - üìù [rank=0] loading belfast_open_field.belfast_open_field from raw samples with raw size 3 bytes
latent_webdataset.py - [rank {rank}] Loading encoder_hidden_states and attention_mask from .emb format
latent_webdataset.py - üìù [rank=0] loading belfast_sunset.belfast_sunset from raw samples with raw size 3 bytes
latent_webdataset.py - [rank {rank}] Loading encoder_hidden_states and attention_mask from .emb format
latent_webdataset.py - üìù [rank=0] loading bell_park_dawn.bell_park_dawn from raw samples with raw size 3 bytes
latent_webdataset.py - [rank {rank}] Loading encoder_hidden_states and attention_mask from .emb format
latent_webdataset.py - üìù [rank=0] loading bell_park_pier.bell_park_pier from raw samples with raw size 3 bytes
latent_webdataset.py - [rank {rank}] Loading encoder_hidden_states and attention_mask from .emb format
latent_webdataset.py - üìù [rank=0] loading belvedere.belvedere from raw samples with raw size 3 bytes
latent_webdataset.py - [rank {rank}] Loading encoder_hidden_states and attention_mask from .emb format
latent_webdataset.py - üìù [rank=0] loading bergen.bergen from raw samples with raw size 3 bytes
latent_webdataset.py - [rank {rank}] Loading encoder_hidden_states and attention_mask from .emb format
latent_webdataset.py - üìù [rank=0] loading bethnal_green_entrance.bethnal_green_entrance from raw samples with raw size 3 bytes
latent_webdataset.py - [rank {rank}] Loading encoder_hidden_states and attention_mask from .emb format
latent_webdataset.py - üìù [rank=0] loading between_bridges.between_bridges from raw samples with raw size 3 bytes
latent_webdataset.py - [rank {rank}] Loading encoder_hidden_states and attention_mask from .emb format
latent_webdataset.py - üìù [rank=0] loading billiard_hall.billiard_hall from raw samples with raw size 3 bytes
latent_webdataset.py - [rank {rank}] Loading encoder_hidden_states and attention_mask from .emb format
latent_webdataset.py - üìù [rank=0] loading binnenalster.binnenalster from raw samples with raw size 3 bytes
latent_webdataset.py - [rank {rank}] Loading encoder_hidden_states and attention_mask from .emb format
latent_webdataset.py - üìù [rank=0] loading bismarckturm.bismarckturm from raw samples with raw size 3 bytes
latent_webdataset.py - [rank {rank}] Loading encoder_hidden_states and attention_mask from .emb format
latent_webdataset.py - üìù [rank=0] loading bismarckturm_hillside.bismarckturm_hillside from raw samples with raw size 3 bytes
latent_webdataset.py - [rank {rank}] Loading encoder_hidden_states and attention_mask from .emb format
latent_webdataset.py - üìù [rank=0] loading blau_river.blau_river from raw samples with raw size 3 bytes
latent_webdataset.py - [rank {rank}] Loading encoder_hidden_states and attention_mask from .emb format
latent_webdataset.py - üìù [rank=0] loading blaubeuren_church_square.blaubeuren_church_square from raw samples with raw size 3 bytes
latent_webdataset.py - [rank {rank}] Loading encoder_hidden_states and attention_mask from .emb format
latent_webdataset.py - üìù [rank=0] loading blaubeuren_hillside.blaubeuren_hillside from raw samples with raw size 3 bytes
latent_webdataset.py - [rank {rank}] Loading encoder_hidden_states and attention_mask from .emb format
latent_webdataset.py - üìù [rank=0] loading blaubeuren_night.blaubeuren_night from raw samples with raw size 3 bytes
latent_webdataset.py - [rank {rank}] Loading encoder_hidden_states and attention_mask from .emb format
latent_webdataset.py - üìù [rank=0] loading blaubeuren_outskirts.blaubeuren_outskirts from raw samples with raw size 3 bytes
latent_webdataset.py - [rank {rank}] Loading encoder_hidden_states and attention_mask from .emb format
latent_webdataset.py - üìù [rank=0] loading blinds.blinds from raw samples with raw size 3 bytes
latent_webdataset.py - [rank {rank}] Loading encoder_hidden_states and attention_mask from .emb format
latent_webdataset.py - üìù [rank=0] loading blocky_photo_studio.blocky_photo_studio from raw samples with raw size 3 bytes
latent_webdataset.py - [rank {rank}] Loading encoder_hidden_states and attention_mask from .emb format
latent_webdataset.py - üìù [rank=0] loading bloem_hill_01.bloem_hill_01 from raw samples with raw size 3 bytes
latent_webdataset.py - [rank {rank}] Loading encoder_hidden_states and attention_mask from .emb format
latent_webdataset.py - üìù [rank=0] loading bloem_train_track_clear.bloem_train_track_clear from raw samples with raw size 3 bytes
latent_webdataset.py - [rank {rank}] Loading encoder_hidden_states and attention_mask from .emb format
latent_webdataset.py - üìù [rank=0] loading blouberg_sunrise_2.blouberg_sunrise_2 from raw samples with raw size 3 bytes
latent_webdataset.py - [rank {rank}] Loading encoder_hidden_states and attention_mask from .emb format
latent_webdataset.py - üìù [rank=0] loading blue_grotto.blue_grotto from raw samples with raw size 3 bytes
latent_webdataset.py - [rank {rank}] Loading encoder_hidden_states and attention_mask from .emb format
latent_webdataset.py - üìù [rank=0] loading blue_lagoon.blue_lagoon from raw samples with raw size 3 bytes
latent_webdataset.py - [rank {rank}] Loading encoder_hidden_states and attention_mask from .emb format
latent_webdataset.py - üìù [rank=0] loading blue_photo_studio.blue_photo_studio from raw samples with raw size 3 bytes
latent_webdataset.py - [rank {rank}] Loading encoder_hidden_states and attention_mask from .emb format
latent_webdataset.py - üìù [rank=0] loading boiler_room.boiler_room from raw samples with raw size 3 bytes
latent_webdataset.py - [rank {rank}] Loading encoder_hidden_states and attention_mask from .emb format
latent_webdataset.py - üìù [rank=0] loading boma.boma from raw samples with raw size 3 bytes
latent_webdataset.py - [rank {rank}] Loading encoder_hidden_states and attention_mask from .emb format
latent_webdataset.py - üìù [rank=0] loading brick_factory_02.brick_factory_02 from raw samples with raw size 3 bytes
latent_webdataset.py - [rank {rank}] Loading encoder_hidden_states and attention_mask from .emb format
latent_webdataset.py - üìù [rank=0] loading brown_photostudio_01.brown_photostudio_01 from raw samples with raw size 3 bytes
latent_webdataset.py - [rank {rank}] Loading encoder_hidden_states and attention_mask from .emb format
latent_webdataset.py - üìù [rank=0] loading brown_photostudio_02.brown_photostudio_02 from raw samples with raw size 3 bytes
latent_webdataset.py - [rank {rank}] Loading encoder_hidden_states and attention_mask from .emb format
latent_webdataset.py - üìù [rank=0] loading brown_photostudio_03.brown_photostudio_03 from raw samples with raw size 3 bytes
latent_webdataset.py - [rank {rank}] Loading encoder_hidden_states and attention_mask from .emb format
latent_webdataset.py - üìù [rank=0] loading brown_photostudio_04.brown_photostudio_04 from raw samples with raw size 3 bytes
latent_webdataset.py - [rank {rank}] Loading encoder_hidden_states and attention_mask from .emb format
latent_webdataset.py - üìù [rank=0] loading brown_photostudio_05.brown_photostudio_05 from raw samples with raw size 3 bytes
latent_webdataset.py - [rank {rank}] Loading encoder_hidden_states and attention_mask from .emb format
latent_webdataset.py - üìù [rank=0] loading brown_photostudio_06.brown_photostudio_06 from raw samples with raw size 3 bytes
latent_webdataset.py - [rank {rank}] Loading encoder_hidden_states and attention_mask from .emb format
latent_webdataset.py - üìù [rank=0] loading brown_photostudio_07.brown_photostudio_07 from raw samples with raw size 3 bytes
latent_webdataset.py - [rank {rank}] Loading encoder_hidden_states and attention_mask from .emb format
latent_webdataset.py - üìù [rank=0] loading buikslotermeerplein.buikslotermeerplein from raw samples with raw size 3 bytes
latent_webdataset.py - [rank {rank}] Loading encoder_hidden_states and attention_mask from .emb format
latent_webdataset.py - üìù [rank=0] loading bush_restaurant.bush_restaurant from raw samples with raw size 3 bytes
latent_webdataset.py - [rank {rank}] Loading encoder_hidden_states and attention_mask from .emb format
latent_webdataset.py - üìù [rank=0] loading cabin.cabin from raw samples with raw size 3 bytes
latent_webdataset.py - [rank {rank}] Loading encoder_hidden_states and attention_mask from .emb format
latent_webdataset.py - üìù [rank=0] loading cambridge.cambridge from raw samples with raw size 3 bytes
latent_webdataset.py - [rank {rank}] Loading encoder_hidden_states and attention_mask from .emb format
latent_webdataset.py - üìù [rank=0] loading canary_wharf.canary_wharf from raw samples with raw size 3 bytes
latent_webdataset.py - [rank {rank}] Loading encoder_hidden_states and attention_mask from .emb format
latent_webdataset.py - üìù [rank=0] loading cannon.cannon from raw samples with raw size 3 bytes
latent_webdataset.py - [rank {rank}] Loading encoder_hidden_states and attention_mask from .emb format
latent_webdataset.py - üìù [rank=0] loading cape_hill.cape_hill from raw samples with raw size 3 bytes
latent_webdataset.py - [rank {rank}] Loading encoder_hidden_states and attention_mask from .emb format
latent_webdataset.py - üìù [rank=0] loading carpentry_shop_01.carpentry_shop_01 from raw samples with raw size 3 bytes
latent_webdataset.py - [rank {rank}] Loading encoder_hidden_states and attention_mask from .emb format
latent_webdataset.py - üìù [rank=0] loading carpentry_shop_02.carpentry_shop_02 from raw samples with raw size 3 bytes
latent_webdataset.py - [rank {rank}] Loading encoder_hidden_states and attention_mask from .emb format
latent_webdataset.py - üìù [rank=0] loading castel_st_angelo_roof.castel_st_angelo_roof from raw samples with raw size 3 bytes
latent_webdataset.py - [rank {rank}] Loading encoder_hidden_states and attention_mask from .emb format
latent_webdataset.py - üìù [rank=0] loading castle_zavelstein_cellar.castle_zavelstein_cellar from raw samples with raw size 3 bytes
latent_webdataset.py - [rank {rank}] Loading encoder_hidden_states and attention_mask from .emb format
latent_webdataset.py - üìù [rank=0] loading cave_wall.cave_wall from raw samples with raw size 3 bytes
latent_webdataset.py - [rank {rank}] Loading encoder_hidden_states and attention_mask from .emb format
latent_webdataset.py - üìù [rank=0] loading cayley_interior.cayley_interior from raw samples with raw size 3 bytes
latent_webdataset.py - [rank {rank}] Loading encoder_hidden_states and attention_mask from .emb format
latent_webdataset.py - üìù [rank=0] loading cedar_bridge.cedar_bridge from raw samples with raw size 3 bytes
latent_webdataset.py - [rank {rank}] Loading encoder_hidden_states and attention_mask from .emb format
latent_webdataset.py - üìù [rank=0] loading champagne_castle_1.champagne_castle_1 from raw samples with raw size 3 bytes
latent_webdataset.py - [rank {rank}] Loading encoder_hidden_states and attention_mask from .emb format
latent_webdataset.py - üìù [rank=0] loading chapel_day.chapel_day from raw samples with raw size 3 bytes
latent_webdataset.py - [rank {rank}] Loading encoder_hidden_states and attention_mask from .emb format
latent_webdataset.py - üìù [rank=0] loading chapmans_drive.chapmans_drive from raw samples with raw size 3 bytes
latent_webdataset.py - [rank {rank}] Loading encoder_hidden_states and attention_mask from .emb format
latent_webdataset.py - üìù [rank=0] loading childrens_hospital.childrens_hospital from raw samples with raw size 3 bytes
latent_webdataset.py - [rank {rank}] Loading encoder_hidden_states and attention_mask from .emb format
latent_webdataset.py - üìù [rank=0] loading chinese_garden.chinese_garden from raw samples with raw size 3 bytes
latent_webdataset.py - [rank {rank}] Loading encoder_hidden_states and attention_mask from .emb format
latent_webdataset.py - üìù [rank=0] loading christmas_photo_studio_01.christmas_photo_studio_01 from raw samples with raw size 3 bytes
latent_webdataset.py - [rank {rank}] Loading encoder_hidden_states and attention_mask from .emb format
latent_webdataset.py - üìù [rank=0] loading christmas_photo_studio_02.christmas_photo_studio_02 from raw samples with raw size 3 bytes
latent_webdataset.py - [rank {rank}] Loading encoder_hidden_states and attention_mask from .emb format
latent_webdataset.py - üìù [rank=0] loading christmas_photo_studio_03.christmas_photo_studio_03 from raw samples with raw size 3 bytes
latent_webdataset.py - [rank {rank}] Loading encoder_hidden_states and attention_mask from .emb format
latent_webdataset.py - üìù [rank=0] loading christmas_photo_studio_04.christmas_photo_studio_04 from raw samples with raw size 3 bytes
latent_webdataset.py - [rank {rank}] Loading encoder_hidden_states and attention_mask from .emb format
latent_webdataset.py - üìù [rank=0] loading christmas_photo_studio_05.christmas_photo_studio_05 from raw samples with raw size 3 bytes
latent_webdataset.py - [rank {rank}] Loading encoder_hidden_states and attention_mask from .emb format
latent_webdataset.py - üìù [rank=0] loading christmas_photo_studio_06.christmas_photo_studio_06 from raw samples with raw size 3 bytes
latent_webdataset.py - [rank {rank}] Loading encoder_hidden_states and attention_mask from .emb format
latent_webdataset.py - üìù [rank=0] loading christmas_photo_studio_07.christmas_photo_studio_07 from raw samples with raw size 3 bytes
latent_webdataset.py - [rank {rank}] Loading encoder_hidden_states and attention_mask from .emb format
latent_webdataset.py - üìù [rank=0] loading cinema_hall.cinema_hall from raw samples with raw size 3 bytes
latent_webdataset.py - [rank {rank}] Loading encoder_hidden_states and attention_mask from .emb format
latent_webdataset.py - üìù [rank=0] loading circus_arena.circus_arena from raw samples with raw size 3 bytes
latent_webdataset.py - [rank {rank}] Loading encoder_hidden_states and attention_mask from .emb format
latent_webdataset.py - üìù [rank=0] loading circus_maximus_1.circus_maximus_1 from raw samples with raw size 3 bytes
latent_webdataset.py - [rank {rank}] Loading encoder_hidden_states and attention_mask from .emb format
latent_webdataset.py - üìù [rank=0] loading circus_maximus_2.circus_maximus_2 from raw samples with raw size 3 bytes
latent_webdataset.py - [rank {rank}] Loading encoder_hidden_states and attention_mask from .emb format
latent_webdataset.py - üìù [rank=0] loading citrus_orchard.citrus_orchard from raw samples with raw size 3 bytes
latent_webdataset.py - [rank {rank}] Loading encoder_hidden_states and attention_mask from .emb format
latent_webdataset.py - üìù [rank=0] loading citrus_orchard_road.citrus_orchard_road from raw samples with raw size 3 bytes
latent_webdataset.py - [rank {rank}] Loading encoder_hidden_states and attention_mask from .emb format
latent_webdataset.py - üìù [rank=0] loading clarens_midday.clarens_midday from raw samples with raw size 3 bytes
latent_webdataset.py - [rank {rank}] Loading encoder_hidden_states and attention_mask from .emb format
latent_webdataset.py - üìù [rank=0] loading clarens_night_01.clarens_night_01 from raw samples with raw size 3 bytes
latent_webdataset.py - [rank {rank}] Loading encoder_hidden_states and attention_mask from .emb format
latent_webdataset.py - üìù [rank=0] loading clarens_night_02.clarens_night_02 from raw samples with raw size 3 bytes
latent_webdataset.py - [rank {rank}] Loading encoder_hidden_states and attention_mask from .emb format
latent_webdataset.py - üìù [rank=0] loading cliffside.cliffside from raw samples with raw size 3 bytes
latent_webdataset.py - [rank {rank}] Loading encoder_hidden_states and attention_mask from .emb format
latent_webdataset.py - üìù [rank=0] loading cloud_layers.cloud_layers from raw samples with raw size 3 bytes
latent_webdataset.py - [rank {rank}] Loading encoder_hidden_states and attention_mask from .emb format
latent_webdataset.py - üìù [rank=0] loading cloudy_cliffside_road.cloudy_cliffside_road from raw samples with raw size 3 bytes
latent_webdataset.py - [rank {rank}] Loading encoder_hidden_states and attention_mask from .emb format
latent_webdataset.py - üìù [rank=0] loading cloudy_vondelpark.cloudy_vondelpark from raw samples with raw size 3 bytes
latent_webdataset.py - [rank {rank}] Loading encoder_hidden_states and attention_mask from .emb format
latent_webdataset.py - üìù [rank=0] loading cobblestone_street_night.cobblestone_street_night from raw samples with raw size 3 bytes
latent_webdataset.py - [rank {rank}] Loading encoder_hidden_states and attention_mask from .emb format
latent_webdataset.py - üìù [rank=0] loading colorful_studio.colorful_studio from raw samples with raw size 3 bytes
latent_webdataset.py - [rank {rank}] Loading encoder_hidden_states and attention_mask from .emb format
latent_webdataset.py - üìù [rank=0] loading colosseum.colosseum from raw samples with raw size 3 bytes
latent_webdataset.py - [rank {rank}] Loading encoder_hidden_states and attention_mask from .emb format
latent_webdataset.py - üìù [rank=0] loading combination_room.combination_room from raw samples with raw size 3 bytes
latent_webdataset.py - [rank {rank}] Loading encoder_hidden_states and attention_mask from .emb format
latent_webdataset.py - üìù [rank=0] loading concrete_tunnel.concrete_tunnel from raw samples with raw size 3 bytes
latent_webdataset.py - [rank {rank}] Loading encoder_hidden_states and attention_mask from .emb format
latent_webdataset.py - üìù [rank=0] loading concrete_tunnel_02.concrete_tunnel_02 from raw samples with raw size 3 bytes
latent_webdataset.py - [rank {rank}] Loading encoder_hidden_states and attention_mask from .emb format
latent_webdataset.py - üìù [rank=0] loading construction_yard.construction_yard from raw samples with raw size 3 bytes
latent_webdataset.py - [rank {rank}] Loading encoder_hidden_states and attention_mask from .emb format
latent_webdataset.py - üìù [rank=0] loading countrytrax_midday.countrytrax_midday from raw samples with raw size 3 bytes
latent_webdataset.py - [rank {rank}] Loading encoder_hidden_states and attention_mask from .emb format
latent_webdataset.py - üìù [rank=0] loading courtyard.courtyard from raw samples with raw size 3 bytes
latent_webdataset.py - [rank {rank}] Loading encoder_hidden_states and attention_mask from .emb format
latent_webdataset.py - üìù [rank=0] loading creepy_bathroom.creepy_bathroom from raw samples with raw size 3 bytes
latent_webdataset.py - [rank {rank}] Loading encoder_hidden_states and attention_mask from .emb format
latent_webdataset.py - üìù [rank=0] loading crosswalk.crosswalk from raw samples with raw size 3 bytes
latent_webdataset.py - [rank {rank}] Loading encoder_hidden_states and attention_mask from .emb format
latent_webdataset.py - üìù [rank=0] loading crystal_falls.crystal_falls from raw samples with raw size 3 bytes
latent_webdataset.py - [rank {rank}] Loading encoder_hidden_states and attention_mask from .emb format
latent_webdataset.py - üìù [rank=0] loading cyclorama_hard_light.cyclorama_hard_light from raw samples with raw size 3 bytes
latent_webdataset.py - [rank {rank}] Loading encoder_hidden_states and attention_mask from .emb format
latent_webdataset.py - üìù [rank=0] loading dalkey_view.dalkey_view from raw samples with raw size 3 bytes
latent_webdataset.py - [rank {rank}] Loading encoder_hidden_states and attention_mask from .emb format
latent_webdataset.py - üìù [rank=0] loading dam_bridge.dam_bridge from raw samples with raw size 3 bytes
latent_webdataset.py - [rank {rank}] Loading encoder_hidden_states and attention_mask from .emb format
latent_webdataset.py - üìù [rank=0] loading dam_road.dam_road from raw samples with raw size 3 bytes
latent_webdataset.py - [rank {rank}] Loading encoder_hidden_states and attention_mask from .emb format
latent_webdataset.py - üìù [rank=0] loading dam_wall.dam_wall from raw samples with raw size 3 bytes
latent_webdataset.py - [rank {rank}] Loading encoder_hidden_states and attention_mask from .emb format
latent_webdataset.py - üìù [rank=0] loading dancing_hall.dancing_hall from raw samples with raw size 3 bytes
latent_webdataset.py - [rank {rank}] Loading encoder_hidden_states and attention_mask from .emb format
latent_webdataset.py - üìù [rank=0] loading dark_autumn_forest.dark_autumn_forest from raw samples with raw size 3 bytes
latent_webdataset.py - [rank {rank}] Loading encoder_hidden_states and attention_mask from .emb format
latent_webdataset.py - üìù [rank=0] loading de_balie.de_balie from raw samples with raw size 3 bytes
latent_webdataset.py - [rank {rank}] Loading encoder_hidden_states and attention_mask from .emb format
latent_webdataset.py - üìù [rank=0] loading decor_shop.decor_shop from raw samples with raw size 3 bytes
latent_webdataset.py - [rank {rank}] Loading encoder_hidden_states and attention_mask from .emb format
latent_webdataset.py - üìù [rank=0] loading delta_2.delta_2 from raw samples with raw size 3 bytes
latent_webdataset.py - [rank {rank}] Loading encoder_hidden_states and attention_mask from .emb format
latent_webdataset.py - üìù [rank=0] loading derelict_highway_midday.derelict_highway_midday from raw samples with raw size 3 bytes
latent_webdataset.py - [rank {rank}] Loading encoder_hidden_states and attention_mask from .emb format
latent_webdataset.py - üìù [rank=0] loading derelict_highway_noon.derelict_highway_noon from raw samples with raw size 3 bytes
latent_webdataset.py - [rank {rank}] Loading encoder_hidden_states and attention_mask from .emb format
latent_webdataset.py - üìù [rank=0] loading derelict_overpass.derelict_overpass from raw samples with raw size 3 bytes
latent_webdataset.py - [rank {rank}] Loading encoder_hidden_states and attention_mask from .emb format
latent_webdataset.py - üìù [rank=0] loading derelict_underpass.derelict_underpass from raw samples with raw size 3 bytes
latent_webdataset.py - [rank {rank}] Loading encoder_hidden_states and attention_mask from .emb format
latent_webdataset.py - üìù [rank=0] loading dikhololo_night.dikhololo_night from raw samples with raw size 3 bytes
latent_webdataset.py - [rank {rank}] Loading encoder_hidden_states and attention_mask from .emb format
latent_webdataset.py - üìù [rank=0] loading dikhololo_sunset.dikhololo_sunset from raw samples with raw size 3 bytes
latent_webdataset.py - [rank {rank}] Loading encoder_hidden_states and attention_mask from .emb format
latent_webdataset.py - üìù [rank=0] loading dirt_bike_track_01.dirt_bike_track_01 from raw samples with raw size 3 bytes
latent_webdataset.py - [rank {rank}] Loading encoder_hidden_states and attention_mask from .emb format
latent_webdataset.py - üìù [rank=0] loading distribution_board.distribution_board from raw samples with raw size 3 bytes
latent_webdataset.py - [rank {rank}] Loading encoder_hidden_states and attention_mask from .emb format
latent_webdataset.py - üìù [rank=0] loading docklands_01.docklands_01 from raw samples with raw size 3 bytes
latent_webdataset.py - [rank {rank}] Loading encoder_hidden_states and attention_mask from .emb format
latent_webdataset.py - üìù [rank=0] loading docklands_02.docklands_02 from raw samples with raw size 3 bytes
latent_webdataset.py - [rank {rank}] Loading encoder_hidden_states and attention_mask from .emb format
latent_webdataset.py - üìù [rank=0] loading drachenfels_cellar.drachenfels_cellar from raw samples with raw size 3 bytes
latent_webdataset.py - [rank {rank}] Loading encoder_hidden_states and attention_mask from .emb format
latent_webdataset.py - üìù [rank=0] loading drackenstein_quarry.drackenstein_quarry from raw samples with raw size 3 bytes
latent_webdataset.py - [rank {rank}] Loading encoder_hidden_states and attention_mask from .emb format
latent_webdataset.py - üìù [rank=0] loading drackenstein_quarry_puresky.drackenstein_quarry_puresky from raw samples with raw size 3 bytes
latent_webdataset.py - [rank {rank}] Loading encoder_hidden_states and attention_mask from .emb format
latent_webdataset.py - üìù [rank=0] loading drakensberg_solitary_mountain.drakensberg_solitary_mountain from raw samples with raw size 3 bytes
latent_webdataset.py - [rank {rank}] Loading encoder_hidden_states and attention_mask from .emb format
latent_webdataset.py - üìù [rank=0] loading drakensberg_solitary_mountain_puresky.drakensberg_solitary_mountain_puresky from raw samples with raw size 3 bytes
latent_webdataset.py - [rank {rank}] Loading encoder_hidden_states and attention_mask from .emb format
latent_webdataset.py - üìù [rank=0] loading dreifaltigkeitsberg.dreifaltigkeitsberg from raw samples with raw size 3 bytes
latent_webdataset.py - [rank {rank}] Loading encoder_hidden_states and attention_mask from .emb format
latent_webdataset.py - üìù [rank=0] loading dresden_square.dresden_square from raw samples with raw size 3 bytes
latent_webdataset.py - [rank {rank}] Loading encoder_hidden_states and attention_mask from .emb format
latent_webdataset.py - üìù [rank=0] loading dresden_station_night.dresden_station_night from raw samples with raw size 3 bytes
latent_webdataset.py - [rank {rank}] Loading encoder_hidden_states and attention_mask from .emb format
latent_webdataset.py - üìù [rank=0] loading driving_school.driving_school from raw samples with raw size 3 bytes
latent_webdataset.py - [rank {rank}] Loading encoder_hidden_states and attention_mask from .emb format
latent_webdataset.py - üìù [rank=0] loading dry_cracked_lake.dry_cracked_lake from raw samples with raw size 3 bytes
latent_webdataset.py - [rank {rank}] Loading encoder_hidden_states and attention_mask from .emb format
latent_webdataset.py - üìù [rank=0] loading dry_field.dry_field from raw samples with raw size 3 bytes
latent_webdataset.py - [rank {rank}] Loading encoder_hidden_states and attention_mask from .emb format
latent_webdataset.py - üìù [rank=0] loading dry_hay_field.dry_hay_field from raw samples with raw size 3 bytes
latent_webdataset.py - [rank {rank}] Loading encoder_hidden_states and attention_mask from .emb format
latent_webdataset.py - üìù [rank=0] loading dry_orchard_meadow.dry_orchard_meadow from raw samples with raw size 3 bytes
latent_webdataset.py - [rank {rank}] Loading encoder_hidden_states and attention_mask from .emb format
latent_webdataset.py - üìù [rank=0] loading dusseldorf_bridge.dusseldorf_bridge from raw samples with raw size 3 bytes
latent_webdataset.py - [rank {rank}] Loading encoder_hidden_states and attention_mask from .emb format
latent_webdataset.py - üìù [rank=0] loading ehingen_hillside.ehingen_hillside from raw samples with raw size 3 bytes
latent_webdataset.py - [rank {rank}] Loading encoder_hidden_states and attention_mask from .emb format
latent_webdataset.py - üìù [rank=0] loading ehingen_hillside_02.ehingen_hillside_02 from raw samples with raw size 3 bytes
latent_webdataset.py - [rank {rank}] Loading encoder_hidden_states and attention_mask from .emb format
latent_webdataset.py - üìù [rank=0] loading eilenriede_labyrinth.eilenriede_labyrinth from raw samples with raw size 3 bytes
latent_webdataset.py - [rank {rank}] Loading encoder_hidden_states and attention_mask from .emb format
latent_webdataset.py - üìù [rank=0] loading eilenriede_park.eilenriede_park from raw samples with raw size 3 bytes
latent_webdataset.py - [rank {rank}] Loading encoder_hidden_states and attention_mask from .emb format
latent_webdataset.py - üìù [rank=0] loading emmarentia.emmarentia from raw samples with raw size 3 bytes
latent_webdataset.py - [rank {rank}] Loading encoder_hidden_states and attention_mask from .emb format
latent_webdataset.py - üìù [rank=0] loading empty_play_room.empty_play_room from raw samples with raw size 3 bytes
latent_webdataset.py - [rank {rank}] Loading encoder_hidden_states and attention_mask from .emb format
latent_webdataset.py - üìù [rank=0] loading empty_warehouse_01.empty_warehouse_01 from raw samples with raw size 3 bytes
latent_webdataset.py - [rank {rank}] Loading encoder_hidden_states and attention_mask from .emb format
latent_webdataset.py - üìù [rank=0] loading empty_workshop.empty_workshop from raw samples with raw size 3 bytes
latent_webdataset.py - [rank {rank}] Loading encoder_hidden_states and attention_mask from .emb format
latent_webdataset.py - üìù [rank=0] loading en_suite.en_suite from raw samples with raw size 3 bytes
latent_webdataset.py - [rank {rank}] Loading encoder_hidden_states and attention_mask from .emb format
latent_webdataset.py - üìù [rank=0] loading entrance_hall.entrance_hall from raw samples with raw size 3 bytes
latent_webdataset.py - [rank {rank}] Loading encoder_hidden_states and attention_mask from .emb format
latent_webdataset.py - üìù [rank=0] loading epping_forest_01.epping_forest_01 from raw samples with raw size 3 bytes
latent_webdataset.py - [rank {rank}] Loading encoder_hidden_states and attention_mask from .emb format
latent_webdataset.py - üìù [rank=0] loading epping_forest_02.epping_forest_02 from raw samples with raw size 3 bytes
latent_webdataset.py - [rank {rank}] Loading encoder_hidden_states and attention_mask from .emb format
latent_webdataset.py - üìù [rank=0] loading etzwihl.etzwihl from raw samples with raw size 3 bytes
latent_webdataset.py - [rank {rank}] Loading encoder_hidden_states and attention_mask from .emb format
latent_webdataset.py - üìù [rank=0] loading evening_field.evening_field from raw samples with raw size 3 bytes
latent_webdataset.py - [rank {rank}] Loading encoder_hidden_states and attention_mask from .emb format
latent_webdataset.py - üìù [rank=0] loading evening_road_01.evening_road_01 from raw samples with raw size 3 bytes
latent_webdataset.py - [rank {rank}] Loading encoder_hidden_states and attention_mask from .emb format
latent_webdataset.py - üìù [rank=0] loading evening_road_01_puresky.evening_road_01_puresky from raw samples with raw size 3 bytes
latent_webdataset.py - [rank {rank}] Loading encoder_hidden_states and attention_mask from .emb format
latent_webdataset.py - üìù [rank=0] loading farm_field.farm_field from raw samples with raw size 3 bytes
latent_webdataset.py - [rank {rank}] Loading encoder_hidden_states and attention_mask from .emb format
latent_webdataset.py - üìù [rank=0] loading farm_sunset.farm_sunset from raw samples with raw size 3 bytes
latent_webdataset.py - [rank {rank}] Loading encoder_hidden_states and attention_mask from .emb format
latent_webdataset.py - üìù [rank=0] loading felsenlabyrinth.felsenlabyrinth from raw samples with raw size 3 bytes
latent_webdataset.py - [rank {rank}] Loading encoder_hidden_states and attention_mask from .emb format
latent_webdataset.py - üìù [rank=0] loading fish_eagle_hill.fish_eagle_hill from raw samples with raw size 3 bytes
latent_webdataset.py - [rank {rank}] Loading encoder_hidden_states and attention_mask from .emb format
latent_webdataset.py - üìù [rank=0] loading fish_hoek_beach.fish_hoek_beach from raw samples with raw size 3 bytes
latent_webdataset.py - [rank {rank}] Loading encoder_hidden_states and attention_mask from .emb format
latent_webdataset.py - üìù [rank=0] loading flamingo_pan.flamingo_pan from raw samples with raw size 3 bytes
latent_webdataset.py - [rank {rank}] Loading encoder_hidden_states and attention_mask from .emb format
latent_webdataset.py - üìù [rank=0] loading floral_tent.floral_tent from raw samples with raw size 3 bytes
latent_webdataset.py - [rank {rank}] Loading encoder_hidden_states and attention_mask from .emb format
latent_webdataset.py - üìù [rank=0] loading flower_road.flower_road from raw samples with raw size 3 bytes
latent_webdataset.py - [rank {rank}] Loading encoder_hidden_states and attention_mask from .emb format
latent_webdataset.py - üìù [rank=0] loading forest_cave.forest_cave from raw samples with raw size 3 bytes
latent_webdataset.py - [rank {rank}] Loading encoder_hidden_states and attention_mask from .emb format
latent_webdataset.py - üìù [rank=0] loading forest_grove.forest_grove from raw samples with raw size 3 bytes
latent_webdataset.py - [rank {rank}] Loading encoder_hidden_states and attention_mask from .emb format
latent_webdataset.py - üìù [rank=0] loading forest_slope.forest_slope from raw samples with raw size 3 bytes
latent_webdataset.py - [rank {rank}] Loading encoder_hidden_states and attention_mask from .emb format
latent_webdataset.py - üìù [rank=0] loading fort_schanskop_morning.fort_schanskop_morning from raw samples with raw size 3 bytes
latent_webdataset.py - [rank {rank}] Loading encoder_hidden_states and attention_mask from .emb format
latent_webdataset.py - üìù [rank=0] loading fouriesburg_mountain_cloudy.fouriesburg_mountain_cloudy from raw samples with raw size 3 bytes
latent_webdataset.py - [rank {rank}] Loading encoder_hidden_states and attention_mask from .emb format
latent_webdataset.py - üìù [rank=0] loading fouriesburg_mountain_lookout.fouriesburg_mountain_lookout from raw samples with raw size 3 bytes
latent_webdataset.py - [rank {rank}] Loading encoder_hidden_states and attention_mask from .emb format
latent_webdataset.py - üìù [rank=0] loading fouriesburg_mountain_lookout_2.fouriesburg_mountain_lookout_2 from raw samples with raw size 3 bytes
latent_webdataset.py - [rank {rank}] Loading encoder_hidden_states and attention_mask from .emb format
latent_webdataset.py - üìù [rank=0] loading fouriesburg_mountain_midday.fouriesburg_mountain_midday from raw samples with raw size 3 bytes
latent_webdataset.py - [rank {rank}] Loading encoder_hidden_states and attention_mask from .emb format
latent_webdataset.py - üìù [rank=0] loading freight_station.freight_station from raw samples with raw size 3 bytes
latent_webdataset.py - [rank {rank}] Loading encoder_hidden_states and attention_mask from .emb format
latent_webdataset.py - üìù [rank=0] loading frozen_lake.frozen_lake from raw samples with raw size 3 bytes
latent_webdataset.py - [rank {rank}] Loading encoder_hidden_states and attention_mask from .emb format
latent_webdataset.py - üìù [rank=0] loading furry_clouds.furry_clouds from raw samples with raw size 3 bytes
latent_webdataset.py - [rank {rank}] Loading encoder_hidden_states and attention_mask from .emb format
latent_webdataset.py - üìù [rank=0] loading future_parking.future_parking from raw samples with raw size 3 bytes
latent_webdataset.py - [rank {rank}] Loading encoder_hidden_states and attention_mask from .emb format
latent_webdataset.py - üìù [rank=0] loading gamrig.gamrig from raw samples with raw size 3 bytes
latent_webdataset.py - [rank {rank}] Loading encoder_hidden_states and attention_mask from .emb format
latent_webdataset.py - üìù [rank=0] loading garage.garage from raw samples with raw size 3 bytes
latent_webdataset.py - [rank {rank}] Loading encoder_hidden_states and attention_mask from .emb format
latent_webdataset.py - üìù [rank=0] loading gear_store.gear_store from raw samples with raw size 3 bytes
latent_webdataset.py - [rank {rank}] Loading encoder_hidden_states and attention_mask from .emb format
latent_webdataset.py - üìù [rank=0] loading geislingen_an_der_steige.geislingen_an_der_steige from raw samples with raw size 3 bytes
latent_webdataset.py - [rank {rank}] Loading encoder_hidden_states and attention_mask from .emb format
latent_webdataset.py - üìù [rank=0] loading georgentor.georgentor from raw samples with raw size 3 bytes
latent_webdataset.py - [rank {rank}] Loading encoder_hidden_states and attention_mask from .emb format
latent_webdataset.py - üìù [rank=0] loading glass_passage.glass_passage from raw samples with raw size 3 bytes
latent_webdataset.py - [rank {rank}] Loading encoder_hidden_states and attention_mask from .emb format
latent_webdataset.py - üìù [rank=0] loading glencairn_expressway.glencairn_expressway from raw samples with raw size 3 bytes
latent_webdataset.py - [rank {rank}] Loading encoder_hidden_states and attention_mask from .emb format
latent_webdataset.py - üìù [rank=0] loading goegap.goegap from raw samples with raw size 3 bytes
latent_webdataset.py - [rank {rank}] Loading encoder_hidden_states and attention_mask from .emb format
latent_webdataset.py - üìù [rank=0] loading goegap_road.goegap_road from raw samples with raw size 3 bytes
latent_webdataset.py - [rank {rank}] Loading encoder_hidden_states and attention_mask from .emb format
latent_webdataset.py - üìù [rank=0] loading golden_bay.golden_bay from raw samples with raw size 3 bytes
latent_webdataset.py - [rank {rank}] Loading encoder_hidden_states and attention_mask from .emb format
latent_webdataset.py - üìù [rank=0] loading golden_gate_hills.golden_gate_hills from raw samples with raw size 3 bytes
latent_webdataset.py - [rank {rank}] Loading encoder_hidden_states and attention_mask from .emb format
latent_webdataset.py - üìù [rank=0] loading golf_course_sunrise.golf_course_sunrise from raw samples with raw size 3 bytes
latent_webdataset.py - [rank {rank}] Loading encoder_hidden_states and attention_mask from .emb format
latent_webdataset.py - üìù [rank=0] loading gothic_manor_01.gothic_manor_01 from raw samples with raw size 3 bytes
latent_webdataset.py - [rank {rank}] Loading encoder_hidden_states and attention_mask from .emb format
latent_webdataset.py - üìù [rank=0] loading gothic_manor_02.gothic_manor_02 from raw samples with raw size 3 bytes
latent_webdataset.py - [rank {rank}] Loading encoder_hidden_states and attention_mask from .emb format
latent_webdataset.py - üìù [rank=0] loading graffiti_shelter.graffiti_shelter from raw samples with raw size 3 bytes
latent_webdataset.py - [rank {rank}] Loading encoder_hidden_states and attention_mask from .emb format
latent_webdataset.py - üìù [rank=0] loading graveyard_pathways.graveyard_pathways from raw samples with raw size 3 bytes
latent_webdataset.py - [rank {rank}] Loading encoder_hidden_states and attention_mask from .emb format
latent_webdataset.py - üìù [rank=0] loading gray_pier.gray_pier from raw samples with raw size 3 bytes
latent_webdataset.py - [rank {rank}] Loading encoder_hidden_states and attention_mask from .emb format
latent_webdataset.py - üìù [rank=0] loading green_point_park.green_point_park from raw samples with raw size 3 bytes
latent_webdataset.py - [rank {rank}] Loading encoder_hidden_states and attention_mask from .emb format
latent_webdataset.py - üìù [rank=0] loading green_sanctuary.green_sanctuary from raw samples with raw size 3 bytes
latent_webdataset.py - [rank {rank}] Loading encoder_hidden_states and attention_mask from .emb format
latent_webdataset.py - üìù [rank=0] loading greenwich_park.greenwich_park from raw samples with raw size 3 bytes
latent_webdataset.py - [rank {rank}] Loading encoder_hidden_states and attention_mask from .emb format
latent_webdataset.py - üìù [rank=0] loading greenwich_park_02.greenwich_park_02 from raw samples with raw size 3 bytes
latent_webdataset.py - [rank {rank}] Loading encoder_hidden_states and attention_mask from .emb format
latent_webdataset.py - üìù [rank=0] loading gum_trees.gum_trees from raw samples with raw size 3 bytes
latent_webdataset.py - [rank {rank}] Loading encoder_hidden_states and attention_mask from .emb format
latent_webdataset.py - üìù [rank=0] loading gym_01.gym_01 from raw samples with raw size 3 bytes
latent_webdataset.py - [rank {rank}] Loading encoder_hidden_states and attention_mask from .emb format
latent_webdataset.py - üìù [rank=0] loading gym_entrance.gym_entrance from raw samples with raw size 3 bytes
latent_webdataset.py - [rank {rank}] Loading encoder_hidden_states and attention_mask from .emb format
latent_webdataset.py - üìù [rank=0] loading hall_of_finfish.hall_of_finfish from raw samples with raw size 3 bytes
latent_webdataset.py - [rank {rank}] Loading encoder_hidden_states and attention_mask from .emb format
latent_webdataset.py - üìù [rank=0] loading hall_of_mammals.hall_of_mammals from raw samples with raw size 3 bytes
latent_webdataset.py - [rank {rank}] Loading encoder_hidden_states and attention_mask from .emb format
latent_webdataset.py - üìù [rank=0] loading hamburg_canal.hamburg_canal from raw samples with raw size 3 bytes
latent_webdataset.py - [rank {rank}] Loading encoder_hidden_states and attention_mask from .emb format
latent_webdataset.py - üìù [rank=0] loading hamburg_hbf.hamburg_hbf from raw samples with raw size 3 bytes
latent_webdataset.py - [rank {rank}] Loading encoder_hidden_states and attention_mask from .emb format
latent_webdataset.py - üìù [rank=0] loading hangar_interior.hangar_interior from raw samples with raw size 3 bytes
latent_webdataset.py - [rank {rank}] Loading encoder_hidden_states and attention_mask from .emb format
latent_webdataset.py - üìù [rank=0] loading hanger_exterior_cloudy.hanger_exterior_cloudy from raw samples with raw size 3 bytes
latent_webdataset.py - [rank {rank}] Loading encoder_hidden_states and attention_mask from .emb format
latent_webdataset.py - üìù [rank=0] loading hansaplatz.hansaplatz from raw samples with raw size 3 bytes
latent_webdataset.py - [rank {rank}] Loading encoder_hidden_states and attention_mask from .emb format
latent_webdataset.py - üìù [rank=0] loading harties_cliff_view.harties_cliff_view from raw samples with raw size 3 bytes
latent_webdataset.py - [rank {rank}] Loading encoder_hidden_states and attention_mask from .emb format
latent_webdataset.py - üìù [rank=0] loading harvest.harvest from raw samples with raw size 3 bytes
latent_webdataset.py - [rank {rank}] Loading encoder_hidden_states and attention_mask from .emb format
latent_webdataset.py - üìù [rank=0] loading hayloft.hayloft from raw samples with raw size 3 bytes
latent_webdataset.py - [rank {rank}] Loading encoder_hidden_states and attention_mask from .emb format
latent_webdataset.py - üìù [rank=0] loading herkulessaulen.herkulessaulen from raw samples with raw size 3 bytes
latent_webdataset.py - [rank {rank}] Loading encoder_hidden_states and attention_mask from .emb format
latent_webdataset.py - üìù [rank=0] loading hikers_cave.hikers_cave from raw samples with raw size 3 bytes
latent_webdataset.py - [rank {rank}] Loading encoder_hidden_states and attention_mask from .emb format
latent_webdataset.py - üìù [rank=0] loading hilltop_construction.hilltop_construction from raw samples with raw size 3 bytes
latent_webdataset.py - [rank {rank}] Loading encoder_hidden_states and attention_mask from .emb format
latent_webdataset.py - üìù [rank=0] loading hilly_terrain_01.hilly_terrain_01 from raw samples with raw size 3 bytes
latent_webdataset.py - [rank {rank}] Loading encoder_hidden_states and attention_mask from .emb format
latent_webdataset.py - üìù [rank=0] loading hilly_terrain_01_puresky.hilly_terrain_01_puresky from raw samples with raw size 3 bytes
latent_webdataset.py - [rank {rank}] Loading encoder_hidden_states and attention_mask from .emb format
latent_webdataset.py - üìù [rank=0] loading hochsal_field.hochsal_field from raw samples with raw size 3 bytes
latent_webdataset.py - [rank {rank}] Loading encoder_hidden_states and attention_mask from .emb format
latent_webdataset.py - üìù [rank=0] loading hochsal_forest.hochsal_forest from raw samples with raw size 3 bytes
latent_webdataset.py - [rank {rank}] Loading encoder_hidden_states and attention_mask from .emb format
latent_webdataset.py - üìù [rank=0] loading horn-koppe_snow.horn-koppe_snow from raw samples with raw size 3 bytes
latent_webdataset.py - [rank {rank}] Loading encoder_hidden_states and attention_mask from .emb format
latent_webdataset.py - üìù [rank=0] loading hospital_room.hospital_room from raw samples with raw size 3 bytes
latent_webdataset.py - [rank {rank}] Loading encoder_hidden_states and attention_mask from .emb format
latent_webdataset.py - üìù [rank=0] loading hospital_room_2.hospital_room_2 from raw samples with raw size 3 bytes
latent_webdataset.py - [rank {rank}] Loading encoder_hidden_states and attention_mask from .emb format
latent_webdataset.py - üìù [rank=0] loading hotel_room.hotel_room from raw samples with raw size 3 bytes
latent_webdataset.py - [rank {rank}] Loading encoder_hidden_states and attention_mask from .emb format
latent_webdataset.py - üìù [rank=0] loading illovo_beach_balcony.illovo_beach_balcony from raw samples with raw size 3 bytes
latent_webdataset.py - [rank {rank}] Loading encoder_hidden_states and attention_mask from .emb format
latent_webdataset.py - üìù [rank=0] loading immenstadter_horn.immenstadter_horn from raw samples with raw size 3 bytes
latent_webdataset.py - [rank {rank}] Loading encoder_hidden_states and attention_mask from .emb format
latent_webdataset.py - üìù [rank=0] loading industrial_pipe_and_valve_01.industrial_pipe_and_valve_01 from raw samples with raw size 3 bytes
latent_webdataset.py - [rank {rank}] Loading encoder_hidden_states and attention_mask from .emb format
latent_webdataset.py - üìù [rank=0] loading industrial_pipe_and_valve_02.industrial_pipe_and_valve_02 from raw samples with raw size 3 bytes
latent_webdataset.py - [rank {rank}] Loading encoder_hidden_states and attention_mask from .emb format
latent_webdataset.py - üìù [rank=0] loading industrial_sunset.industrial_sunset from raw samples with raw size 3 bytes
latent_webdataset.py - [rank {rank}] Loading encoder_hidden_states and attention_mask from .emb format
latent_webdataset.py - üìù [rank=0] loading industrial_sunset_02.industrial_sunset_02 from raw samples with raw size 3 bytes
latent_webdataset.py - [rank {rank}] Loading encoder_hidden_states and attention_mask from .emb format
latent_webdataset.py - üìù [rank=0] loading industrial_sunset_02_puresky.industrial_sunset_02_puresky from raw samples with raw size 3 bytes
latent_webdataset.py - [rank {rank}] Loading encoder_hidden_states and attention_mask from .emb format
latent_webdataset.py - üìù [rank=0] loading industrial_sunset_puresky.industrial_sunset_puresky from raw samples with raw size 3 bytes
latent_webdataset.py - [rank {rank}] Loading encoder_hidden_states and attention_mask from .emb format
latent_webdataset.py - üìù [rank=0] loading industrial_workshop_foundry.industrial_workshop_foundry from raw samples with raw size 3 bytes
latent_webdataset.py - [rank {rank}] Loading encoder_hidden_states and attention_mask from .emb format
latent_webdataset.py - üìù [rank=0] loading interior_construction.interior_construction from raw samples with raw size 3 bytes
latent_webdataset.py - [rank {rank}] Loading encoder_hidden_states and attention_mask from .emb format
latent_webdataset.py - üìù [rank=0] loading irish_institute.irish_institute from raw samples with raw size 3 bytes
latent_webdataset.py - [rank {rank}] Loading encoder_hidden_states and attention_mask from .emb format
latent_webdataset.py - üìù [rank=0] loading je_gray_02.je_gray_02 from raw samples with raw size 3 bytes
latent_webdataset.py - [rank {rank}] Loading encoder_hidden_states and attention_mask from .emb format
latent_webdataset.py - üìù [rank=0] loading je_gray_park.je_gray_park from raw samples with raw size 3 bytes
latent_webdataset.py - [rank {rank}] Loading encoder_hidden_states and attention_mask from .emb format
latent_webdataset.py - üìù [rank=0] loading kart_club.kart_club from raw samples with raw size 3 bytes
latent_webdataset.py - [rank {rank}] Loading encoder_hidden_states and attention_mask from .emb format
latent_webdataset.py - üìù [rank=0] loading kiara_1_dawn.kiara_1_dawn from raw samples with raw size 3 bytes
latent_webdataset.py - [rank {rank}] Loading encoder_hidden_states and attention_mask from .emb format
latent_webdataset.py - üìù [rank=0] loading kiara_2_sunrise.kiara_2_sunrise from raw samples with raw size 3 bytes
latent_webdataset.py - [rank {rank}] Loading encoder_hidden_states and attention_mask from .emb format
latent_webdataset.py - üìù [rank=0] loading kiara_3_morning.kiara_3_morning from raw samples with raw size 3 bytes
latent_webdataset.py - [rank {rank}] Loading encoder_hidden_states and attention_mask from .emb format
latent_webdataset.py - üìù [rank=0] loading kiara_4_mid-morning.kiara_4_mid-morning from raw samples with raw size 3 bytes
latent_webdataset.py - [rank {rank}] Loading encoder_hidden_states and attention_mask from .emb format
latent_webdataset.py - üìù [rank=0] loading kiara_5_noon.kiara_5_noon from raw samples with raw size 3 bytes
latent_webdataset.py - [rank {rank}] Loading encoder_hidden_states and attention_mask from .emb format
latent_webdataset.py - üìù [rank=0] loading kiara_6_afternoon.kiara_6_afternoon from raw samples with raw size 3 bytes
latent_webdataset.py - [rank {rank}] Loading encoder_hidden_states and attention_mask from .emb format
latent_webdataset.py - üìù [rank=0] loading kiara_7_late-afternoon.kiara_7_late-afternoon from raw samples with raw size 3 bytes
latent_webdataset.py - [rank {rank}] Loading encoder_hidden_states and attention_mask from .emb format
latent_webdataset.py - üìù [rank=0] loading kiara_8_sunset.kiara_8_sunset from raw samples with raw size 3 bytes
latent_webdataset.py - [rank {rank}] Loading encoder_hidden_states and attention_mask from .emb format
latent_webdataset.py - üìù [rank=0] loading kiara_9_dusk.kiara_9_dusk from raw samples with raw size 3 bytes
latent_webdataset.py - [rank {rank}] Loading encoder_hidden_states and attention_mask from .emb format
latent_webdataset.py - üìù [rank=0] loading kiara_interior.kiara_interior from raw samples with raw size 3 bytes
latent_webdataset.py - [rank {rank}] Loading encoder_hidden_states and attention_mask from .emb format
latent_webdataset.py - üìù [rank=0] loading killesberg_park.killesberg_park from raw samples with raw size 3 bytes
latent_webdataset.py - [rank {rank}] Loading encoder_hidden_states and attention_mask from .emb format
latent_webdataset.py - üìù [rank=0] loading klippad_dawn_1.klippad_dawn_1 from raw samples with raw size 3 bytes
latent_webdataset.py - [rank {rank}] Loading encoder_hidden_states and attention_mask from .emb format
latent_webdataset.py - üìù [rank=0] loading klippad_dawn_2.klippad_dawn_2 from raw samples with raw size 3 bytes
latent_webdataset.py - [rank {rank}] Loading encoder_hidden_states and attention_mask from .emb format
latent_webdataset.py - üìù [rank=0] loading klippad_sunrise_1.klippad_sunrise_1 from raw samples with raw size 3 bytes
latent_webdataset.py - [rank {rank}] Loading encoder_hidden_states and attention_mask from .emb format
latent_webdataset.py - üìù [rank=0] loading klippad_sunrise_2.klippad_sunrise_2 from raw samples with raw size 3 bytes
latent_webdataset.py - [rank {rank}] Loading encoder_hidden_states and attention_mask from .emb format
latent_webdataset.py - üìù [rank=0] loading kloetzle_blei.kloetzle_blei from raw samples with raw size 3 bytes
latent_webdataset.py - [rank {rank}] Loading encoder_hidden_states and attention_mask from .emb format
latent_webdataset.py - üìù [rank=0] loading kloofendal_28d_misty.kloofendal_28d_misty from raw samples with raw size 3 bytes
latent_webdataset.py - [rank {rank}] Loading encoder_hidden_states and attention_mask from .emb format
latent_webdataset.py - üìù [rank=0] loading kloofendal_28d_misty_puresky.kloofendal_28d_misty_puresky from raw samples with raw size 3 bytes
latent_webdataset.py - [rank {rank}] Loading encoder_hidden_states and attention_mask from .emb format
latent_webdataset.py - üìù [rank=0] loading kloofendal_38d_partly_cloudy.kloofendal_38d_partly_cloudy from raw samples with raw size 3 bytes
latent_webdataset.py - [rank {rank}] Loading encoder_hidden_states and attention_mask from .emb format
latent_webdataset.py - üìù [rank=0] loading kloofendal_38d_partly_cloudy_puresky.kloofendal_38d_partly_cloudy_puresky from raw samples with raw size 3 bytes
latent_webdataset.py - [rank {rank}] Loading encoder_hidden_states and attention_mask from .emb format
latent_webdataset.py - üìù [rank=0] loading kloofendal_43d_clear.kloofendal_43d_clear from raw samples with raw size 3 bytes
latent_webdataset.py - [rank {rank}] Loading encoder_hidden_states and attention_mask from .emb format
latent_webdataset.py - üìù [rank=0] loading kloofendal_43d_clear_puresky.kloofendal_43d_clear_puresky from raw samples with raw size 3 bytes
latent_webdataset.py - [rank {rank}] Loading encoder_hidden_states and attention_mask from .emb format
latent_webdataset.py - üìù [rank=0] loading kloofendal_48d_partly_cloudy.kloofendal_48d_partly_cloudy from raw samples with raw size 3 bytes
latent_webdataset.py - [rank {rank}] Loading encoder_hidden_states and attention_mask from .emb format
latent_webdataset.py - üìù [rank=0] loading kloofendal_48d_partly_cloudy_puresky.kloofendal_48d_partly_cloudy_puresky from raw samples with raw size 3 bytes
latent_webdataset.py - [rank {rank}] Loading encoder_hidden_states and attention_mask from .emb format
latent_webdataset.py - üìù [rank=0] loading kloofendal_misty_morning.kloofendal_misty_morning from raw samples with raw size 3 bytes
latent_webdataset.py - [rank {rank}] Loading encoder_hidden_states and attention_mask from .emb format
latent_webdataset.py - üìù [rank=0] loading kloofendal_misty_morning_puresky.kloofendal_misty_morning_puresky from raw samples with raw size 3 bytes
latent_webdataset.py - [rank {rank}] Loading encoder_hidden_states and attention_mask from .emb format
latent_webdataset.py - üìù [rank=0] loading kloofendal_overcast.kloofendal_overcast from raw samples with raw size 3 bytes
latent_webdataset.py - [rank {rank}] Loading encoder_hidden_states and attention_mask from .emb format
latent_webdataset.py - üìù [rank=0] loading kloofendal_overcast_puresky.kloofendal_overcast_puresky from raw samples with raw size 3 bytes
latent_webdataset.py - [rank {rank}] Loading encoder_hidden_states and attention_mask from .emb format
latent_webdataset.py - üìù [rank=0] loading kloppenheim_01.kloppenheim_01 from raw samples with raw size 3 bytes
latent_webdataset.py - [rank {rank}] Loading encoder_hidden_states and attention_mask from .emb format
latent_webdataset.py - üìù [rank=0] loading kloppenheim_01_puresky.kloppenheim_01_puresky from raw samples with raw size 3 bytes
latent_webdataset.py - [rank {rank}] Loading encoder_hidden_states and attention_mask from .emb format
latent_webdataset.py - üìù [rank=0] loading kloppenheim_02.kloppenheim_02 from raw samples with raw size 3 bytes
latent_webdataset.py - [rank {rank}] Loading encoder_hidden_states and attention_mask from .emb format
latent_webdataset.py - üìù [rank=0] loading kloppenheim_03.kloppenheim_03 from raw samples with raw size 3 bytes
latent_webdataset.py - [rank {rank}] Loading encoder_hidden_states and attention_mask from .emb format
latent_webdataset.py - üìù [rank=0] loading kloppenheim_03_puresky.kloppenheim_03_puresky from raw samples with raw size 3 bytes
latent_webdataset.py - [rank {rank}] Loading encoder_hidden_states and attention_mask from .emb format
latent_webdataset.py - üìù [rank=0] loading kloppenheim_04.kloppenheim_04 from raw samples with raw size 3 bytes
latent_webdataset.py - [rank {rank}] Loading encoder_hidden_states and attention_mask from .emb format
latent_webdataset.py - üìù [rank=0] loading kloppenheim_05_puresky.kloppenheim_05_puresky from raw samples with raw size 3 bytes
latent_webdataset.py - [rank {rank}] Loading encoder_hidden_states and attention_mask from .emb format
latent_webdataset.py - üìù [rank=0] loading kloppenheim_06.kloppenheim_06 from raw samples with raw size 3 bytes
latent_webdataset.py - [rank {rank}] Loading encoder_hidden_states and attention_mask from .emb format
latent_webdataset.py - üìù [rank=0] loading kloppenheim_06_puresky.kloppenheim_06_puresky from raw samples with raw size 3 bytes
latent_webdataset.py - [rank {rank}] Loading encoder_hidden_states and attention_mask from .emb format
latent_webdataset.py - üìù [rank=0] loading kloppenheim_07_puresky.kloppenheim_07_puresky from raw samples with raw size 3 bytes
latent_webdataset.py - [rank {rank}] Loading encoder_hidden_states and attention_mask from .emb format
latent_webdataset.py - üìù [rank=0] loading konigsallee.konigsallee from raw samples with raw size 3 bytes
latent_webdataset.py - [rank {rank}] Loading encoder_hidden_states and attention_mask from .emb format
latent_webdataset.py - üìù [rank=0] loading konzerthaus.konzerthaus from raw samples with raw size 3 bytes
latent_webdataset.py - [rank {rank}] Loading encoder_hidden_states and attention_mask from .emb format
latent_webdataset.py - üìù [rank=0] loading lago_disola.lago_disola from raw samples with raw size 3 bytes
latent_webdataset.py - [rank {rank}] Loading encoder_hidden_states and attention_mask from .emb format
latent_webdataset.py - üìù [rank=0] loading lake_pier.lake_pier from raw samples with raw size 3 bytes
latent_webdataset.py - [rank {rank}] Loading encoder_hidden_states and attention_mask from .emb format
latent_webdataset.py - üìù [rank=0] loading lakes.lakes from raw samples with raw size 3 bytes
latent_webdataset.py - [rank {rank}] Loading encoder_hidden_states and attention_mask from .emb format
latent_webdataset.py - üìù [rank=0] loading lakeside.lakeside from raw samples with raw size 3 bytes
latent_webdataset.py - [rank {rank}] Loading encoder_hidden_states and attention_mask from .emb format
latent_webdataset.py - üìù [rank=0] loading lakeside_dawn.lakeside_dawn from raw samples with raw size 3 bytes
latent_webdataset.py - [rank {rank}] Loading encoder_hidden_states and attention_mask from .emb format
latent_webdataset.py - üìù [rank=0] loading lakeside_night.lakeside_night from raw samples with raw size 3 bytes
latent_webdataset.py - [rank {rank}] Loading encoder_hidden_states and attention_mask from .emb format
latent_webdataset.py - üìù [rank=0] loading lakeside_sunrise.lakeside_sunrise from raw samples with raw size 3 bytes
latent_webdataset.py - [rank {rank}] Loading encoder_hidden_states and attention_mask from .emb format
latent_webdataset.py - üìù [rank=0] loading lapa.lapa from raw samples with raw size 3 bytes
latent_webdataset.py - [rank {rank}] Loading encoder_hidden_states and attention_mask from .emb format
latent_webdataset.py - üìù [rank=0] loading large_corridor.large_corridor from raw samples with raw size 3 bytes
latent_webdataset.py - [rank {rank}] Loading encoder_hidden_states and attention_mask from .emb format
latent_webdataset.py - üìù [rank=0] loading laufenurg_church.laufenurg_church from raw samples with raw size 3 bytes
latent_webdataset.py - [rank {rank}] Loading encoder_hidden_states and attention_mask from .emb format
latent_webdataset.py - üìù [rank=0] loading lauter_waterfall.lauter_waterfall from raw samples with raw size 3 bytes
latent_webdataset.py - [rank {rank}] Loading encoder_hidden_states and attention_mask from .emb format
latent_webdataset.py - üìù [rank=0] loading learner_park.learner_park from raw samples with raw size 3 bytes
latent_webdataset.py - [rank {rank}] Loading encoder_hidden_states and attention_mask from .emb format
latent_webdataset.py - üìù [rank=0] loading lebombo.lebombo from raw samples with raw size 3 bytes
latent_webdataset.py - [rank {rank}] Loading encoder_hidden_states and attention_mask from .emb format
latent_webdataset.py - üìù [rank=0] loading lenong_1.lenong_1 from raw samples with raw size 3 bytes
latent_webdataset.py - [rank {rank}] Loading encoder_hidden_states and attention_mask from .emb format
latent_webdataset.py - üìù [rank=0] loading lenong_2.lenong_2 from raw samples with raw size 3 bytes
latent_webdataset.py - [rank {rank}] Loading encoder_hidden_states and attention_mask from .emb format
latent_webdataset.py - üìù [rank=0] loading lilienstein.lilienstein from raw samples with raw size 3 bytes
latent_webdataset.py - [rank {rank}] Loading encoder_hidden_states and attention_mask from .emb format
latent_webdataset.py - üìù [rank=0] loading limehouse.limehouse from raw samples with raw size 3 bytes
latent_webdataset.py - [rank {rank}] Loading encoder_hidden_states and attention_mask from .emb format
latent_webdataset.py - üìù [rank=0] loading limpopo_golf_course.limpopo_golf_course from raw samples with raw size 3 bytes
latent_webdataset.py - [rank {rank}] Loading encoder_hidden_states and attention_mask from .emb format
latent_webdataset.py - üìù [rank=0] loading little_paris_eiffel_tower.little_paris_eiffel_tower from raw samples with raw size 3 bytes
latent_webdataset.py - [rank {rank}] Loading encoder_hidden_states and attention_mask from .emb format
latent_webdataset.py - üìù [rank=0] loading little_paris_under_tower.little_paris_under_tower from raw samples with raw size 3 bytes
latent_webdataset.py - [rank {rank}] Loading encoder_hidden_states and attention_mask from .emb format
latent_webdataset.py - üìù [rank=0] loading lonely_road_afternoon.lonely_road_afternoon from raw samples with raw size 3 bytes
latent_webdataset.py - [rank {rank}] Loading encoder_hidden_states and attention_mask from .emb format
latent_webdataset.py - üìù [rank=0] loading lonely_road_afternoon_puresky.lonely_road_afternoon_puresky from raw samples with raw size 3 bytes
latent_webdataset.py - [rank {rank}] Loading encoder_hidden_states and attention_mask from .emb format
latent_webdataset.py - üìù [rank=0] loading lookout.lookout from raw samples with raw size 3 bytes
latent_webdataset.py - [rank {rank}] Loading encoder_hidden_states and attention_mask from .emb format
latent_webdataset.py - üìù [rank=0] loading lot_01.lot_01 from raw samples with raw size 3 bytes
latent_webdataset.py - [rank {rank}] Loading encoder_hidden_states and attention_mask from .emb format
latent_webdataset.py - üìù [rank=0] loading lot_02.lot_02 from raw samples with raw size 3 bytes
latent_webdataset.py - [rank {rank}] Loading encoder_hidden_states and attention_mask from .emb format
latent_webdataset.py - üìù [rank=0] loading lush_dirt_path.lush_dirt_path from raw samples with raw size 3 bytes
latent_webdataset.py - [rank {rank}] Loading encoder_hidden_states and attention_mask from .emb format
latent_webdataset.py - üìù [rank=0] loading lythwood_field.lythwood_field from raw samples with raw size 3 bytes
latent_webdataset.py - [rank {rank}] Loading encoder_hidden_states and attention_mask from .emb format
latent_webdataset.py - üìù [rank=0] loading lythwood_lounge.lythwood_lounge from raw samples with raw size 3 bytes
latent_webdataset.py - [rank {rank}] Loading encoder_hidden_states and attention_mask from .emb format
latent_webdataset.py - üìù [rank=0] loading lythwood_terrace.lythwood_terrace from raw samples with raw size 3 bytes
latent_webdataset.py - [rank {rank}] Loading encoder_hidden_states and attention_mask from .emb format
latent_webdataset.py - üìù [rank=0] loading machine_shop_02.machine_shop_02 from raw samples with raw size 3 bytes
latent_webdataset.py - [rank {rank}] Loading encoder_hidden_states and attention_mask from .emb format
latent_webdataset.py - üìù [rank=0] loading machine_shop_03.machine_shop_03 from raw samples with raw size 3 bytes
latent_webdataset.py - [rank {rank}] Loading encoder_hidden_states and attention_mask from .emb format
latent_webdataset.py - üìù [rank=0] loading mall_parking_lot.mall_parking_lot from raw samples with raw size 3 bytes
latent_webdataset.py - [rank {rank}] Loading encoder_hidden_states and attention_mask from .emb format
latent_webdataset.py - üìù [rank=0] loading marry_hall.marry_hall from raw samples with raw size 3 bytes
latent_webdataset.py - [rank {rank}] Loading encoder_hidden_states and attention_mask from .emb format
latent_webdataset.py - üìù [rank=0] loading meadow.meadow from raw samples with raw size 3 bytes
latent_webdataset.py - [rank {rank}] Loading encoder_hidden_states and attention_mask from .emb format
latent_webdataset.py - üìù [rank=0] loading meadow_2.meadow_2 from raw samples with raw size 3 bytes
latent_webdataset.py - [rank {rank}] Loading encoder_hidden_states and attention_mask from .emb format
latent_webdataset.py - üìù [rank=0] loading mealie_road.mealie_road from raw samples with raw size 3 bytes
latent_webdataset.py - [rank {rank}] Loading encoder_hidden_states and attention_mask from .emb format
latent_webdataset.py - üìù [rank=0] loading medieval_cafe.medieval_cafe from raw samples with raw size 3 bytes
latent_webdataset.py - [rank {rank}] Loading encoder_hidden_states and attention_mask from .emb format
latent_webdataset.py - üìù [rank=0] loading metro_noord.metro_noord from raw samples with raw size 3 bytes
latent_webdataset.py - [rank {rank}] Loading encoder_hidden_states and attention_mask from .emb format
latent_webdataset.py - üìù [rank=0] loading metro_vijzelgracht.metro_vijzelgracht from raw samples with raw size 3 bytes
latent_webdataset.py - [rank {rank}] Loading encoder_hidden_states and attention_mask from .emb format
latent_webdataset.py - üìù [rank=0] loading minedump_flats.minedump_flats from raw samples with raw size 3 bytes
latent_webdataset.py - [rank {rank}] Loading encoder_hidden_states and attention_mask from .emb format
latent_webdataset.py - üìù [rank=0] loading mirrored_hall.mirrored_hall from raw samples with raw size 3 bytes
latent_webdataset.py - [rank {rank}] Loading encoder_hidden_states and attention_mask from .emb format
latent_webdataset.py - üìù [rank=0] loading missile_launch_facility_01.missile_launch_facility_01 from raw samples with raw size 3 bytes
latent_webdataset.py - [rank {rank}] Loading encoder_hidden_states and attention_mask from .emb format
latent_webdataset.py - üìù [rank=0] loading missile_launch_facility_02.missile_launch_facility_02 from raw samples with raw size 3 bytes
latent_webdataset.py - [rank {rank}] Loading encoder_hidden_states and attention_mask from .emb format
latent_webdataset.py - üìù [rank=0] loading missile_launch_facility_03.missile_launch_facility_03 from raw samples with raw size 3 bytes
latent_webdataset.py - [rank {rank}] Loading encoder_hidden_states and attention_mask from .emb format
latent_webdataset.py - üìù [rank=0] loading misty_dawn.misty_dawn from raw samples with raw size 3 bytes
latent_webdataset.py - [rank {rank}] Loading encoder_hidden_states and attention_mask from .emb format
latent_webdataset.py - üìù [rank=0] loading misty_farm_road.misty_farm_road from raw samples with raw size 3 bytes
latent_webdataset.py - [rank {rank}] Loading encoder_hidden_states and attention_mask from .emb format
latent_webdataset.py - üìù [rank=0] loading misty_pines.misty_pines from raw samples with raw size 3 bytes
latent_webdataset.py - [rank {rank}] Loading encoder_hidden_states and attention_mask from .emb format
latent_webdataset.py - üìù [rank=0] loading modern_buildings.modern_buildings from raw samples with raw size 3 bytes
latent_webdataset.py - [rank {rank}] Loading encoder_hidden_states and attention_mask from .emb format
latent_webdataset.py - üìù [rank=0] loading modern_bathroom.modern_bathroom from raw samples with raw size 3 bytes
latent_webdataset.py - [rank {rank}] Loading encoder_hidden_states and attention_mask from .emb format
latent_webdataset.py - üìù [rank=0] loading modern_buildings_2.modern_buildings_2 from raw samples with raw size 3 bytes
latent_webdataset.py - [rank {rank}] Loading encoder_hidden_states and attention_mask from .emb format
latent_webdataset.py - üìù [rank=0] loading modern_buildings_night.modern_buildings_night from raw samples with raw size 3 bytes
latent_webdataset.py - [rank {rank}] Loading encoder_hidden_states and attention_mask from .emb format
latent_webdataset.py - üìù [rank=0] loading monbachtal_riverbank.monbachtal_riverbank from raw samples with raw size 3 bytes
latent_webdataset.py - [rank {rank}] Loading encoder_hidden_states and attention_mask from .emb format
latent_webdataset.py - üìù [rank=0] loading monks_forest.monks_forest from raw samples with raw size 3 bytes
latent_webdataset.py - [rank {rank}] Loading encoder_hidden_states and attention_mask from .emb format
latent_webdataset.py - üìù [rank=0] loading monkstown_castle.monkstown_castle from raw samples with raw size 3 bytes
latent_webdataset.py - [rank {rank}] Loading encoder_hidden_states and attention_mask from .emb format
latent_webdataset.py - üìù [rank=0] loading monte_scherbelino.monte_scherbelino from raw samples with raw size 3 bytes
latent_webdataset.py - [rank {rank}] Loading encoder_hidden_states and attention_mask from .emb format
latent_webdataset.py - üìù [rank=0] loading montorfano.montorfano from raw samples with raw size 3 bytes
latent_webdataset.py - [rank {rank}] Loading encoder_hidden_states and attention_mask from .emb format
latent_webdataset.py - üìù [rank=0] loading moonless_golf.moonless_golf from raw samples with raw size 3 bytes
latent_webdataset.py - [rank {rank}] Loading encoder_hidden_states and attention_mask from .emb format
latent_webdataset.py - üìù [rank=0] loading moonlit_golf.moonlit_golf from raw samples with raw size 3 bytes
latent_webdataset.py - [rank {rank}] Loading encoder_hidden_states and attention_mask from .emb format
latent_webdataset.py - üìù [rank=0] loading mosaic_tunnel.mosaic_tunnel from raw samples with raw size 3 bytes
latent_webdataset.py - [rank {rank}] Loading encoder_hidden_states and attention_mask from .emb format
latent_webdataset.py - üìù [rank=0] loading mossy_forest.mossy_forest from raw samples with raw size 3 bytes
latent_webdataset.py - [rank {rank}] Loading encoder_hidden_states and attention_mask from .emb format
latent_webdataset.py - üìù [rank=0] loading moulton_falls_train_tunnel_east.moulton_falls_train_tunnel_east from raw samples with raw size 3 bytes
latent_webdataset.py - [rank {rank}] Loading encoder_hidden_states and attention_mask from .emb format
latent_webdataset.py - üìù [rank=0] loading moulton_station_train_tunnel_west.moulton_station_train_tunnel_west from raw samples with raw size 3 bytes
latent_webdataset.py - [rank {rank}] Loading encoder_hidden_states and attention_mask from .emb format
latent_webdataset.py - üìù [rank=0] loading mpumalanga_veld.mpumalanga_veld from raw samples with raw size 3 bytes
latent_webdataset.py - [rank {rank}] Loading encoder_hidden_states and attention_mask from .emb format
latent_webdataset.py - üìù [rank=0] loading mpumalanga_veld_puresky.mpumalanga_veld_puresky from raw samples with raw size 3 bytes
latent_webdataset.py - [rank {rank}] Loading encoder_hidden_states and attention_mask from .emb format
latent_webdataset.py - üìù [rank=0] loading mud_road.mud_road from raw samples with raw size 3 bytes
latent_webdataset.py - [rank {rank}] Loading encoder_hidden_states and attention_mask from .emb format
latent_webdataset.py - üìù [rank=0] loading mud_road_puresky.mud_road_puresky from raw samples with raw size 3 bytes
latent_webdataset.py - [rank {rank}] Loading encoder_hidden_states and attention_mask from .emb format
latent_webdataset.py - üìù [rank=0] loading muddy_autumn_forest.muddy_autumn_forest from raw samples with raw size 3 bytes
latent_webdataset.py - [rank {rank}] Loading encoder_hidden_states and attention_mask from .emb format
latent_webdataset.py - üìù [rank=0] loading museum_of_ethnography.museum_of_ethnography from raw samples with raw size 3 bytes
latent_webdataset.py - [rank {rank}] Loading encoder_hidden_states and attention_mask from .emb format
latent_webdataset.py - üìù [rank=0] loading museumplein.museumplein from raw samples with raw size 3 bytes
latent_webdataset.py - [rank {rank}] Loading encoder_hidden_states and attention_mask from .emb format
latent_webdataset.py - üìù [rank=0] loading music_hall_01.music_hall_01 from raw samples with raw size 3 bytes
latent_webdataset.py - [rank {rank}] Loading encoder_hidden_states and attention_mask from .emb format
latent_webdataset.py - üìù [rank=0] loading mutianyu.mutianyu from raw samples with raw size 3 bytes
latent_webdataset.py - [rank {rank}] Loading encoder_hidden_states and attention_mask from .emb format
latent_webdataset.py - üìù [rank=0] loading nagoya_wall_path.nagoya_wall_path from raw samples with raw size 3 bytes
latent_webdataset.py - [rank {rank}] Loading encoder_hidden_states and attention_mask from .emb format
latent_webdataset.py - üìù [rank=0] loading narrow_moonlit_road.narrow_moonlit_road from raw samples with raw size 3 bytes
latent_webdataset.py - [rank {rank}] Loading encoder_hidden_states and attention_mask from .emb format
latent_webdataset.py - üìù [rank=0] loading nature_reserve_forest.nature_reserve_forest from raw samples with raw size 3 bytes
latent_webdataset.py - [rank {rank}] Loading encoder_hidden_states and attention_mask from .emb format
latent_webdataset.py - üìù [rank=0] loading near_the_river_01.near_the_river_01 from raw samples with raw size 3 bytes
latent_webdataset.py - [rank {rank}] Loading encoder_hidden_states and attention_mask from .emb format
latent_webdataset.py - üìù [rank=0] loading near_the_river_02.near_the_river_02 from raw samples with raw size 3 bytes
latent_webdataset.py - [rank {rank}] Loading encoder_hidden_states and attention_mask from .emb format
latent_webdataset.py - üìù [rank=0] loading neon_photostudio.neon_photostudio from raw samples with raw size 3 bytes
latent_webdataset.py - [rank {rank}] Loading encoder_hidden_states and attention_mask from .emb format
latent_webdataset.py - üìù [rank=0] loading netball_court.netball_court from raw samples with raw size 3 bytes
latent_webdataset.py - [rank {rank}] Loading encoder_hidden_states and attention_mask from .emb format
latent_webdataset.py - üìù [rank=0] loading neuer_zollhof.neuer_zollhof from raw samples with raw size 3 bytes
latent_webdataset.py - [rank {rank}] Loading encoder_hidden_states and attention_mask from .emb format
latent_webdataset.py - üìù [rank=0] loading neurathen_rock_castle.neurathen_rock_castle from raw samples with raw size 3 bytes
latent_webdataset.py - [rank {rank}] Loading encoder_hidden_states and attention_mask from .emb format
latent_webdataset.py - üìù [rank=0] loading niederwihl_forest.niederwihl_forest from raw samples with raw size 3 bytes
latent_webdataset.py - [rank {rank}] Loading encoder_hidden_states and attention_mask from .emb format
latent_webdataset.py - üìù [rank=0] loading night_bridge.night_bridge from raw samples with raw size 3 bytes
latent_webdataset.py - [rank {rank}] Loading encoder_hidden_states and attention_mask from .emb format
latent_webdataset.py - üìù [rank=0] loading ninomaru_teien.ninomaru_teien from raw samples with raw size 3 bytes
latent_webdataset.py - [rank {rank}] Loading encoder_hidden_states and attention_mask from .emb format
latent_webdataset.py - üìù [rank=0] loading nkuhlu.nkuhlu from raw samples with raw size 3 bytes
latent_webdataset.py - [rank {rank}] Loading encoder_hidden_states and attention_mask from .emb format
latent_webdataset.py - üìù [rank=0] loading noga.noga from raw samples with raw size 3 bytes
latent_webdataset.py - [rank {rank}] Loading encoder_hidden_states and attention_mask from .emb format
latent_webdataset.py - üìù [rank=0] loading noon_grass.noon_grass from raw samples with raw size 3 bytes
latent_webdataset.py - [rank {rank}] Loading encoder_hidden_states and attention_mask from .emb format
latent_webdataset.py - üìù [rank=0] loading northcliff.northcliff from raw samples with raw size 3 bytes
latent_webdataset.py - [rank {rank}] Loading encoder_hidden_states and attention_mask from .emb format
latent_webdataset.py - üìù [rank=0] loading oberer_kuhberg.oberer_kuhberg from raw samples with raw size 3 bytes
latent_webdataset.py - [rank {rank}] Loading encoder_hidden_states and attention_mask from .emb format
latent_webdataset.py - üìù [rank=0] loading old_apartments_walkway.old_apartments_walkway from raw samples with raw size 3 bytes
latent_webdataset.py - [rank {rank}] Loading encoder_hidden_states and attention_mask from .emb format
latent_webdataset.py - üìù [rank=0] loading old_bus_depot.old_bus_depot from raw samples with raw size 3 bytes
latent_webdataset.py - [rank {rank}] Loading encoder_hidden_states and attention_mask from .emb format
latent_webdataset.py - üìù [rank=0] loading old_depot.old_depot from raw samples with raw size 3 bytes
latent_webdataset.py - [rank {rank}] Loading encoder_hidden_states and attention_mask from .emb format
latent_webdataset.py - üìù [rank=0] loading old_hall.old_hall from raw samples with raw size 3 bytes
latent_webdataset.py - [rank {rank}] Loading encoder_hidden_states and attention_mask from .emb format
latent_webdataset.py - üìù [rank=0] loading old_outdoor_theater.old_outdoor_theater from raw samples with raw size 3 bytes
latent_webdataset.py - [rank {rank}] Loading encoder_hidden_states and attention_mask from .emb format
latent_webdataset.py - üìù [rank=0] loading old_quarry_gerlingen.old_quarry_gerlingen from raw samples with raw size 3 bytes
latent_webdataset.py - [rank {rank}] Loading encoder_hidden_states and attention_mask from .emb format
latent_webdataset.py - üìù [rank=0] loading old_room.old_room from raw samples with raw size 3 bytes
latent_webdataset.py - [rank {rank}] Loading encoder_hidden_states and attention_mask from .emb format
latent_webdataset.py - üìù [rank=0] loading old_tree_in_city_park.old_tree_in_city_park from raw samples with raw size 3 bytes
latent_webdataset.py - [rank {rank}] Loading encoder_hidden_states and attention_mask from .emb format
latent_webdataset.py - üìù [rank=0] loading orlando_stadium.orlando_stadium from raw samples with raw size 3 bytes
latent_webdataset.py - [rank {rank}] Loading encoder_hidden_states and attention_mask from .emb format
latent_webdataset.py - üìù [rank=0] loading ostrich_road.ostrich_road from raw samples with raw size 3 bytes
latent_webdataset.py - [rank {rank}] Loading encoder_hidden_states and attention_mask from .emb format
latent_webdataset.py - üìù [rank=0] loading ouchy_pier.ouchy_pier from raw samples with raw size 3 bytes
latent_webdataset.py - [rank {rank}] Loading encoder_hidden_states and attention_mask from .emb format
latent_webdataset.py - üìù [rank=0] loading outdoor_umbrellas.outdoor_umbrellas from raw samples with raw size 3 bytes
latent_webdataset.py - [rank {rank}] Loading encoder_hidden_states and attention_mask from .emb format
latent_webdataset.py - üìù [rank=0] loading overcast_soil.overcast_soil from raw samples with raw size 3 bytes
latent_webdataset.py - [rank {rank}] Loading encoder_hidden_states and attention_mask from .emb format
latent_webdataset.py - üìù [rank=0] loading overcast_soil_2.overcast_soil_2 from raw samples with raw size 3 bytes
latent_webdataset.py - [rank {rank}] Loading encoder_hidden_states and attention_mask from .emb format
latent_webdataset.py - üìù [rank=0] loading overcast_soil_puresky.overcast_soil_puresky from raw samples with raw size 3 bytes
latent_webdataset.py - [rank {rank}] Loading encoder_hidden_states and attention_mask from .emb format
latent_webdataset.py - üìù [rank=0] loading ox_bridge_morning.ox_bridge_morning from raw samples with raw size 3 bytes
latent_webdataset.py - [rank {rank}] Loading encoder_hidden_states and attention_mask from .emb format
latent_webdataset.py - üìù [rank=0] loading palermo_park.palermo_park from raw samples with raw size 3 bytes
latent_webdataset.py - [rank {rank}] Loading encoder_hidden_states and attention_mask from .emb format
latent_webdataset.py - üìù [rank=0] loading palermo_square.palermo_square from raw samples with raw size 3 bytes
latent_webdataset.py - [rank {rank}] Loading encoder_hidden_states and attention_mask from .emb format
latent_webdataset.py - üìù [rank=0] loading parched_canal.parched_canal from raw samples with raw size 3 bytes
latent_webdataset.py - [rank {rank}] Loading encoder_hidden_states and attention_mask from .emb format
latent_webdataset.py - üìù [rank=0] loading park_bench.park_bench from raw samples with raw size 3 bytes
latent_webdataset.py - [rank {rank}] Loading encoder_hidden_states and attention_mask from .emb format
latent_webdataset.py - üìù [rank=0] loading park_parking.park_parking from raw samples with raw size 3 bytes
latent_webdataset.py - [rank {rank}] Loading encoder_hidden_states and attention_mask from .emb format
latent_webdataset.py - üìù [rank=0] loading parking_garage.parking_garage from raw samples with raw size 3 bytes
latent_webdataset.py - [rank {rank}] Loading encoder_hidden_states and attention_mask from .emb format
latent_webdataset.py - üìù [rank=0] loading partial_eclipse.partial_eclipse from raw samples with raw size 3 bytes
latent_webdataset.py - [rank {rank}] Loading encoder_hidden_states and attention_mask from .emb format
latent_webdataset.py - üìù [rank=0] loading passendorf_snow.passendorf_snow from raw samples with raw size 3 bytes
latent_webdataset.py - [rank {rank}] Loading encoder_hidden_states and attention_mask from .emb format
latent_webdataset.py - üìù [rank=0] loading paul_lobe_haus.paul_lobe_haus from raw samples with raw size 3 bytes
latent_webdataset.py - [rank {rank}] Loading encoder_hidden_states and attention_mask from .emb format
latent_webdataset.py - üìù [rank=0] loading pedestrian_overpass.pedestrian_overpass from raw samples with raw size 3 bytes
latent_webdataset.py - [rank {rank}] Loading encoder_hidden_states and attention_mask from .emb format
latent_webdataset.py - üìù [rank=0] loading peppermint_powerplant.peppermint_powerplant from raw samples with raw size 3 bytes
latent_webdataset.py - [rank {rank}] Loading encoder_hidden_states and attention_mask from .emb format
latent_webdataset.py - üìù [rank=0] loading peppermint_powerplant_2.peppermint_powerplant_2 from raw samples with raw size 3 bytes
latent_webdataset.py - [rank {rank}] Loading encoder_hidden_states and attention_mask from .emb format
latent_webdataset.py - üìù [rank=0] loading petit_port.petit_port from raw samples with raw size 3 bytes
latent_webdataset.py - [rank {rank}] Loading encoder_hidden_states and attention_mask from .emb format
latent_webdataset.py - üìù [rank=0] loading phalzer_forest_01.phalzer_forest_01 from raw samples with raw size 3 bytes
latent_webdataset.py - [rank {rank}] Loading encoder_hidden_states and attention_mask from .emb format
latent_webdataset.py - üìù [rank=0] loading phone_shop.phone_shop from raw samples with raw size 3 bytes
latent_webdataset.py - [rank {rank}] Loading encoder_hidden_states and attention_mask from .emb format
latent_webdataset.py - üìù [rank=0] loading photo_studio_01.photo_studio_01 from raw samples with raw size 3 bytes
latent_webdataset.py - [rank {rank}] Loading encoder_hidden_states and attention_mask from .emb format
latent_webdataset.py - üìù [rank=0] loading photo_studio_broadway_hall.photo_studio_broadway_hall from raw samples with raw size 3 bytes
latent_webdataset.py - [rank {rank}] Loading encoder_hidden_states and attention_mask from .emb format
latent_webdataset.py - üìù [rank=0] loading photo_studio_loft_hall.photo_studio_loft_hall from raw samples with raw size 3 bytes
latent_webdataset.py - [rank {rank}] Loading encoder_hidden_states and attention_mask from .emb format
latent_webdataset.py - üìù [rank=0] loading photo_studio_london_hall.photo_studio_london_hall from raw samples with raw size 3 bytes
latent_webdataset.py - [rank {rank}] Loading encoder_hidden_states and attention_mask from .emb format
latent_webdataset.py - üìù [rank=0] loading piazza_bologni.piazza_bologni from raw samples with raw size 3 bytes
latent_webdataset.py - [rank {rank}] Loading encoder_hidden_states and attention_mask from .emb format
latent_webdataset.py - üìù [rank=0] loading piazza_martin_lutero.piazza_martin_lutero from raw samples with raw size 3 bytes
latent_webdataset.py - [rank {rank}] Loading encoder_hidden_states and attention_mask from .emb format
latent_webdataset.py - üìù [rank=0] loading piazza_san_marco.piazza_san_marco from raw samples with raw size 3 bytes
latent_webdataset.py - [rank {rank}] Loading encoder_hidden_states and attention_mask from .emb format
latent_webdataset.py - üìù [rank=0] loading pillars.pillars from raw samples with raw size 3 bytes
latent_webdataset.py - [rank {rank}] Loading encoder_hidden_states and attention_mask from .emb format
latent_webdataset.py - üìù [rank=0] loading pine_attic.pine_attic from raw samples with raw size 3 bytes
latent_webdataset.py - [rank {rank}] Loading encoder_hidden_states and attention_mask from .emb format
latent_webdataset.py - üìù [rank=0] loading pink_sunrise.pink_sunrise from raw samples with raw size 3 bytes
latent_webdataset.py - [rank {rank}] Loading encoder_hidden_states and attention_mask from .emb format
latent_webdataset.py - üìù [rank=0] loading pizzo_pernice.pizzo_pernice from raw samples with raw size 3 bytes
latent_webdataset.py - [rank {rank}] Loading encoder_hidden_states and attention_mask from .emb format
latent_webdataset.py - üìù [rank=0] loading pizzo_pernice_puresky.pizzo_pernice_puresky from raw samples with raw size 3 bytes
latent_webdataset.py - [rank {rank}] Loading encoder_hidden_states and attention_mask from .emb format
latent_webdataset.py - üìù [rank=0] loading poly_haven_studio.poly_haven_studio from raw samples with raw size 3 bytes
latent_webdataset.py - [rank {rank}] Loading encoder_hidden_states and attention_mask from .emb format
latent_webdataset.py - üìù [rank=0] loading pond.pond from raw samples with raw size 3 bytes
latent_webdataset.py - [rank {rank}] Loading encoder_hidden_states and attention_mask from .emb format
latent_webdataset.py - üìù [rank=0] loading pond_bridge_night.pond_bridge_night from raw samples with raw size 3 bytes
latent_webdataset.py - [rank {rank}] Loading encoder_hidden_states and attention_mask from .emb format
latent_webdataset.py - üìù [rank=0] loading pool.pool from raw samples with raw size 3 bytes
latent_webdataset.py - [rank {rank}] Loading encoder_hidden_states and attention_mask from .emb format
latent_webdataset.py - üìù [rank=0] loading portland_landing_pad.portland_landing_pad from raw samples with raw size 3 bytes
latent_webdataset.py - [rank {rank}] Loading encoder_hidden_states and attention_mask from .emb format
latent_webdataset.py - üìù [rank=0] loading potsdamer_platz.potsdamer_platz from raw samples with raw size 3 bytes
latent_webdataset.py - [rank {rank}] Loading encoder_hidden_states and attention_mask from .emb format
latent_webdataset.py - üìù [rank=0] loading preller_drive.preller_drive from raw samples with raw size 3 bytes
latent_webdataset.py - [rank {rank}] Loading encoder_hidden_states and attention_mask from .emb format
latent_webdataset.py - üìù [rank=0] loading pretoria_gardens.pretoria_gardens from raw samples with raw size 3 bytes
latent_webdataset.py - [rank {rank}] Loading encoder_hidden_states and attention_mask from .emb format
latent_webdataset.py - üìù [rank=0] loading pretville_cinema.pretville_cinema from raw samples with raw size 3 bytes
latent_webdataset.py - [rank {rank}] Loading encoder_hidden_states and attention_mask from .emb format
latent_webdataset.py - üìù [rank=0] loading pretville_street.pretville_street from raw samples with raw size 3 bytes
latent_webdataset.py - [rank {rank}] Loading encoder_hidden_states and attention_mask from .emb format
latent_webdataset.py - üìù [rank=0] loading promenade_de_vidy.promenade_de_vidy from raw samples with raw size 3 bytes
latent_webdataset.py - [rank {rank}] Loading encoder_hidden_states and attention_mask from .emb format
latent_webdataset.py - üìù [rank=0] loading provence_studio.provence_studio from raw samples with raw size 3 bytes
latent_webdataset.py - [rank {rank}] Loading encoder_hidden_states and attention_mask from .emb format
latent_webdataset.py - üìù [rank=0] loading pump_house.pump_house from raw samples with raw size 3 bytes
latent_webdataset.py - [rank {rank}] Loading encoder_hidden_states and attention_mask from .emb format
latent_webdataset.py - üìù [rank=0] loading pump_station.pump_station from raw samples with raw size 3 bytes
latent_webdataset.py - [rank {rank}] Loading encoder_hidden_states and attention_mask from .emb format
latent_webdataset.py - üìù [rank=0] loading pylons.pylons from raw samples with raw size 3 bytes
latent_webdataset.py - [rank {rank}] Loading encoder_hidden_states and attention_mask from .emb format
latent_webdataset.py - üìù [rank=0] loading quarry_01.quarry_01 from raw samples with raw size 3 bytes
latent_webdataset.py - [rank {rank}] Loading encoder_hidden_states and attention_mask from .emb format
latent_webdataset.py - üìù [rank=0] loading quarry_01_puresky.quarry_01_puresky from raw samples with raw size 3 bytes
latent_webdataset.py - [rank {rank}] Loading encoder_hidden_states and attention_mask from .emb format
latent_webdataset.py - üìù [rank=0] loading quarry_02.quarry_02 from raw samples with raw size 3 bytes
latent_webdataset.py - [rank {rank}] Loading encoder_hidden_states and attention_mask from .emb format
latent_webdataset.py - üìù [rank=0] loading quarry_03.quarry_03 from raw samples with raw size 3 bytes
latent_webdataset.py - [rank {rank}] Loading encoder_hidden_states and attention_mask from .emb format
latent_webdataset.py - üìù [rank=0] loading quarry_04.quarry_04 from raw samples with raw size 3 bytes
latent_webdataset.py - [rank {rank}] Loading encoder_hidden_states and attention_mask from .emb format
latent_webdataset.py - üìù [rank=0] loading quarry_cloudy.quarry_cloudy from raw samples with raw size 3 bytes
latent_webdataset.py - [rank {rank}] Loading encoder_hidden_states and attention_mask from .emb format
latent_webdataset.py - üìù [rank=0] loading quattro_canti.quattro_canti from raw samples with raw size 3 bytes
latent_webdataset.py - [rank {rank}] Loading encoder_hidden_states and attention_mask from .emb format
latent_webdataset.py - üìù [rank=0] loading qwantani.qwantani from raw samples with raw size 3 bytes
latent_webdataset.py - [rank {rank}] Loading encoder_hidden_states and attention_mask from .emb format
latent_webdataset.py - üìù [rank=0] loading qwantani_afternoon.qwantani_afternoon from raw samples with raw size 3 bytes
latent_webdataset.py - [rank {rank}] Loading encoder_hidden_states and attention_mask from .emb format
latent_webdataset.py - üìù [rank=0] loading qwantani_dawn.qwantani_dawn from raw samples with raw size 3 bytes
latent_webdataset.py - [rank {rank}] Loading encoder_hidden_states and attention_mask from .emb format
latent_webdataset.py - üìù [rank=0] loading qwantani_dusk_1.qwantani_dusk_1 from raw samples with raw size 3 bytes
latent_webdataset.py - [rank {rank}] Loading encoder_hidden_states and attention_mask from .emb format
latent_webdataset.py - üìù [rank=0] loading qwantani_dusk_2.qwantani_dusk_2 from raw samples with raw size 3 bytes
latent_webdataset.py - [rank {rank}] Loading encoder_hidden_states and attention_mask from .emb format
latent_webdataset.py - üìù [rank=0] loading qwantani_late_afternoon.qwantani_late_afternoon from raw samples with raw size 3 bytes
latent_webdataset.py - [rank {rank}] Loading encoder_hidden_states and attention_mask from .emb format
latent_webdataset.py - üìù [rank=0] loading qwantani_mid_morning.qwantani_mid_morning from raw samples with raw size 3 bytes
latent_webdataset.py - [rank {rank}] Loading encoder_hidden_states and attention_mask from .emb format
latent_webdataset.py - üìù [rank=0] loading qwantani_moon_noon.qwantani_moon_noon from raw samples with raw size 3 bytes
latent_webdataset.py - [rank {rank}] Loading encoder_hidden_states and attention_mask from .emb format
latent_webdataset.py - üìù [rank=0] loading qwantani_moonrise.qwantani_moonrise from raw samples with raw size 3 bytes
latent_webdataset.py - [rank {rank}] Loading encoder_hidden_states and attention_mask from .emb format
latent_webdataset.py - üìù [rank=0] loading qwantani_morning.qwantani_morning from raw samples with raw size 3 bytes
latent_webdataset.py - [rank {rank}] Loading encoder_hidden_states and attention_mask from .emb format
latent_webdataset.py - üìù [rank=0] loading qwantani_night.qwantani_night from raw samples with raw size 3 bytes
latent_webdataset.py - [rank {rank}] Loading encoder_hidden_states and attention_mask from .emb format
latent_webdataset.py - üìù [rank=0] loading qwantani_noon.qwantani_noon from raw samples with raw size 3 bytes
latent_webdataset.py - [rank {rank}] Loading encoder_hidden_states and attention_mask from .emb format
latent_webdataset.py - üìù [rank=0] loading qwantani_patio.qwantani_patio from raw samples with raw size 3 bytes
latent_webdataset.py - [rank {rank}] Loading encoder_hidden_states and attention_mask from .emb format
latent_webdataset.py - üìù [rank=0] loading qwantani_puresky.qwantani_puresky from raw samples with raw size 3 bytes
latent_webdataset.py - [rank {rank}] Loading encoder_hidden_states and attention_mask from .emb format
latent_webdataset.py - üìù [rank=0] loading qwantani_sunrise.qwantani_sunrise from raw samples with raw size 3 bytes
latent_webdataset.py - [rank {rank}] Loading encoder_hidden_states and attention_mask from .emb format
latent_webdataset.py - üìù [rank=0] loading qwantani_sunset.qwantani_sunset from raw samples with raw size 3 bytes
latent_webdataset.py - [rank {rank}] Loading encoder_hidden_states and attention_mask from .emb format
latent_webdataset.py - üìù [rank=0] loading railway_bridge_02.railway_bridge_02 from raw samples with raw size 3 bytes
latent_webdataset.py - [rank {rank}] Loading encoder_hidden_states and attention_mask from .emb format
latent_webdataset.py - üìù [rank=0] loading railway_bridges.railway_bridges from raw samples with raw size 3 bytes
latent_webdataset.py - [rank {rank}] Loading encoder_hidden_states and attention_mask from .emb format
latent_webdataset.py - üìù [rank=0] loading rainforest_trail.rainforest_trail from raw samples with raw size 3 bytes
latent_webdataset.py - [rank {rank}] Loading encoder_hidden_states and attention_mask from .emb format
latent_webdataset.py - üìù [rank=0] loading rathaus.rathaus from raw samples with raw size 3 bytes
latent_webdataset.py - [rank {rank}] Loading encoder_hidden_states and attention_mask from .emb format
latent_webdataset.py - üìù [rank=0] loading reading_room.reading_room from raw samples with raw size 3 bytes
latent_webdataset.py - [rank {rank}] Loading encoder_hidden_states and attention_mask from .emb format
latent_webdataset.py - üìù [rank=0] loading red_hill_cloudy.red_hill_cloudy from raw samples with raw size 3 bytes
latent_webdataset.py - [rank {rank}] Loading encoder_hidden_states and attention_mask from .emb format
latent_webdataset.py - üìù [rank=0] loading red_hill_curve.red_hill_curve from raw samples with raw size 3 bytes
latent_webdataset.py - [rank {rank}] Loading encoder_hidden_states and attention_mask from .emb format
latent_webdataset.py - üìù [rank=0] loading red_hill_straight.red_hill_straight from raw samples with raw size 3 bytes
latent_webdataset.py - [rank {rank}] Loading encoder_hidden_states and attention_mask from .emb format
latent_webdataset.py - üìù [rank=0] loading red_wall.red_wall from raw samples with raw size 3 bytes
latent_webdataset.py - [rank {rank}] Loading encoder_hidden_states and attention_mask from .emb format
latent_webdataset.py - üìù [rank=0] loading reichstag_1.reichstag_1 from raw samples with raw size 3 bytes
latent_webdataset.py - [rank {rank}] Loading encoder_hidden_states and attention_mask from .emb format
latent_webdataset.py - üìù [rank=0] loading reinforced_concrete_01.reinforced_concrete_01 from raw samples with raw size 3 bytes
latent_webdataset.py - [rank {rank}] Loading encoder_hidden_states and attention_mask from .emb format
latent_webdataset.py - üìù [rank=0] loading reinforced_concrete_02.reinforced_concrete_02 from raw samples with raw size 3 bytes
latent_webdataset.py - [rank {rank}] Loading encoder_hidden_states and attention_mask from .emb format
latent_webdataset.py - üìù [rank=0] loading residential_garden.residential_garden from raw samples with raw size 3 bytes
latent_webdataset.py - [rank {rank}] Loading encoder_hidden_states and attention_mask from .emb format
latent_webdataset.py - üìù [rank=0] loading resting_place_2.resting_place_2 from raw samples with raw size 3 bytes
latent_webdataset.py - [rank {rank}] Loading encoder_hidden_states and attention_mask from .emb format
latent_webdataset.py - üìù [rank=0] loading rhodes_memorial.rhodes_memorial from raw samples with raw size 3 bytes
latent_webdataset.py - [rank {rank}] Loading encoder_hidden_states and attention_mask from .emb format
latent_webdataset.py - üìù [rank=0] loading river_rocks.river_rocks from raw samples with raw size 3 bytes
latent_webdataset.py - [rank {rank}] Loading encoder_hidden_states and attention_mask from .emb format
latent_webdataset.py - üìù [rank=0] loading river_walk_1.river_walk_1 from raw samples with raw size 3 bytes
latent_webdataset.py - [rank {rank}] Loading encoder_hidden_states and attention_mask from .emb format
latent_webdataset.py - üìù [rank=0] loading river_walk_2.river_walk_2 from raw samples with raw size 3 bytes
latent_webdataset.py - [rank {rank}] Loading encoder_hidden_states and attention_mask from .emb format
latent_webdataset.py - üìù [rank=0] loading rocky_ridge.rocky_ridge from raw samples with raw size 3 bytes
latent_webdataset.py - [rank {rank}] Loading encoder_hidden_states and attention_mask from .emb format
latent_webdataset.py - üìù [rank=0] loading rocky_ridge_puresky.rocky_ridge_puresky from raw samples with raw size 3 bytes
latent_webdataset.py - [rank {rank}] Loading encoder_hidden_states and attention_mask from .emb format
latent_webdataset.py - üìù [rank=0] loading rogland_clear_night.rogland_clear_night from raw samples with raw size 3 bytes
latent_webdataset.py - [rank {rank}] Loading encoder_hidden_states and attention_mask from .emb format
latent_webdataset.py - üìù [rank=0] loading rogland_overcast.rogland_overcast from raw samples with raw size 3 bytes
latent_webdataset.py - [rank {rank}] Loading encoder_hidden_states and attention_mask from .emb format
latent_webdataset.py - üìù [rank=0] loading rogland_sunset.rogland_sunset from raw samples with raw size 3 bytes
latent_webdataset.py - [rank {rank}] Loading encoder_hidden_states and attention_mask from .emb format
latent_webdataset.py - üìù [rank=0] loading rolling_hills.rolling_hills from raw samples with raw size 3 bytes
latent_webdataset.py - [rank {rank}] Loading encoder_hidden_states and attention_mask from .emb format
latent_webdataset.py - üìù [rank=0] loading roof_garden.roof_garden from raw samples with raw size 3 bytes
latent_webdataset.py - [rank {rank}] Loading encoder_hidden_states and attention_mask from .emb format
latent_webdataset.py - üìù [rank=0] loading roofless_ruins.roofless_ruins from raw samples with raw size 3 bytes
latent_webdataset.py - [rank {rank}] Loading encoder_hidden_states and attention_mask from .emb format
latent_webdataset.py - üìù [rank=0] loading rooftop_day.rooftop_day from raw samples with raw size 3 bytes
latent_webdataset.py - [rank {rank}] Loading encoder_hidden_states and attention_mask from .emb format
latent_webdataset.py - üìù [rank=0] loading rooitou_park.rooitou_park from raw samples with raw size 3 bytes
latent_webdataset.py - [rank {rank}] Loading encoder_hidden_states and attention_mask from .emb format
latent_webdataset.py - üìù [rank=0] loading rosendal_mountain_midmorning.rosendal_mountain_midmorning from raw samples with raw size 3 bytes
latent_webdataset.py - [rank {rank}] Loading encoder_hidden_states and attention_mask from .emb format
latent_webdataset.py - üìù [rank=0] loading rosendal_park_sunset.rosendal_park_sunset from raw samples with raw size 3 bytes
latent_webdataset.py - [rank {rank}] Loading encoder_hidden_states and attention_mask from .emb format
latent_webdataset.py - üìù [rank=0] loading rosendal_park_sunset_puresky.rosendal_park_sunset_puresky from raw samples with raw size 3 bytes
latent_webdataset.py - [rank {rank}] Loading encoder_hidden_states and attention_mask from .emb format
latent_webdataset.py - üìù [rank=0] loading rosendal_plains_1.rosendal_plains_1 from raw samples with raw size 3 bytes
latent_webdataset.py - [rank {rank}] Loading encoder_hidden_states and attention_mask from .emb format
latent_webdataset.py - üìù [rank=0] loading rosendal_plains_2.rosendal_plains_2 from raw samples with raw size 3 bytes
latent_webdataset.py - [rank {rank}] Loading encoder_hidden_states and attention_mask from .emb format
latent_webdataset.py - üìù [rank=0] loading rostock_arches.rostock_arches from raw samples with raw size 3 bytes
latent_webdataset.py - [rank {rank}] Loading encoder_hidden_states and attention_mask from .emb format
latent_webdataset.py - üìù [rank=0] loading rostock_laage_airport.rostock_laage_airport from raw samples with raw size 3 bytes
latent_webdataset.py - [rank {rank}] Loading encoder_hidden_states and attention_mask from .emb format
latent_webdataset.py - üìù [rank=0] loading round_platform.round_platform from raw samples with raw size 3 bytes
latent_webdataset.py - [rank {rank}] Loading encoder_hidden_states and attention_mask from .emb format
latent_webdataset.py - üìù [rank=0] loading royal_esplanade.royal_esplanade from raw samples with raw size 3 bytes
latent_webdataset.py - [rank {rank}] Loading encoder_hidden_states and attention_mask from .emb format
latent_webdataset.py - üìù [rank=0] loading ruckenkreuz.ruckenkreuz from raw samples with raw size 3 bytes
latent_webdataset.py - [rank {rank}] Loading encoder_hidden_states and attention_mask from .emb format
latent_webdataset.py - üìù [rank=0] loading rural_asphalt_road.rural_asphalt_road from raw samples with raw size 3 bytes
latent_webdataset.py - [rank {rank}] Loading encoder_hidden_states and attention_mask from .emb format
latent_webdataset.py - üìù [rank=0] loading rural_crossroads.rural_crossroads from raw samples with raw size 3 bytes
latent_webdataset.py - [rank {rank}] Loading encoder_hidden_states and attention_mask from .emb format
latent_webdataset.py - üìù [rank=0] loading rural_graffiti_tower.rural_graffiti_tower from raw samples with raw size 3 bytes
latent_webdataset.py - [rank {rank}] Loading encoder_hidden_states and attention_mask from .emb format
latent_webdataset.py - üìù [rank=0] loading rural_landscape.rural_landscape from raw samples with raw size 3 bytes
latent_webdataset.py - [rank {rank}] Loading encoder_hidden_states and attention_mask from .emb format
latent_webdataset.py - üìù [rank=0] loading rural_winter_roadside.rural_winter_roadside from raw samples with raw size 3 bytes
latent_webdataset.py - [rank {rank}] Loading encoder_hidden_states and attention_mask from .emb format
latent_webdataset.py - üìù [rank=0] loading rustig_koppie.rustig_koppie from raw samples with raw size 3 bytes
latent_webdataset.py - [rank {rank}] Loading encoder_hidden_states and attention_mask from .emb format
latent_webdataset.py - üìù [rank=0] loading rustig_koppie_puresky.rustig_koppie_puresky from raw samples with raw size 3 bytes
latent_webdataset.py - [rank {rank}] Loading encoder_hidden_states and attention_mask from .emb format
latent_webdataset.py - üìù [rank=0] loading sabie_tent.sabie_tent from raw samples with raw size 3 bytes
latent_webdataset.py - [rank {rank}] Loading encoder_hidden_states and attention_mask from .emb format
latent_webdataset.py - üìù [rank=0] loading safari_sunset.safari_sunset from raw samples with raw size 3 bytes
latent_webdataset.py - [rank {rank}] Loading encoder_hidden_states and attention_mask from .emb format
latent_webdataset.py - üìù [rank=0] loading san_giuseppe_bridge.san_giuseppe_bridge from raw samples with raw size 3 bytes
latent_webdataset.py - [rank {rank}] Loading encoder_hidden_states and attention_mask from .emb format
latent_webdataset.py - üìù [rank=0] loading sandsloot.sandsloot from raw samples with raw size 3 bytes
latent_webdataset.py - [rank {rank}] Loading encoder_hidden_states and attention_mask from .emb format
latent_webdataset.py - üìù [rank=0] loading satara_night.satara_night from raw samples with raw size 3 bytes
latent_webdataset.py - [rank {rank}] Loading encoder_hidden_states and attention_mask from .emb format
latent_webdataset.py - üìù [rank=0] loading satara_night_no_lamps.satara_night_no_lamps from raw samples with raw size 3 bytes
latent_webdataset.py - [rank {rank}] Loading encoder_hidden_states and attention_mask from .emb format
latent_webdataset.py - üìù [rank=0] loading schadowplatz.schadowplatz from raw samples with raw size 3 bytes
latent_webdataset.py - [rank {rank}] Loading encoder_hidden_states and attention_mask from .emb format
latent_webdataset.py - üìù [rank=0] loading school_hall.school_hall from raw samples with raw size 3 bytes
latent_webdataset.py - [rank {rank}] Loading encoder_hidden_states and attention_mask from .emb format
latent_webdataset.py - üìù [rank=0] loading school_quad.school_quad from raw samples with raw size 3 bytes
latent_webdataset.py - [rank {rank}] Loading encoder_hidden_states and attention_mask from .emb format
latent_webdataset.py - üìù [rank=0] loading sculpture_exhibition.sculpture_exhibition from raw samples with raw size 3 bytes
latent_webdataset.py - [rank {rank}] Loading encoder_hidden_states and attention_mask from .emb format
latent_webdataset.py - üìù [rank=0] loading scythian_tombs.scythian_tombs from raw samples with raw size 3 bytes
latent_webdataset.py - [rank {rank}] Loading encoder_hidden_states and attention_mask from .emb format
latent_webdataset.py - üìù [rank=0] loading scythian_tombs_2.scythian_tombs_2 from raw samples with raw size 3 bytes
latent_webdataset.py - [rank {rank}] Loading encoder_hidden_states and attention_mask from .emb format
latent_webdataset.py - üìù [rank=0] loading scythian_tombs_puresky.scythian_tombs_puresky from raw samples with raw size 3 bytes
latent_webdataset.py - [rank {rank}] Loading encoder_hidden_states and attention_mask from .emb format
latent_webdataset.py - üìù [rank=0] loading secluded_beach.secluded_beach from raw samples with raw size 3 bytes
latent_webdataset.py - [rank {rank}] Loading encoder_hidden_states and attention_mask from .emb format
latent_webdataset.py - üìù [rank=0] loading sepulchral_chapel_basement.sepulchral_chapel_basement from raw samples with raw size 3 bytes
latent_webdataset.py - [rank {rank}] Loading encoder_hidden_states and attention_mask from .emb format
latent_webdataset.py - üìù [rank=0] loading sepulchral_chapel_rotunda.sepulchral_chapel_rotunda from raw samples with raw size 3 bytes
latent_webdataset.py - [rank {rank}] Loading encoder_hidden_states and attention_mask from .emb format
latent_webdataset.py - üìù [rank=0] loading shady_patch.shady_patch from raw samples with raw size 3 bytes
latent_webdataset.py - [rank {rank}] Loading encoder_hidden_states and attention_mask from .emb format
latent_webdataset.py - üìù [rank=0] loading short_tunnel.short_tunnel from raw samples with raw size 3 bytes
latent_webdataset.py - [rank {rank}] Loading encoder_hidden_states and attention_mask from .emb format
latent_webdataset.py - üìù [rank=0] loading shudu_lake.shudu_lake from raw samples with raw size 3 bytes
latent_webdataset.py - [rank {rank}] Loading encoder_hidden_states and attention_mask from .emb format
latent_webdataset.py - üìù [rank=0] loading signal_hill_dawn.signal_hill_dawn from raw samples with raw size 3 bytes
latent_webdataset.py - [rank {rank}] Loading encoder_hidden_states and attention_mask from .emb format
latent_webdataset.py - üìù [rank=0] loading signal_hill_sunrise.signal_hill_sunrise from raw samples with raw size 3 bytes
latent_webdataset.py - [rank {rank}] Loading encoder_hidden_states and attention_mask from .emb format
latent_webdataset.py - üìù [rank=0] loading simons_town_harbour.simons_town_harbour from raw samples with raw size 3 bytes
latent_webdataset.py - [rank {rank}] Loading encoder_hidden_states and attention_mask from .emb format
latent_webdataset.py - üìù [rank=0] loading simons_town_road.simons_town_road from raw samples with raw size 3 bytes
latent_webdataset.py - [rank {rank}] Loading encoder_hidden_states and attention_mask from .emb format
latent_webdataset.py - üìù [rank=0] loading simons_town_rocks.simons_town_rocks from raw samples with raw size 3 bytes
latent_webdataset.py - [rank {rank}] Loading encoder_hidden_states and attention_mask from .emb format
latent_webdataset.py - üìù [rank=0] loading sisulu.sisulu from raw samples with raw size 3 bytes
latent_webdataset.py - [rank {rank}] Loading encoder_hidden_states and attention_mask from .emb format
latent_webdataset.py - üìù [rank=0] loading skate_park.skate_park from raw samples with raw size 3 bytes
latent_webdataset.py - [rank {rank}] Loading encoder_hidden_states and attention_mask from .emb format
latent_webdataset.py - üìù [rank=0] loading skidpan.skidpan from raw samples with raw size 3 bytes
latent_webdataset.py - [rank {rank}] Loading encoder_hidden_states and attention_mask from .emb format
latent_webdataset.py - üìù [rank=0] loading skukuza_golf.skukuza_golf from raw samples with raw size 3 bytes
latent_webdataset.py - [rank {rank}] Loading encoder_hidden_states and attention_mask from .emb format
latent_webdataset.py - üìù [rank=0] loading small_cathedral.small_cathedral from raw samples with raw size 3 bytes
latent_webdataset.py - [rank {rank}] Loading encoder_hidden_states and attention_mask from .emb format
latent_webdataset.py - üìù [rank=0] loading small_cave.small_cave from raw samples with raw size 3 bytes
latent_webdataset.py - [rank {rank}] Loading encoder_hidden_states and attention_mask from .emb format
latent_webdataset.py - üìù [rank=0] loading small_empty_house.small_empty_house from raw samples with raw size 3 bytes
latent_webdataset.py - [rank {rank}] Loading encoder_hidden_states and attention_mask from .emb format
latent_webdataset.py - üìù [rank=0] loading small_empty_room_1.small_empty_room_1 from raw samples with raw size 3 bytes
latent_webdataset.py - [rank {rank}] Loading encoder_hidden_states and attention_mask from .emb format
latent_webdataset.py - üìù [rank=0] loading small_empty_room_2.small_empty_room_2 from raw samples with raw size 3 bytes
latent_webdataset.py - [rank {rank}] Loading encoder_hidden_states and attention_mask from .emb format
latent_webdataset.py - üìù [rank=0] loading small_empty_room_3.small_empty_room_3 from raw samples with raw size 3 bytes
latent_webdataset.py - [rank {rank}] Loading encoder_hidden_states and attention_mask from .emb format
latent_webdataset.py - üìù [rank=0] loading small_empty_room_4.small_empty_room_4 from raw samples with raw size 3 bytes
latent_webdataset.py - [rank {rank}] Loading encoder_hidden_states and attention_mask from .emb format
latent_webdataset.py - üìù [rank=0] loading small_hangar_01.small_hangar_01 from raw samples with raw size 3 bytes
latent_webdataset.py - [rank {rank}] Loading encoder_hidden_states and attention_mask from .emb format
latent_webdataset.py - üìù [rank=0] loading small_hangar_02.small_hangar_02 from raw samples with raw size 3 bytes
latent_webdataset.py - [rank {rank}] Loading encoder_hidden_states and attention_mask from .emb format
latent_webdataset.py - üìù [rank=0] loading small_harbor_01.small_harbor_01 from raw samples with raw size 3 bytes
latent_webdataset.py - [rank {rank}] Loading encoder_hidden_states and attention_mask from .emb format
latent_webdataset.py - üìù [rank=0] loading small_harbor_02.small_harbor_02 from raw samples with raw size 3 bytes
latent_webdataset.py - [rank {rank}] Loading encoder_hidden_states and attention_mask from .emb format
latent_webdataset.py - üìù [rank=0] loading small_harbour_morning.small_harbour_morning from raw samples with raw size 3 bytes
latent_webdataset.py - [rank {rank}] Loading encoder_hidden_states and attention_mask from .emb format
latent_webdataset.py - üìù [rank=0] loading small_harbour_sunset.small_harbour_sunset from raw samples with raw size 3 bytes
latent_webdataset.py - [rank {rank}] Loading encoder_hidden_states and attention_mask from .emb format
latent_webdataset.py - üìù [rank=0] loading small_rural_road.small_rural_road from raw samples with raw size 3 bytes
latent_webdataset.py - [rank {rank}] Loading encoder_hidden_states and attention_mask from .emb format
latent_webdataset.py - üìù [rank=0] loading small_rural_road_02.small_rural_road_02 from raw samples with raw size 3 bytes
latent_webdataset.py - [rank {rank}] Loading encoder_hidden_states and attention_mask from .emb format
latent_webdataset.py - üìù [rank=0] loading snow_field.snow_field from raw samples with raw size 3 bytes
latent_webdataset.py - [rank {rank}] Loading encoder_hidden_states and attention_mask from .emb format
latent_webdataset.py - üìù [rank=0] loading snow_field_2.snow_field_2 from raw samples with raw size 3 bytes
latent_webdataset.py - [rank {rank}] Loading encoder_hidden_states and attention_mask from .emb format
latent_webdataset.py - üìù [rank=0] loading snow_field_2_puresky.snow_field_2_puresky from raw samples with raw size 3 bytes
latent_webdataset.py - [rank {rank}] Loading encoder_hidden_states and attention_mask from .emb format
latent_webdataset.py - üìù [rank=0] loading snow_field_puresky.snow_field_puresky from raw samples with raw size 3 bytes
latent_webdataset.py - [rank {rank}] Loading encoder_hidden_states and attention_mask from .emb format
latent_webdataset.py - üìù [rank=0] loading snowy_cemetery.snowy_cemetery from raw samples with raw size 3 bytes
latent_webdataset.py - [rank {rank}] Loading encoder_hidden_states and attention_mask from .emb format
latent_webdataset.py - üìù [rank=0] loading snowy_field.snowy_field from raw samples with raw size 3 bytes
latent_webdataset.py - [rank {rank}] Loading encoder_hidden_states and attention_mask from .emb format
latent_webdataset.py - üìù [rank=0] loading snowy_forest.snowy_forest from raw samples with raw size 3 bytes
latent_webdataset.py - [rank {rank}] Loading encoder_hidden_states and attention_mask from .emb format
latent_webdataset.py - üìù [rank=0] loading snowy_forest_path_01.snowy_forest_path_01 from raw samples with raw size 3 bytes
latent_webdataset.py - [rank {rank}] Loading encoder_hidden_states and attention_mask from .emb format
latent_webdataset.py - üìù [rank=0] loading snowy_forest_path_02.snowy_forest_path_02 from raw samples with raw size 3 bytes
latent_webdataset.py - [rank {rank}] Loading encoder_hidden_states and attention_mask from .emb format
latent_webdataset.py - üìù [rank=0] loading snowy_hillside_02.snowy_hillside_02 from raw samples with raw size 3 bytes
latent_webdataset.py - [rank {rank}] Loading encoder_hidden_states and attention_mask from .emb format
latent_webdataset.py - üìù [rank=0] loading snowy_park_01.snowy_park_01 from raw samples with raw size 3 bytes
latent_webdataset.py - [rank {rank}] Loading encoder_hidden_states and attention_mask from .emb format
latent_webdataset.py - üìù [rank=0] loading soliltude.soliltude from raw samples with raw size 3 bytes
latent_webdataset.py - [rank {rank}] Loading encoder_hidden_states and attention_mask from .emb format
latent_webdataset.py - üìù [rank=0] loading solitude_interior.solitude_interior from raw samples with raw size 3 bytes
latent_webdataset.py - [rank {rank}] Loading encoder_hidden_states and attention_mask from .emb format
latent_webdataset.py - üìù [rank=0] loading solitude_night.solitude_night from raw samples with raw size 3 bytes
latent_webdataset.py - [rank {rank}] Loading encoder_hidden_states and attention_mask from .emb format
latent_webdataset.py - üìù [rank=0] loading spaichingen_hill.spaichingen_hill from raw samples with raw size 3 bytes
latent_webdataset.py - [rank {rank}] Loading encoder_hidden_states and attention_mask from .emb format
latent_webdataset.py - üìù [rank=0] loading spiaggia_di_mondello.spiaggia_di_mondello from raw samples with raw size 3 bytes
latent_webdataset.py - [rank {rank}] Loading encoder_hidden_states and attention_mask from .emb format
latent_webdataset.py - üìù [rank=0] loading spree_bank.spree_bank from raw samples with raw size 3 bytes
latent_webdataset.py - [rank {rank}] Loading encoder_hidden_states and attention_mask from .emb format
latent_webdataset.py - üìù [rank=0] loading spruit_dawn.spruit_dawn from raw samples with raw size 3 bytes
latent_webdataset.py - [rank {rank}] Loading encoder_hidden_states and attention_mask from .emb format
latent_webdataset.py - üìù [rank=0] loading spruit_sunrise.spruit_sunrise from raw samples with raw size 3 bytes
latent_webdataset.py - [rank {rank}] Loading encoder_hidden_states and attention_mask from .emb format
latent_webdataset.py - üìù [rank=0] loading st_fagans_interior.st_fagans_interior from raw samples with raw size 3 bytes
latent_webdataset.py - [rank {rank}] Loading encoder_hidden_states and attention_mask from .emb format
latent_webdataset.py - üìù [rank=0] loading st_peters_square_night.st_peters_square_night from raw samples with raw size 3 bytes
latent_webdataset.py - [rank {rank}] Loading encoder_hidden_states and attention_mask from .emb format
latent_webdataset.py - üìù [rank=0] loading stadium_01.stadium_01 from raw samples with raw size 3 bytes
latent_webdataset.py - [rank {rank}] Loading encoder_hidden_states and attention_mask from .emb format
latent_webdataset.py - üìù [rank=0] loading steinbach_field.steinbach_field from raw samples with raw size 3 bytes
latent_webdataset.py - [rank {rank}] Loading encoder_hidden_states and attention_mask from .emb format
latent_webdataset.py - üìù [rank=0] loading sterkspruit_falls.sterkspruit_falls from raw samples with raw size 3 bytes
latent_webdataset.py - [rank {rank}] Loading encoder_hidden_states and attention_mask from .emb format
latent_webdataset.py - üìù [rank=0] loading stone_alley.stone_alley from raw samples with raw size 3 bytes
latent_webdataset.py - [rank {rank}] Loading encoder_hidden_states and attention_mask from .emb format
latent_webdataset.py - üìù [rank=0] loading stone_alley_02.stone_alley_02 from raw samples with raw size 3 bytes
latent_webdataset.py - [rank {rank}] Loading encoder_hidden_states and attention_mask from .emb format
latent_webdataset.py - üìù [rank=0] loading stone_alley_03.stone_alley_03 from raw samples with raw size 3 bytes
latent_webdataset.py - [rank {rank}] Loading encoder_hidden_states and attention_mask from .emb format
latent_webdataset.py - üìù [rank=0] loading stone_pines.stone_pines from raw samples with raw size 3 bytes
latent_webdataset.py - [rank {rank}] Loading encoder_hidden_states and attention_mask from .emb format
latent_webdataset.py - üìù [rank=0] loading stream.stream from raw samples with raw size 3 bytes
latent_webdataset.py - [rank {rank}] Loading encoder_hidden_states and attention_mask from .emb format
latent_webdataset.py - üìù [rank=0] loading street_lamp.street_lamp from raw samples with raw size 3 bytes
latent_webdataset.py - [rank {rank}] Loading encoder_hidden_states and attention_mask from .emb format
latent_webdataset.py - üìù [rank=0] loading studio_country_hall.studio_country_hall from raw samples with raw size 3 bytes
latent_webdataset.py - [rank {rank}] Loading encoder_hidden_states and attention_mask from .emb format
latent_webdataset.py - üìù [rank=0] loading studio_garden.studio_garden from raw samples with raw size 3 bytes
latent_webdataset.py - [rank {rank}] Loading encoder_hidden_states and attention_mask from .emb format
latent_webdataset.py - üìù [rank=0] loading studio_small_02.studio_small_02 from raw samples with raw size 3 bytes
latent_webdataset.py - [rank {rank}] Loading encoder_hidden_states and attention_mask from .emb format
latent_webdataset.py - üìù [rank=0] loading studio_small_03.studio_small_03 from raw samples with raw size 3 bytes
latent_webdataset.py - [rank {rank}] Loading encoder_hidden_states and attention_mask from .emb format
latent_webdataset.py - üìù [rank=0] loading studio_small_04.studio_small_04 from raw samples with raw size 3 bytes
latent_webdataset.py - [rank {rank}] Loading encoder_hidden_states and attention_mask from .emb format
latent_webdataset.py - üìù [rank=0] loading studio_small_05.studio_small_05 from raw samples with raw size 3 bytes
latent_webdataset.py - [rank {rank}] Loading encoder_hidden_states and attention_mask from .emb format
latent_webdataset.py - üìù [rank=0] loading studio_small_06.studio_small_06 from raw samples with raw size 3 bytes
latent_webdataset.py - [rank {rank}] Loading encoder_hidden_states and attention_mask from .emb format
latent_webdataset.py - üìù [rank=0] loading studio_small_07.studio_small_07 from raw samples with raw size 3 bytes
latent_webdataset.py - [rank {rank}] Loading encoder_hidden_states and attention_mask from .emb format
latent_webdataset.py - üìù [rank=0] loading studio_small_08.studio_small_08 from raw samples with raw size 3 bytes
latent_webdataset.py - [rank {rank}] Loading encoder_hidden_states and attention_mask from .emb format
latent_webdataset.py - üìù [rank=0] loading studio_small_09.studio_small_09 from raw samples with raw size 3 bytes
latent_webdataset.py - [rank {rank}] Loading encoder_hidden_states and attention_mask from .emb format
latent_webdataset.py - üìù [rank=0] loading stuttgart_hillside.stuttgart_hillside from raw samples with raw size 3 bytes
latent_webdataset.py - [rank {rank}] Loading encoder_hidden_states and attention_mask from .emb format
latent_webdataset.py - üìù [rank=0] loading stuttgart_suburbs.stuttgart_suburbs from raw samples with raw size 3 bytes
latent_webdataset.py - [rank {rank}] Loading encoder_hidden_states and attention_mask from .emb format
latent_webdataset.py - üìù [rank=0] loading suburban_field_01.suburban_field_01 from raw samples with raw size 3 bytes
latent_webdataset.py - [rank {rank}] Loading encoder_hidden_states and attention_mask from .emb format
latent_webdataset.py - üìù [rank=0] loading suburban_parking_area.suburban_parking_area from raw samples with raw size 3 bytes
latent_webdataset.py - [rank {rank}] Loading encoder_hidden_states and attention_mask from .emb format
latent_webdataset.py - üìù [rank=0] loading subway_entrance.subway_entrance from raw samples with raw size 3 bytes
latent_webdataset.py - [rank {rank}] Loading encoder_hidden_states and attention_mask from .emb format
latent_webdataset.py - üìù [rank=0] loading summer_stage_01.summer_stage_01 from raw samples with raw size 3 bytes
latent_webdataset.py - [rank {rank}] Loading encoder_hidden_states and attention_mask from .emb format
latent_webdataset.py - üìù [rank=0] loading summer_stage_02.summer_stage_02 from raw samples with raw size 3 bytes
latent_webdataset.py - [rank {rank}] Loading encoder_hidden_states and attention_mask from .emb format
latent_webdataset.py - üìù [rank=0] loading sunflowers_puresky.sunflowers_puresky from raw samples with raw size 3 bytes
latent_webdataset.py - [rank {rank}] Loading encoder_hidden_states and attention_mask from .emb format
latent_webdataset.py - üìù [rank=0] loading sunny_vondelpark.sunny_vondelpark from raw samples with raw size 3 bytes
latent_webdataset.py - [rank {rank}] Loading encoder_hidden_states and attention_mask from .emb format
latent_webdataset.py - üìù [rank=0] loading sunset_in_the_chalk_quarry.sunset_in_the_chalk_quarry from raw samples with raw size 3 bytes
latent_webdataset.py - [rank {rank}] Loading encoder_hidden_states and attention_mask from .emb format
latent_webdataset.py - üìù [rank=0] loading sunset_jhbcentral.sunset_jhbcentral from raw samples with raw size 3 bytes
latent_webdataset.py - [rank {rank}] Loading encoder_hidden_states and attention_mask from .emb format
latent_webdataset.py - üìù [rank=0] loading surgery.surgery from raw samples with raw size 3 bytes
latent_webdataset.py - [rank {rank}] Loading encoder_hidden_states and attention_mask from .emb format
latent_webdataset.py - üìù [rank=0] loading syferfontein_0d_clear.syferfontein_0d_clear from raw samples with raw size 3 bytes
latent_webdataset.py - [rank {rank}] Loading encoder_hidden_states and attention_mask from .emb format
latent_webdataset.py - üìù [rank=0] loading syferfontein_0d_clear_puresky.syferfontein_0d_clear_puresky from raw samples with raw size 3 bytes
latent_webdataset.py - [rank {rank}] Loading encoder_hidden_states and attention_mask from .emb format
latent_webdataset.py - üìù [rank=0] loading syferfontein_18d_clear.syferfontein_18d_clear from raw samples with raw size 3 bytes
latent_webdataset.py - [rank {rank}] Loading encoder_hidden_states and attention_mask from .emb format
latent_webdataset.py - üìù [rank=0] loading syferfontein_18d_clear_puresky.syferfontein_18d_clear_puresky from raw samples with raw size 3 bytes
latent_webdataset.py - [rank {rank}] Loading encoder_hidden_states and attention_mask from .emb format
[latent_webdataset] rank 0 ‚Äì is_eval=False ‚Äì  length = 630 (expected ~630)
latent_webdataset.py - [latent_webdataset] rank=0 ‚Äì global shuffle applied for training
[latent_webdataset] rank 0 ‚Äì taking slice [0:630] = 630 samples from 630 total
[latent_webdataset] rank 0 ‚Äì DataLoader ready: 630 samples, batch_size=2, ~315 batches
Train dataloader created successfully
Successfully loaded a sample batch with keys: dict_keys(['latent', 'encoder_hidden_states', 'attention_mask'])
[latent_webdataset] rank=0 loading 70 raw samples
latent_webdataset.py - üìù [rank=0] loading abandoned_games_room_02.abandoned_games_room_02 from raw samples with raw size 3 bytes
latent_webdataset.py - [rank {rank}] Loading encoder_hidden_states and attention_mask from .emb format
latent_webdataset.py - üìù [rank=0] loading autumn_field.autumn_field from raw samples with raw size 3 bytes
latent_webdataset.py - [rank {rank}] Loading encoder_hidden_states and attention_mask from .emb format
latent_webdataset.py - üìù [rank=0] loading autoshop_01.autoshop_01 from raw samples with raw size 3 bytes
latent_webdataset.py - [rank {rank}] Loading encoder_hidden_states and attention_mask from .emb format
latent_webdataset.py - üìù [rank=0] loading air_museum_playground.air_museum_playground from raw samples with raw size 3 bytes
latent_webdataset.py - [rank {rank}] Loading encoder_hidden_states and attention_mask from .emb format
latent_webdataset.py - üìù [rank=0] loading aerodynamics_workshop.aerodynamics_workshop from raw samples with raw size 3 bytes
latent_webdataset.py - [rank {rank}] Loading encoder_hidden_states and attention_mask from .emb format
latent_webdataset.py - üìù [rank=0] loading aloe_farm_shade_house.aloe_farm_shade_house from raw samples with raw size 3 bytes
latent_webdataset.py - [rank {rank}] Loading encoder_hidden_states and attention_mask from .emb format
latent_webdataset.py - üìù [rank=0] loading belfast_sunset_puresky.belfast_sunset_puresky from raw samples with raw size 3 bytes
latent_webdataset.py - [rank {rank}] Loading encoder_hidden_states and attention_mask from .emb format
latent_webdataset.py - üìù [rank=0] loading acoustical_shell.acoustical_shell from raw samples with raw size 3 bytes
latent_webdataset.py - [rank {rank}] Loading encoder_hidden_states and attention_mask from .emb format
latent_webdataset.py - üìù [rank=0] loading birbeck_street_underpass.birbeck_street_underpass from raw samples with raw size 3 bytes
latent_webdataset.py - [rank {rank}] Loading encoder_hidden_states and attention_mask from .emb format
latent_webdataset.py - üìù [rank=0] loading birchwood.birchwood from raw samples with raw size 3 bytes
latent_webdataset.py - [rank {rank}] Loading encoder_hidden_states and attention_mask from .emb format
latent_webdataset.py - üìù [rank=0] loading blender_institute.blender_institute from raw samples with raw size 3 bytes
latent_webdataset.py - [rank {rank}] Loading encoder_hidden_states and attention_mask from .emb format
latent_webdataset.py - üìù [rank=0] loading bloem_train_track_cloudy.bloem_train_track_cloudy from raw samples with raw size 3 bytes
latent_webdataset.py - [rank {rank}] Loading encoder_hidden_states and attention_mask from .emb format
latent_webdataset.py - üìù [rank=0] loading blouberg_sunrise_1.blouberg_sunrise_1 from raw samples with raw size 3 bytes
latent_webdataset.py - [rank {rank}] Loading encoder_hidden_states and attention_mask from .emb format
latent_webdataset.py - üìù [rank=0] loading blue_lagoon_night.blue_lagoon_night from raw samples with raw size 3 bytes
latent_webdataset.py - [rank {rank}] Loading encoder_hidden_states and attention_mask from .emb format
latent_webdataset.py - üìù [rank=0] loading borghese_gardens.borghese_gardens from raw samples with raw size 3 bytes
latent_webdataset.py - [rank {rank}] Loading encoder_hidden_states and attention_mask from .emb format
latent_webdataset.py - üìù [rank=0] loading brick_factory_01.brick_factory_01 from raw samples with raw size 3 bytes
latent_webdataset.py - [rank {rank}] Loading encoder_hidden_states and attention_mask from .emb format
latent_webdataset.py - üìù [rank=0] loading cinema_lobby.cinema_lobby from raw samples with raw size 3 bytes
latent_webdataset.py - [rank {rank}] Loading encoder_hidden_states and attention_mask from .emb format
latent_webdataset.py - üìù [rank=0] loading burnt_warehouse.burnt_warehouse from raw samples with raw size 3 bytes
latent_webdataset.py - [rank {rank}] Loading encoder_hidden_states and attention_mask from .emb format
latent_webdataset.py - üìù [rank=0] loading cayley_lookout.cayley_lookout from raw samples with raw size 3 bytes
latent_webdataset.py - [rank {rank}] Loading encoder_hidden_states and attention_mask from .emb format
latent_webdataset.py - üìù [rank=0] loading comfy_cafe.comfy_cafe from raw samples with raw size 3 bytes
latent_webdataset.py - [rank {rank}] Loading encoder_hidden_states and attention_mask from .emb format
latent_webdataset.py - üìù [rank=0] loading country_club.country_club from raw samples with raw size 3 bytes
latent_webdataset.py - [rank {rank}] Loading encoder_hidden_states and attention_mask from .emb format
latent_webdataset.py - üìù [rank=0] loading courtyard_night.courtyard_night from raw samples with raw size 3 bytes
latent_webdataset.py - [rank {rank}] Loading encoder_hidden_states and attention_mask from .emb format
latent_webdataset.py - üìù [rank=0] loading dresden_moat.dresden_moat from raw samples with raw size 3 bytes
latent_webdataset.py - [rank {rank}] Loading encoder_hidden_states and attention_mask from .emb format
latent_webdataset.py - üìù [rank=0] loading dry_meadow.dry_meadow from raw samples with raw size 3 bytes
latent_webdataset.py - [rank {rank}] Loading encoder_hidden_states and attention_mask from .emb format
latent_webdataset.py - üìù [rank=0] loading evening_meadow.evening_meadow from raw samples with raw size 3 bytes
latent_webdataset.py - [rank {rank}] Loading encoder_hidden_states and attention_mask from .emb format
latent_webdataset.py - üìù [rank=0] loading factory_yard.factory_yard from raw samples with raw size 3 bytes
latent_webdataset.py - [rank {rank}] Loading encoder_hidden_states and attention_mask from .emb format
latent_webdataset.py - üìù [rank=0] loading farm_field_puresky.farm_field_puresky from raw samples with raw size 3 bytes
latent_webdataset.py - [rank {rank}] Loading encoder_hidden_states and attention_mask from .emb format
latent_webdataset.py - üìù [rank=0] loading fireplace.fireplace from raw samples with raw size 3 bytes
latent_webdataset.py - [rank {rank}] Loading encoder_hidden_states and attention_mask from .emb format
latent_webdataset.py - üìù [rank=0] loading flower_hillside.flower_hillside from raw samples with raw size 3 bytes
latent_webdataset.py - [rank {rank}] Loading encoder_hidden_states and attention_mask from .emb format
latent_webdataset.py - üìù [rank=0] loading forgotten_miniland.forgotten_miniland from raw samples with raw size 3 bytes
latent_webdataset.py - [rank {rank}] Loading encoder_hidden_states and attention_mask from .emb format
latent_webdataset.py - üìù [rank=0] loading garden_nook.garden_nook from raw samples with raw size 3 bytes
latent_webdataset.py - [rank {rank}] Loading encoder_hidden_states and attention_mask from .emb format
latent_webdataset.py - üìù [rank=0] loading greenwich_park_03.greenwich_park_03 from raw samples with raw size 3 bytes
latent_webdataset.py - [rank {rank}] Loading encoder_hidden_states and attention_mask from .emb format
latent_webdataset.py - üìù [rank=0] loading harties.harties from raw samples with raw size 3 bytes
latent_webdataset.py - [rank {rank}] Loading encoder_hidden_states and attention_mask from .emb format
latent_webdataset.py - üìù [rank=0] loading hausdorf_clear_sky.hausdorf_clear_sky from raw samples with raw size 3 bytes
latent_webdataset.py - [rank {rank}] Loading encoder_hidden_states and attention_mask from .emb format
latent_webdataset.py - üìù [rank=0] loading hotel_rooftop_balcony.hotel_rooftop_balcony from raw samples with raw size 3 bytes
latent_webdataset.py - [rank {rank}] Loading encoder_hidden_states and attention_mask from .emb format
latent_webdataset.py - üìù [rank=0] loading indoor_pool.indoor_pool from raw samples with raw size 3 bytes
latent_webdataset.py - [rank {rank}] Loading encoder_hidden_states and attention_mask from .emb format
latent_webdataset.py - üìù [rank=0] loading kloppenheim_02_puresky.kloppenheim_02_puresky from raw samples with raw size 3 bytes
latent_webdataset.py - [rank {rank}] Loading encoder_hidden_states and attention_mask from .emb format
latent_webdataset.py - üìù [rank=0] loading kloppenheim_05.kloppenheim_05 from raw samples with raw size 3 bytes
latent_webdataset.py - [rank {rank}] Loading encoder_hidden_states and attention_mask from .emb format
latent_webdataset.py - üìù [rank=0] loading kloppenheim_07.kloppenheim_07 from raw samples with raw size 3 bytes
latent_webdataset.py - [rank {rank}] Loading encoder_hidden_states and attention_mask from .emb format
latent_webdataset.py - üìù [rank=0] loading leadenhall_market.leadenhall_market from raw samples with raw size 3 bytes
latent_webdataset.py - [rank {rank}] Loading encoder_hidden_states and attention_mask from .emb format
latent_webdataset.py - üìù [rank=0] loading leibstadt.leibstadt from raw samples with raw size 3 bytes
latent_webdataset.py - [rank {rank}] Loading encoder_hidden_states and attention_mask from .emb format
latent_webdataset.py - üìù [rank=0] loading lenong_3.lenong_3 from raw samples with raw size 3 bytes
latent_webdataset.py - [rank {rank}] Loading encoder_hidden_states and attention_mask from .emb format
latent_webdataset.py - üìù [rank=0] loading lythwood_room.lythwood_room from raw samples with raw size 3 bytes
latent_webdataset.py - [rank {rank}] Loading encoder_hidden_states and attention_mask from .emb format
latent_webdataset.py - üìù [rank=0] loading machine_shop_01.machine_shop_01 from raw samples with raw size 3 bytes
latent_webdataset.py - [rank {rank}] Loading encoder_hidden_states and attention_mask from .emb format
latent_webdataset.py - üìù [rank=0] loading museum_of_history.museum_of_history from raw samples with raw size 3 bytes
latent_webdataset.py - [rank {rank}] Loading encoder_hidden_states and attention_mask from .emb format
latent_webdataset.py - üìù [rank=0] loading music_hall_02.music_hall_02 from raw samples with raw size 3 bytes
latent_webdataset.py - [rank {rank}] Loading encoder_hidden_states and attention_mask from .emb format
latent_webdataset.py - üìù [rank=0] loading orbita.orbita from raw samples with raw size 3 bytes
latent_webdataset.py - [rank {rank}] Loading encoder_hidden_states and attention_mask from .emb format
latent_webdataset.py - üìù [rank=0] loading outdoor_workshop.outdoor_workshop from raw samples with raw size 3 bytes
latent_webdataset.py - [rank {rank}] Loading encoder_hidden_states and attention_mask from .emb format
latent_webdataset.py - üìù [rank=0] loading palermo_sidewalk.palermo_sidewalk from raw samples with raw size 3 bytes
latent_webdataset.py - [rank {rank}] Loading encoder_hidden_states and attention_mask from .emb format
latent_webdataset.py - üìù [rank=0] loading quarry_04_puresky.quarry_04_puresky from raw samples with raw size 3 bytes
latent_webdataset.py - [rank {rank}] Loading encoder_hidden_states and attention_mask from .emb format
latent_webdataset.py - üìù [rank=0] loading resting_place.resting_place from raw samples with raw size 3 bytes
latent_webdataset.py - [rank {rank}] Loading encoder_hidden_states and attention_mask from .emb format
latent_webdataset.py - üìù [rank=0] loading rogland_moonlit_night.rogland_moonlit_night from raw samples with raw size 3 bytes
latent_webdataset.py - [rank {rank}] Loading encoder_hidden_states and attention_mask from .emb format
latent_webdataset.py - üìù [rank=0] loading rooftop_night.rooftop_night from raw samples with raw size 3 bytes
latent_webdataset.py - [rank {rank}] Loading encoder_hidden_states and attention_mask from .emb format
latent_webdataset.py - üìù [rank=0] loading rotes_rathaus.rotes_rathaus from raw samples with raw size 3 bytes
latent_webdataset.py - [rank {rank}] Loading encoder_hidden_states and attention_mask from .emb format
latent_webdataset.py - üìù [rank=0] loading schachen_forest.schachen_forest from raw samples with raw size 3 bytes
latent_webdataset.py - [rank {rank}] Loading encoder_hidden_states and attention_mask from .emb format
latent_webdataset.py - üìù [rank=0] loading shanghai_bund.shanghai_bund from raw samples with raw size 3 bytes
latent_webdataset.py - [rank {rank}] Loading encoder_hidden_states and attention_mask from .emb format
latent_webdataset.py - üìù [rank=0] loading shanghai_riverside.shanghai_riverside from raw samples with raw size 3 bytes
latent_webdataset.py - [rank {rank}] Loading encoder_hidden_states and attention_mask from .emb format
latent_webdataset.py - üìù [rank=0] loading skylit_garage.skylit_garage from raw samples with raw size 3 bytes
latent_webdataset.py - [rank {rank}] Loading encoder_hidden_states and attention_mask from .emb format
latent_webdataset.py - üìù [rank=0] loading small_cathedral_02.small_cathedral_02 from raw samples with raw size 3 bytes
latent_webdataset.py - [rank {rank}] Loading encoder_hidden_states and attention_mask from .emb format
latent_webdataset.py - üìù [rank=0] loading small_workshop.small_workshop from raw samples with raw size 3 bytes
latent_webdataset.py - [rank {rank}] Loading encoder_hidden_states and attention_mask from .emb format
latent_webdataset.py - üìù [rank=0] loading snowy_hillside.snowy_hillside from raw samples with raw size 3 bytes
latent_webdataset.py - [rank {rank}] Loading encoder_hidden_states and attention_mask from .emb format
latent_webdataset.py - üìù [rank=0] loading squash_court.squash_court from raw samples with raw size 3 bytes
latent_webdataset.py - [rank {rank}] Loading encoder_hidden_states and attention_mask from .emb format
latent_webdataset.py - üìù [rank=0] loading stierberg_sunrise.stierberg_sunrise from raw samples with raw size 3 bytes
latent_webdataset.py - [rank {rank}] Loading encoder_hidden_states and attention_mask from .emb format
latent_webdataset.py - üìù [rank=0] loading storeroom.storeroom from raw samples with raw size 3 bytes
latent_webdataset.py - [rank {rank}] Loading encoder_hidden_states and attention_mask from .emb format
latent_webdataset.py - üìù [rank=0] loading straw_rolls_field_01.straw_rolls_field_01 from raw samples with raw size 3 bytes
latent_webdataset.py - [rank {rank}] Loading encoder_hidden_states and attention_mask from .emb format
latent_webdataset.py - üìù [rank=0] loading studio_small_01.studio_small_01 from raw samples with raw size 3 bytes
latent_webdataset.py - [rank {rank}] Loading encoder_hidden_states and attention_mask from .emb format
latent_webdataset.py - üìù [rank=0] loading suburban_field_02.suburban_field_02 from raw samples with raw size 3 bytes
latent_webdataset.py - [rank {rank}] Loading encoder_hidden_states and attention_mask from .emb format
latent_webdataset.py - üìù [rank=0] loading sunflowers.sunflowers from raw samples with raw size 3 bytes
latent_webdataset.py - [rank {rank}] Loading encoder_hidden_states and attention_mask from .emb format
latent_webdataset.py - üìù [rank=0] loading sunset_fairway.sunset_fairway from raw samples with raw size 3 bytes
latent_webdataset.py - [rank {rank}] Loading encoder_hidden_states and attention_mask from .emb format
latent_webdataset.py - üìù [rank=0] loading sunset_forest.sunset_forest from raw samples with raw size 3 bytes
latent_webdataset.py - [rank {rank}] Loading encoder_hidden_states and attention_mask from .emb format
[latent_webdataset] rank 0 ‚Äì is_eval=True ‚Äì  length = 70 (expected ~70)
[latent_webdataset] rank 0 ‚Äì taking slice [0:70] = 70 samples from 70 total
[latent_webdataset] rank 0 ‚Äì DataLoader ready: 70 samples, batch_size=2, ~35 batches
Val dataloader created successfully
Successfully loaded a val sample batch with keys: dict_keys(['latent', 'encoder_hidden_states', 'attention_mask'])
trainer.py - CubeDiffTrainer - build_dataloaders - val_dataloader_list created as 35 batches per rank

Preparing model and dataloader with accelerator
trainer.py - cubedifftrainer - build_datyaloader - Model type after prepare: <class 'torch.nn.parallel.distributed.DistributedDataParallel'>
trainer.py - cubedifftrainer - build_datyaloader - Has save_checkpoint: False
trainer.py - cubedifftrainer - build_datyaloader - Accelerator distributed type: MULTI_GPU
trainer.py - cubedifftrainer - build_datyaloader -  DeepSpeed plugin: None
Moving components to cuda:0; keeping perceptual_net in FP32
Dataloader building completed successfully
debug LR - step  0: 8.000e-05
debug LR - step   1: 0.000e+00
trainer.py - CubeDiffTrainer - train - lr_scheduler created - self.total_steps 1, warmup 0, warmup_ratio 0.03

trainer.py - CubeDiffTrainer - train - sample_prompts is ['A beautiful mountain lake at sunset with snow-capped peaks']

‚ñ∂Ô∏è rank 0 - Starting gstep 0/1
	Rank 0 - gstep 0 - batch_indx 0 - lat shape is torch.Size([2, 6, 4, 64, 64]), txt_emb shape is torch.Size([2, 77, 768]), mask shape is torch.Size([2, 77])
	Rank 0 - gstep 0 - batch_indx 0 - noisy_lat shape is torch.Size([2, 6, 4, 64, 64]), prediction done, cost 3.5313 seconds
	rank0 - gstep 0 - batch_indx 0 - check noise std mean - noise std: 1.0   mean: 0.00262451171875
	rank0 - gstep 0 - batch_indx 0 - check pred std mean  - pred std: 0.22488237917423248   mean: 0.006512405350804329
	Rank 0 - gstep 0 - batch_indx 0 - backward done, cost 5.04 seconds - loss type is <class 'torch.Tensor'> shape is torch.Size([]), loss is 1.0463979244232178 
	Rank 0 - gstep 0 - out of accumulate - batch_indx 0 cost 8.73 seconds, before waiting for all ranks to finish accumulate ...
	Rank 0 - gstep 0 - out of accumulate - batch_indx 0 after waiting for all ranks to finish accumulate, cost 0.00 seconds, real_sample_size_per_rank is 2 samples
‚ñ∂Ô∏è rank 0 - Starting gstep 0/1
	Rank 0 - gstep 0 - batch_indx 1 - lat shape is torch.Size([2, 6, 4, 64, 64]), txt_emb shape is torch.Size([2, 77, 768]), mask shape is torch.Size([2, 77])
	Rank 0 - gstep 0 - batch_indx 1 - noisy_lat shape is torch.Size([2, 6, 4, 64, 64]), prediction done, cost 0.4115 seconds
	rank0 - gstep 0 - batch_indx 1 - check noise std mean - noise std: 1.0   mean: -0.00015163421630859375
	rank0 - gstep 0 - batch_indx 1 - check pred std mean  - pred std: 0.20430564880371094   mean: 0.0033638670574873686
	Rank 0 - gstep 0 - batch_indx 1 - backward done, cost 2.00 seconds - loss type is <class 'torch.Tensor'> shape is torch.Size([]), loss is 1.0420308113098145 
	Rank 0 - gstep 0 - out of accumulate - batch_indx 1 cost 2.79 seconds, before waiting for all ranks to finish accumulate ...
	Rank 0 - gstep 0 - out of accumulate - batch_indx 1 after waiting for all ranks to finish accumulate, cost 0.00 seconds, real_sample_size_per_rank is 4 samples
‚ñ∂Ô∏è rank 0 - Starting gstep 0/1
	Rank 0 - gstep 0 - batch_indx 2 - lat shape is torch.Size([2, 6, 4, 64, 64]), txt_emb shape is torch.Size([2, 77, 768]), mask shape is torch.Size([2, 77])
	Rank 0 - gstep 0 - batch_indx 2 - noisy_lat shape is torch.Size([2, 6, 4, 64, 64]), prediction done, cost 0.2929 seconds
	rank0 - gstep 0 - batch_indx 2 - check noise std mean - noise std: 1.0   mean: -0.0011138916015625
	rank0 - gstep 0 - batch_indx 2 - check pred std mean  - pred std: 0.22058753669261932   mean: 0.0052236756309866905
	Rank 0 - gstep 0 - batch_indx 2 - backward done, cost 2.02 seconds - loss type is <class 'torch.Tensor'> shape is torch.Size([]), loss is 1.0453124046325684 
	Rank 0 - gstep 0 - out of accumulate - batch_indx 2 cost 2.80 seconds, before waiting for all ranks to finish accumulate ...
	Rank 0 - gstep 0 - out of accumulate - batch_indx 2 after waiting for all ranks to finish accumulate, cost 0.00 seconds, real_sample_size_per_rank is 6 samples
‚ñ∂Ô∏è rank 0 - Starting gstep 0/1
	Rank 0 - gstep 0 - batch_indx 3 - lat shape is torch.Size([2, 6, 4, 64, 64]), txt_emb shape is torch.Size([2, 77, 768]), mask shape is torch.Size([2, 77])
	Rank 0 - gstep 0 - batch_indx 3 - noisy_lat shape is torch.Size([2, 6, 4, 64, 64]), prediction done, cost 0.2924 seconds
	rank0 - gstep 0 - batch_indx 3 - check noise std mean - noise std: 1.0   mean: 0.001190185546875
	rank0 - gstep 0 - batch_indx 3 - check pred std mean  - pred std: 0.22949916124343872   mean: -0.0015452839434146881
	Rank 0 - gstep 0 - batch_indx 3 - backward done, cost 2.02 seconds - loss type is <class 'torch.Tensor'> shape is torch.Size([]), loss is 1.0480539798736572 
trainer.py - train - [rank 0] ‚Üí Using standard checkpoint (not DeepSpeed): <class 'model.architecture.CubeDiffModel'>, Standard checkpoint saved to: outputs/cubediff_tiny_lora/ds_ckpt/deepspeed_ckpt_step1
[rank 0] ‚Üí Checkpoint saved, cost 11.3110 seconds
	Rank 0 - gstep 1 - batch_indx 3 - global_loss done, cost 0.0010 seconds, global_loss shape is torch.Size([])
lr and sample and epoch progress: rank 0 ‚Äî gstep 1/1 ‚ñ∂ 100.0% updates | Samples ‚ñ∂ 100.0% (8/8) | Equivalent Epochs seen ‚ñ∂ 0.01 LR at step 1: 0.000e+00 total_samples = 8
Rank 0 - for current udpate step - gstep 1 - processed batch 3 - real_sample_size_per_rank 8 samples done, cost 14.3988 seconds

	Rank 0 - gstep 1 - out of accumulate - batch_indx 3 cost 14.40 seconds, before waiting for all ranks to finish accumulate ...
	Rank 0 - gstep 1 - out of accumulate - batch_indx 3 after waiting for all ranks to finish accumulate, cost 0.00 seconds, real_sample_size_per_rank is 8 samples
	rank 0 ‚Üí gstep 1 - generate_samples (total_processed_samples=8, prompts=['A beautiful mountain lake at sunset with snow-capped peaks'])
		[rank 0] loading ZeRO‚Äêsharded checkpoint from outputs/cubediff_tiny_lora/ds_ckpt/deepspeed_ckpt_step1 ...
		trainer.py - generate_sampes - [rank 0] Loading standard checkpoint...
		trainer.py - generate_sampes - [rank 0] Standard checkpoint loaded successfully
		[rank 0] >>>> Scheduler class: <class 'diffusers.schedulers.scheduling_ddpm.DDPMScheduler'>
		[rank 0] >>>> prediction_type: 'epsilon'
		[rank 0] >>>> num_train_timesteps: 1000
		[rank 0] ZeRO load_state completed in 2.665s
pipeline.py - CubeDiffPipeline - generate - Rotated face 1 by 270¬∞ CCW, dec shape is torch.Size([3, 512, 512]), dec dtype is torch.bfloat16
pipeline.py - CubeDiffPipeline - generate - stacking faces and projecting to equirectangular...
pipeline.py - CubeDiffPipeline - generate - each face shape: torch.Size([3, 512, 512]), num faces: 6
pipeline.py - generate - cube_np shape: (6, 3, 512, 512), dtype: float32, min: 0.0, max: 1.0
pipeline.py - generate - after cube_np.permute(0, 2, 3, 1), cube_np shape: (6, 512, 512, 3), dtype: float32, min: 0.0, max: 1.0
pipeline.py - generate - self.height: 512, self.width: 512
pipeline.py - CubeDiffPipeline - generate - pano shape: torch.Size([512, 3072, 3])
		[rank 0] denoised in 43.692s
		[rank 0], trainer.py, pano shape = torch.Size([512, 3072, 3]) dtype= torch.float32
		trainer.py - [rank 0] - gstep 1 - generate_samples - total_processed_samples: 8, CLIP sim: 0.1751
		[rank 0] saved panorama ‚Üí outputs/cubediff_tiny_lora/samples/gstep1_samples8.png
	rank 0 ‚Üí generate_samples finished (wall-clock ~46.912s)
[rank 0] ‚ûñ found 1 checkpoints in outputs/cubediff_tiny_lora/ds_ckpt, current_ckpt is deepspeed_ckpt_step1
[rank 0] ‚ûñ checking checkpoint folder: deepspeed_ckpt_step1, current_ckpt is deepspeed_ckpt_step1
[rank 0] ‚úÖ Checkpoint cleanup completed: kept deepspeed_ckpt_step1, deleted 0 old checkpoints
trainer.py - [rank 0] ‚Üí generate_samples completed successfully with checkpoint management
	Rank 0 - gstep 1 - batch_indx 3 - generate_samples done - cost 46.91 seconds
	rank 0 - gstep 1 trainer.py - evaluate() - Determined local_batches = 35 (actual DataLoader length)
rank 0 ‚Äì evaluate - before accelerator.wait_for_everyone
rank 0 ‚Äì evaluate - after accelerator.wait_for_everyone, cost 0.0007 seconds
rank 0 ‚Äì evaluate - calling accelerator.gather(batch_counts=[35]
rank 0 ‚Äì evaluate - after accelerator.gather, cost 0.0008 seconds, ‚Üí gathered = [35]
	rank 0 - gstep 1 trainer.py - evaluate() - config: val_size_total=70, world_size=1, batch_size=2, local_batches=35, max_batches=35

	rank 0 - gstep 1 trainer.py - evaluate() - Looping over max_batches=35 steps

	rank 0 - gstep 1 - evaluate() - in loop - batch_idx 0, batch_loss=1.0476, num_samples=2, cost 0.4709 seconds

	rank 0 - gstep 1 - evaluate() - in loop - batch_idx 1, batch_loss=1.0478, num_samples=2, cost 0.5378 seconds

	rank 0 - gstep 1 - evaluate() - in loop - batch_idx 2, batch_loss=1.0420, num_samples=2, cost 0.2897 seconds

	rank 0 - gstep 1 - evaluate() - in loop - batch_idx 3, batch_loss=1.0442, num_samples=2, cost 0.2885 seconds

	rank 0 - gstep 1 - evaluate() - in loop - batch_idx 4, batch_loss=1.0426, num_samples=2, cost 0.2923 seconds

	rank 0 - gstep 1 - evaluate() - in loop - batch_idx 5, batch_loss=1.0457, num_samples=2, cost 0.4302 seconds

	rank 0 - gstep 1 - evaluate() - in loop - batch_idx 6, batch_loss=1.0440, num_samples=2, cost 0.2935 seconds

	rank 0 - gstep 1 - evaluate() - in loop - batch_idx 7, batch_loss=1.0391, num_samples=2, cost 0.2881 seconds

	rank 0 - gstep 1 - evaluate() - in loop - batch_idx 8, batch_loss=1.0405, num_samples=2, cost 0.2904 seconds

	rank 0 - gstep 1 - evaluate() - in loop - batch_idx 9, batch_loss=1.0432, num_samples=2, cost 0.2933 seconds

	rank 0 - gstep 1 - evaluate() - in loop - batch_idx 10, batch_loss=1.0394, num_samples=2, cost 0.2925 seconds

	rank 0 - gstep 1 - evaluate() - in loop - batch_idx 11, batch_loss=1.0354, num_samples=2, cost 0.2928 seconds

	rank 0 - gstep 1 - evaluate() - in loop - batch_idx 12, batch_loss=1.0418, num_samples=2, cost 0.2895 seconds

	rank 0 - gstep 1 - evaluate() - in loop - batch_idx 13, batch_loss=1.0527, num_samples=2, cost 0.2892 seconds

	rank 0 - gstep 1 - evaluate() - in loop - batch_idx 14, batch_loss=1.0441, num_samples=2, cost 0.2930 seconds

	rank 0 - gstep 1 - evaluate() - in loop - batch_idx 15, batch_loss=1.0419, num_samples=2, cost 0.2933 seconds

	rank 0 - gstep 1 - evaluate() - in loop - batch_idx 16, batch_loss=1.0435, num_samples=2, cost 0.2910 seconds

	rank 0 - gstep 1 - evaluate() - in loop - batch_idx 17, batch_loss=1.0461, num_samples=2, cost 0.2898 seconds

	rank 0 - gstep 1 - evaluate() - in loop - batch_idx 18, batch_loss=1.0432, num_samples=2, cost 0.2900 seconds

	rank 0 - gstep 1 - evaluate() - in loop - batch_idx 19, batch_loss=1.0461, num_samples=2, cost 0.2927 seconds

	rank 0 - gstep 1 - evaluate() - in loop - batch_idx 20, batch_loss=1.0353, num_samples=2, cost 0.2915 seconds

	rank 0 - gstep 1 - evaluate() - in loop - batch_idx 21, batch_loss=1.0397, num_samples=2, cost 0.2891 seconds

	rank 0 - gstep 1 - evaluate() - in loop - batch_idx 22, batch_loss=1.0420, num_samples=2, cost 0.2899 seconds

	rank 0 - gstep 1 - evaluate() - in loop - batch_idx 23, batch_loss=1.0489, num_samples=2, cost 0.2914 seconds

	rank 0 - gstep 1 - evaluate() - in loop - batch_idx 24, batch_loss=1.0436, num_samples=2, cost 0.2906 seconds

	rank 0 - gstep 1 - evaluate() - in loop - batch_idx 25, batch_loss=1.0438, num_samples=2, cost 0.2900 seconds

	rank 0 - gstep 1 - evaluate() - in loop - batch_idx 26, batch_loss=1.0482, num_samples=2, cost 0.2896 seconds

	rank 0 - gstep 1 - evaluate() - in loop - batch_idx 27, batch_loss=1.0481, num_samples=2, cost 0.2917 seconds

	rank 0 - gstep 1 - evaluate() - in loop - batch_idx 28, batch_loss=1.0401, num_samples=2, cost 0.2897 seconds

	rank 0 - gstep 1 - evaluate() - in loop - batch_idx 29, batch_loss=1.0470, num_samples=2, cost 0.2869 seconds

	rank 0 - gstep 1 - evaluate() - in loop - batch_idx 30, batch_loss=1.0419, num_samples=2, cost 0.2913 seconds

	rank 0 - gstep 1 - evaluate() - in loop - batch_idx 31, batch_loss=1.0452, num_samples=2, cost 0.2914 seconds

	rank 0 - gstep 1 - evaluate() - in loop - batch_idx 32, batch_loss=1.0491, num_samples=2, cost 0.2901 seconds

	rank 0 - gstep 1 - evaluate() - in loop - batch_idx 33, batch_loss=1.0422, num_samples=2, cost 0.2912 seconds

	rank 0 - gstep 1 - evaluate() - in loop - batch_idx 34, batch_loss=1.0439, num_samples=2, cost 0.2905 seconds

	rank 0 - gstep 1 - evaluate() - all batches done - Finished looping over max_batches=35 cost 27.4814 seconds

	rank 0 ‚Äì evaluate - calling accelerator.reduce(pack=[73.05892944335938, 70])
	rank 0 ‚Äì evaluate - after accelerator.reduce ‚Üí reduced_pack = [73.05892944335938, 70.0], cost 0.0008 seconds

	Rank 0 - gstep 1 - evaluate done, cost 27.4942 seconds, val_loss is 1.0437
‚úî _plot_loss_curves saved loss curve ‚Üí outputs/cubediff_tiny_lora/loss_curve_up_to_total_processed_samples.png
Rank 0 - gstep 1 - out of the training loop - total_processed_samples is 8 -  all updates steps done, cost 104.6584 seconds


Rank 0 - trainer.py - CubeDiffTrainer - train - move everything to CPU and full precision - ‚úî saved U-Net adapter weights to outputs/cubediff_tiny_lora/final_unet_adapter_model_gstep1.bin
‚úî loss curves saved to outputs/cubediff_tiny_lora/loss_curve_gstep1_total_processed_ds8.png
