{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1ab72c2-bf81-4c1a-980c-569e40b88a77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2025-6-28 run it on \"python 3\" base env on ins-gl-pt-gpu24-dea5707-gpuprof-3env-j4-1b with L4/A100 GPU\n",
    "# on personal, test access model in cache\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fea48c05-73db-4cda-a525-1f572e1d6925",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 4.0K\n",
      "drwxr-xr-x 2 root root 4.0K Jun 28 21:17 32bd64288804d66eefd0ccbe215aa642df71cc41\n"
     ]
    }
   ],
   "source": [
    "!ls  -lh /home/jupyter/.cache/huggingface/hub/models--openai--clip-vit-large-patch14/snapshots/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a6515c27-5969-4566-a0f9-6cc18b63f4f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 1.6G\n",
      "-rw-r--r-- 1 root root 4.5K Jun 28 21:16 config.json\n",
      "-rw-r--r-- 1 root root 513K Jun 28 21:16 merges.txt\n",
      "-rw-r--r-- 1 root root 1.6G Jun 28 21:17 model.safetensors\n",
      "-rw-r--r-- 1 root root  389 Jun 28 21:16 special_tokens_map.json\n",
      "-rw-r--r-- 1 root root 2.2M Jun 28 21:16 tokenizer.json\n",
      "-rw-r--r-- 1 root root  905 Jun 28 21:16 tokenizer_config.json\n",
      "-rw-r--r-- 1 root root 939K Jun 28 21:16 vocab.json\n"
     ]
    }
   ],
   "source": [
    "!ls  -lh /home/jupyter/.cache/huggingface/hub/models--openai--clip-vit-large-patch14/snapshots/32bd64288804d66eefd0ccbe215aa642df71cc41"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "133f610f-0bea-4a9f-83d4-e304886e57d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading CLIP tokenizer from cached path...\n",
      "Loading CLIP tokenizer done\n",
      "Loading CLIP text encoder from cached path...\n",
      "Loading CLIP text encoder done\n"
     ]
    }
   ],
   "source": [
    "from transformers import CLIPTokenizer, CLIPTextModel\n",
    "import torch\n",
    "# Load tokenizer directly from cached path\n",
    "cached_path = \"/home/jupyter/.cache/huggingface/hub/models--openai--clip-vit-large-patch14/snapshots/32bd64288804d66eefd0ccbe215aa642df71cc41\"\n",
    "\n",
    "print(\"Loading CLIP tokenizer from cached path...\")\n",
    "tokenizer = CLIPTokenizer.from_pretrained(cached_path, local_files_only=True)\n",
    "print(\"Loading CLIP tokenizer done\")\n",
    "\n",
    "print(\"Loading CLIP text encoder from cached path...\")\n",
    "text_encoder = CLIPTextModel.from_pretrained(\n",
    "    cached_path,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    local_files_only=True,\n",
    ").to(\"cuda\").eval()\n",
    "print(\"Loading CLIP text encoder done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b44534c4-b587-4584-9705-ec8694f09756",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2025-7-6 run it on \"python 3\" base env on ins-gl-pt-gpu24-dea5707-gpuprof-3env-j4-1b with L4/A100 GPU\n",
    "# on personal, test access model (CLIPModel)  in cache\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fda2afa4-4987-4e94-9e32-5a1c551aca34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 4.0K\n",
      "drwxr-xr-x 2 root root 4.0K Jun 28 21:17 32bd64288804d66eefd0ccbe215aa642df71cc41\n"
     ]
    }
   ],
   "source": [
    "!ls  -lh /home/jupyter/.cache/huggingface/hub/models--openai--clip-vit-large-patch14/snapshots/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "13b39163-42de-4e37-8174-a15cff143149",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 1.6G\n",
      "-rw-r--r-- 1 root root 4.5K Jun 28 21:16 config.json\n",
      "-rw-r--r-- 1 root root 513K Jun 28 21:16 merges.txt\n",
      "-rw-r--r-- 1 root root 1.6G Jun 28 21:17 model.safetensors\n",
      "-rw-r--r-- 1 root root  389 Jun 28 21:16 special_tokens_map.json\n",
      "-rw-r--r-- 1 root root 2.2M Jun 28 21:16 tokenizer.json\n",
      "-rw-r--r-- 1 root root  905 Jun 28 21:16 tokenizer_config.json\n",
      "-rw-r--r-- 1 root root 939K Jun 28 21:16 vocab.json\n"
     ]
    }
   ],
   "source": [
    "!ls  -lh /home/jupyter/.cache/huggingface/hub/models--openai--clip-vit-large-patch14/snapshots/32bd64288804d66eefd0ccbe215aa642df71cc41"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "591309d3-4680-4b17-a2ba-b45f09dd939d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading CLIP tokenizer from cached path...\n",
      "Loading CLIP tokenizer done\n",
      "Loading CLIP text encoder from cached path...\n",
      "Loading CLIP text encoder done\n"
     ]
    }
   ],
   "source": [
    "from transformers import CLIPTokenizer, CLIPTextModel\n",
    "import torch\n",
    "# Load tokenizer directly from cached path\n",
    "cached_path = \"/home/jupyter/.cache/huggingface/hub/models--openai--clip-vit-large-patch14/snapshots/32bd64288804d66eefd0ccbe215aa642df71cc41\"\n",
    "\n",
    "print(\"Loading CLIP tokenizer from cached path...\")\n",
    "tokenizer = CLIPTokenizer.from_pretrained(cached_path, local_files_only=True)\n",
    "print(\"Loading CLIP tokenizer done\")\n",
    "\n",
    "print(\"Loading CLIP text encoder from cached path...\")\n",
    "text_encoder = CLIPTextModel.from_pretrained(\n",
    "    cached_path,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    local_files_only=True,\n",
    ").to(\"cuda\").eval()\n",
    "print(\"Loading CLIP text encoder done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "42ba187d-6cfd-48ce-8a08-a225085373de",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import CLIPModel\n",
    "# self.clip_model = CLIPModel.from_pretrained(\n",
    "#                 \"openai/clip-vit-base-patch32\",\n",
    "#                 use_safetensors=True,\n",
    "#                 local_files_only=True\n",
    "#             )\n",
    "\n",
    "clip_model = CLIPModel.from_pretrained(\n",
    "                cached_path,\n",
    "                use_safetensors=True,\n",
    "                local_files_only=True\n",
    "            )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bf6d0314-bc11-4882-aa5a-8bf447456aee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "transformers.models.clip.modeling_clip.CLIPModel"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(clip_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0f621f24-603e-42f3-b162-377d751f5a6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 20K\n",
      "drwxr-xr-x 5 root root 4.0K Jul  6 07:02 models--openai--clip-vit-base-patch32\n",
      "drwxr-xr-x 6 root root 4.0K Jun 28 21:16 models--openai--clip-vit-large-patch14\n",
      "drwxr-xr-x 6 root root 4.0K Jun  9 06:28 models--runwayml--stable-diffusion-v1-5\n",
      "-rw-r--r-- 1 root root    1 Jun  9 06:28 version.txt\n",
      "-rw-r--r-- 1 root root    1 Jun  9 06:28 version_diffusers_cache.txt\n"
     ]
    }
   ],
   "source": [
    "!ls -lh ~/.cache/huggingface/hub/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "beb92495-57ee-4d13-8b8c-80c73bce6345",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 12K\n",
      "drwxr-xr-x 2 root root 4.0K Jun 28 21:17 blobs\n",
      "drwxr-xr-x 2 root root 4.0K Jun 28 21:16 refs\n",
      "drwxr-xr-x 3 root root 4.0K Jun 28 16:08 snapshots\n"
     ]
    }
   ],
   "source": [
    "!ls -lh ~/.cache/huggingface/hub/models--openai--clip-vit-large-patch14"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6b43dcf4-ab7a-4b34-aa0f-71d095cf2697",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 4.0K\n",
      "drwxr-xr-x 2 root root 4.0K Jun 28 21:17 32bd64288804d66eefd0ccbe215aa642df71cc41\n"
     ]
    }
   ],
   "source": [
    "!ls -lh ~/.cache/huggingface/hub/models--openai--clip-vit-large-patch14/snapshots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "649bdf9f-429b-4c81-8fd7-458748f0cb96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 12K\n",
      "drwxr-xr-x 2 root root 4.0K Jul  6 07:03 blobs\n",
      "drwxr-xr-x 3 root root 4.0K Jul  6 07:02 refs\n",
      "drwxr-xr-x 4 root root 4.0K Jul  6 07:02 snapshots\n"
     ]
    }
   ],
   "source": [
    "!ls -lh ~/.cache/huggingface/hub/models--openai--clip-vit-base-patch32/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5f17457a-3630-4b9b-9321-6d45119395a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 8.0K\n",
      "drwxr-xr-x 2 root root 4.0K Jul  6 07:03 37000f5cdccec47a3f6f83142bff131370757470\n",
      "drwxr-xr-x 2 root root 4.0K Jul  6 07:03 3d74acf9a28c67741b2f4f2ea7635f0aaf6f0268\n"
     ]
    }
   ],
   "source": [
    "!ls -lh ~/.cache/huggingface/hub/models--openai--clip-vit-base-patch32/snapshots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "60f749e9-3eb1-40bd-99d6-1fd09cfeeed2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 578M\n",
      "-rw-r--r-- 1 root root 578M Jul  6 07:03 model.safetensors\n"
     ]
    }
   ],
   "source": [
    "!ls -lh ~/.cache/huggingface/hub/models--openai--clip-vit-base-patch32/snapshots/37000f5cdccec47a3f6f83142bff131370757470"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1ee90b80-7ea0-4b8c-9710-f0f2de14faef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 581M\n",
      "-rw-r--r-- 1 root root 4.1K Jul  6 07:02 config.json\n",
      "-rw-r--r-- 1 root root 513K Jul  6 07:02 merges.txt\n",
      "-rw-r--r-- 1 root root  316 Jul  6 07:02 preprocessor_config.json\n",
      "-rw-r--r-- 1 root root 578M Jul  6 07:03 pytorch_model.bin\n",
      "-rw-r--r-- 1 root root  389 Jul  6 07:02 special_tokens_map.json\n",
      "-rw-r--r-- 1 root root 2.2M Jul  6 07:02 tokenizer.json\n",
      "-rw-r--r-- 1 root root  592 Jul  6 07:02 tokenizer_config.json\n",
      "-rw-r--r-- 1 root root 843K Jul  6 07:02 vocab.json\n"
     ]
    }
   ],
   "source": [
    "!ls -lh ~/.cache/huggingface/hub/models--openai--clip-vit-base-patch32/snapshots/3d74acf9a28c67741b2f4f2ea7635f0aaf6f0268"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4c1da879-8cac-4a88-a3f6-ae6812d67ada",
   "metadata": {},
   "outputs": [],
   "source": [
    "cached_path = \"/home/jupyter/.cache/huggingface/hub/models--openai--clip-vit-base-patch32/snapshots/37000f5cdccec47a3f6f83142bff131370757470\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "dac01b1d-0721-4e15-81b0-c4a8b8972b0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "clip_model = CLIPModel.from_pretrained(\n",
    "                cached_path,\n",
    "                use_safetensors=True,\n",
    "                local_files_only=True\n",
    "            )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "642eb97f-e339-4ed5-b8af-f123599d4312",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "transformers.models.clip.modeling_clip.CLIPModel"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(clip_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "1cf7029b-0158-46d8-93d5-3763b7706e52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current ref: 3d74acf9a28c67741b2f4f2ea7635f0aaf6f0268\n"
     ]
    }
   ],
   "source": [
    "refs_path = \"/home/jupyter/.cache/huggingface/hub/models--openai--clip-vit-base-patch32/refs/main\"\n",
    "try:\n",
    "    with open(refs_path, 'r') as f:\n",
    "        current_ref = f.read().strip()\n",
    "    print(f\"Current ref: {current_ref}\")\n",
    "except:\n",
    "    print(\"refs/main file not found or corrupted\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "af0a31b9-1be5-452c-bebe-a6d4b1902a72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-rw-r--r-- 1 root root 40 Jul  6 07:02 /home/jupyter/.cache/huggingface/hub/models--openai--clip-vit-base-patch32/refs/main\n"
     ]
    }
   ],
   "source": [
    "!ls -lh /home/jupyter/.cache/huggingface/hub/models--openai--clip-vit-base-patch32/refs/main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "4caf3af0-a33f-466d-ac67-45fe5ad4d65b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3d74acf9a28c67741b2f4f2ea7635f0aaf6f0268"
     ]
    }
   ],
   "source": [
    "!cat /home/jupyter/.cache/huggingface/hub/models--openai--clip-vit-base-patch32/refs/main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "e718f585-4e0a-445a-bd75-305e9a12af88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 1.2G\n",
      "-rw-r--r-- 1 root root 843K Jul  6 07:02 182766ce89b439768edadda342519f33802f5364\n",
      "-rw-r--r-- 1 root root  592 Jul  6 07:02 4fdaf6842dd5a725b940c92b6b692490ce59d548\n",
      "-rw-r--r-- 1 root root 2.2M Jul  6 07:02 564c0ebd5ce29c4ee4864004aee693deadd3128c\n",
      "-rw-r--r-- 1 root root  316 Jul  6 07:02 5a12a1eb250987a4eee0e3e7d7338c4b22724be1\n",
      "-rw-r--r-- 1 root root 578M Jul  6 07:03 99d28a652e6ec46629ab7047a0ac82c69b1fe11e0ce672c43af65d3a9a3fc05d\n",
      "-rw-r--r-- 1 root root  389 Jul  6 07:02 9bfb42aa97dcd61e89f279ccaee988bccb4fabae\n",
      "-rw-r--r-- 1 root root 4.1K Jul  6 07:02 a2a88b96561196777ca173b15309ea859f4d2ce0\n",
      "-rw-r--r-- 1 root root 578M Jul  6 07:03 a63082132ba4f97a80bea76823f544493bffa8082296d62d71581a4feff1576f\n",
      "-rw-r--r-- 1 root root 513K Jul  6 07:02 bbfec752c9a675946c6dce106def6f35c882dcc2\n"
     ]
    }
   ],
   "source": [
    "!ls -lh /home/jupyter/.cache/huggingface/hub/models--openai--clip-vit-base-patch32/blobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "7b1bc1a7-fb4c-40ba-8d7f-cb9731eb3620",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-rw-r--r-- 1 root root 578M Jul  6 07:03 /home/jupyter/.cache/huggingface/hub/models--openai--clip-vit-base-patch32/blobs/99d28a652e6ec46629ab7047a0ac82c69b1fe11e0ce672c43af65d3a9a3fc05d\n"
     ]
    }
   ],
   "source": [
    "!ls -lh /home/jupyter/.cache/huggingface/hub/models--openai--clip-vit-base-patch32/blobs/99d28a652e6ec46629ab7047a0ac82c69b1fe11e0ce672c43af65d3a9a3fc05d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c5eaf756-3ec0-4e47-953a-4eac1a7ece98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fix refs/main to point to the correct snapshot\n",
    "with open(refs_path, 'w') as f:\n",
    "    f.write(\"37000f5cdccec47a3f6f83142bff131370757470\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b5a2a54b-7cf5-47f6-adb8-f713fb694e8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "37000f5cdccec47a3f6f83142bff131370757470"
     ]
    }
   ],
   "source": [
    "!cat /home/jupyter/.cache/huggingface/hub/models--openai--clip-vit-base-patch32/refs/main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "e27ed6a0-9b86-436b-aa16-6d2af689bd4a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "We couldn't connect to 'https://huggingface.co' to load the files, and couldn't find them in the cached files.\nCheck your internet connection or see how to run the library in offline mode at 'https://huggingface.co/docs/transformers/installation#offline-mode'.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mLocalEntryNotFoundError\u001b[0m                   Traceback (most recent call last)",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/utils/hub.py:470\u001b[0m, in \u001b[0;36mcached_files\u001b[0;34m(path_or_repo_id, filenames, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, subfolder, repo_type, user_agent, _raise_exceptions_for_gated_repo, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash, **deprecated_kwargs)\u001b[0m\n\u001b[1;32m    468\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(full_filenames) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m    469\u001b[0m     \u001b[38;5;66;03m# This is slightly better for only 1 file\u001b[39;00m\n\u001b[0;32m--> 470\u001b[0m     \u001b[43mhf_hub_download\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    471\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpath_or_repo_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    472\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfilenames\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    473\u001b[0m \u001b[43m        \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43msubfolder\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    474\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrepo_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrepo_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    475\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    476\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    477\u001b[0m \u001b[43m        \u001b[49m\u001b[43muser_agent\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muser_agent\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    478\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    479\u001b[0m \u001b[43m        \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    480\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    481\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    482\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    483\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    484\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py:114\u001b[0m, in \u001b[0;36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m     kwargs \u001b[38;5;241m=\u001b[39m smoothly_deprecate_use_auth_token(fn_name\u001b[38;5;241m=\u001b[39mfn\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, has_token\u001b[38;5;241m=\u001b[39mhas_token, kwargs\u001b[38;5;241m=\u001b[39mkwargs)\n\u001b[0;32m--> 114\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/huggingface_hub/file_download.py:1008\u001b[0m, in \u001b[0;36mhf_hub_download\u001b[0;34m(repo_id, filename, subfolder, repo_type, revision, library_name, library_version, cache_dir, local_dir, user_agent, force_download, proxies, etag_timeout, token, local_files_only, headers, endpoint, resume_download, force_filename, local_dir_use_symlinks)\u001b[0m\n\u001b[1;32m   1007\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1008\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_hf_hub_download_to_cache_dir\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1009\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Destination\u001b[39;49;00m\n\u001b[1;32m   1010\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1011\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# File info\u001b[39;49;00m\n\u001b[1;32m   1012\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrepo_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrepo_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1013\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfilename\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1014\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrepo_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrepo_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1015\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1016\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# HTTP info\u001b[39;49;00m\n\u001b[1;32m   1017\u001b[0m \u001b[43m        \u001b[49m\u001b[43mendpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mendpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1018\u001b[0m \u001b[43m        \u001b[49m\u001b[43metag_timeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43metag_timeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1019\u001b[0m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhf_headers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1020\u001b[0m \u001b[43m        \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1021\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1022\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Additional options\u001b[39;49;00m\n\u001b[1;32m   1023\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1024\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1025\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/huggingface_hub/file_download.py:1115\u001b[0m, in \u001b[0;36m_hf_hub_download_to_cache_dir\u001b[0;34m(cache_dir, repo_id, filename, repo_type, revision, endpoint, etag_timeout, headers, proxies, token, local_files_only, force_download)\u001b[0m\n\u001b[1;32m   1114\u001b[0m     \u001b[38;5;66;03m# Otherwise, raise appropriate error\u001b[39;00m\n\u001b[0;32m-> 1115\u001b[0m     \u001b[43m_raise_on_head_call_error\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhead_call_error\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1117\u001b[0m \u001b[38;5;66;03m# From now on, etag, commit_hash, url and size are not None.\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/huggingface_hub/file_download.py:1636\u001b[0m, in \u001b[0;36m_raise_on_head_call_error\u001b[0;34m(head_call_error, force_download, local_files_only)\u001b[0m\n\u001b[1;32m   1635\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m local_files_only:\n\u001b[0;32m-> 1636\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m LocalEntryNotFoundError(\n\u001b[1;32m   1637\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot find the requested files in the disk cache and outgoing traffic has been disabled. To enable\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1638\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m hf.co look-ups and downloads online, set \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlocal_files_only\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m to False.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1639\u001b[0m     )\n\u001b[1;32m   1640\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(head_call_error, (RepositoryNotFoundError, GatedRepoError)) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[1;32m   1641\u001b[0m     \u001b[38;5;28misinstance\u001b[39m(head_call_error, HfHubHTTPError) \u001b[38;5;129;01mand\u001b[39;00m head_call_error\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mstatus_code \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m401\u001b[39m\n\u001b[1;32m   1642\u001b[0m ):\n\u001b[1;32m   1643\u001b[0m     \u001b[38;5;66;03m# Repo not found or gated => let's raise the actual error\u001b[39;00m\n\u001b[1;32m   1644\u001b[0m     \u001b[38;5;66;03m# Unauthorized => likely a token issue => let's raise the actual error\u001b[39;00m\n",
      "\u001b[0;31mLocalEntryNotFoundError\u001b[0m: Cannot find the requested files in the disk cache and outgoing traffic has been disabled. To enable hf.co look-ups and downloads online, set 'local_files_only' to False.",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[31], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m clip_model \u001b[38;5;241m=\u001b[39m \u001b[43mCLIPModel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mopenai/clip-vit-base-patch32\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m                \u001b[49m\u001b[43muse_safetensors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m                \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\n\u001b[1;32m      5\u001b[0m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/modeling_utils.py:311\u001b[0m, in \u001b[0;36mrestore_default_torch_dtype.<locals>._wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    309\u001b[0m old_dtype \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mget_default_dtype()\n\u001b[1;32m    310\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 311\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    312\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    313\u001b[0m     torch\u001b[38;5;241m.\u001b[39mset_default_dtype(old_dtype)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/modeling_utils.py:4583\u001b[0m, in \u001b[0;36mPreTrainedModel.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, weights_only, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m   4581\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(config, PretrainedConfig):\n\u001b[1;32m   4582\u001b[0m     config_path \u001b[38;5;241m=\u001b[39m config \u001b[38;5;28;01mif\u001b[39;00m config \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m pretrained_model_name_or_path\n\u001b[0;32m-> 4583\u001b[0m     config, model_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconfig_class\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   4584\u001b[0m \u001b[43m        \u001b[49m\u001b[43mconfig_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4585\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4586\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_unused_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   4587\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4588\u001b[0m \u001b[43m        \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4589\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4590\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4591\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4592\u001b[0m \u001b[43m        \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msubfolder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4593\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgguf_file\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgguf_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4594\u001b[0m \u001b[43m        \u001b[49m\u001b[43m_from_auto\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfrom_auto_class\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4595\u001b[0m \u001b[43m        \u001b[49m\u001b[43m_from_pipeline\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfrom_pipeline\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4596\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4597\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   4598\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgguf_file\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m model_kwargs:\n\u001b[1;32m   4599\u001b[0m         model_kwargs\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgguf_file\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/configuration_utils.py:568\u001b[0m, in \u001b[0;36mPretrainedConfig.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, cache_dir, force_download, local_files_only, token, revision, **kwargs)\u001b[0m\n\u001b[1;32m    564\u001b[0m kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrevision\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m revision\n\u001b[1;32m    566\u001b[0m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_set_token_in_kwargs(kwargs, token)\n\u001b[0;32m--> 568\u001b[0m config_dict, kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_config_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    569\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39mbase_config_key \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39mbase_config_key \u001b[38;5;129;01min\u001b[39;00m config_dict:\n\u001b[1;32m    570\u001b[0m     config_dict \u001b[38;5;241m=\u001b[39m config_dict[\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39mbase_config_key]\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/configuration_utils.py:608\u001b[0m, in \u001b[0;36mPretrainedConfig.get_config_dict\u001b[0;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n\u001b[1;32m    606\u001b[0m original_kwargs \u001b[38;5;241m=\u001b[39m copy\u001b[38;5;241m.\u001b[39mdeepcopy(kwargs)\n\u001b[1;32m    607\u001b[0m \u001b[38;5;66;03m# Get config dict associated with the base config file\u001b[39;00m\n\u001b[0;32m--> 608\u001b[0m config_dict, kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_config_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    609\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m config_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    610\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m {}, kwargs\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/configuration_utils.py:667\u001b[0m, in \u001b[0;36mPretrainedConfig._get_config_dict\u001b[0;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n\u001b[1;32m    663\u001b[0m configuration_file \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_configuration_file\u001b[39m\u001b[38;5;124m\"\u001b[39m, CONFIG_NAME) \u001b[38;5;28;01mif\u001b[39;00m gguf_file \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m gguf_file\n\u001b[1;32m    665\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    666\u001b[0m     \u001b[38;5;66;03m# Load from local folder or from cache or download from model Hub and cache\u001b[39;00m\n\u001b[0;32m--> 667\u001b[0m     resolved_config_file \u001b[38;5;241m=\u001b[39m \u001b[43mcached_file\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    668\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    669\u001b[0m \u001b[43m        \u001b[49m\u001b[43mconfiguration_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    670\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    671\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    672\u001b[0m \u001b[43m        \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    673\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    674\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    675\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    676\u001b[0m \u001b[43m        \u001b[49m\u001b[43muser_agent\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muser_agent\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    677\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    678\u001b[0m \u001b[43m        \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msubfolder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    679\u001b[0m \u001b[43m        \u001b[49m\u001b[43m_commit_hash\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcommit_hash\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    680\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    681\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m resolved_config_file \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    682\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, kwargs\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/utils/hub.py:312\u001b[0m, in \u001b[0;36mcached_file\u001b[0;34m(path_or_repo_id, filename, **kwargs)\u001b[0m\n\u001b[1;32m    254\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mcached_file\u001b[39m(\n\u001b[1;32m    255\u001b[0m     path_or_repo_id: Union[\u001b[38;5;28mstr\u001b[39m, os\u001b[38;5;241m.\u001b[39mPathLike],\n\u001b[1;32m    256\u001b[0m     filename: \u001b[38;5;28mstr\u001b[39m,\n\u001b[1;32m    257\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m    258\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Optional[\u001b[38;5;28mstr\u001b[39m]:\n\u001b[1;32m    259\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    260\u001b[0m \u001b[38;5;124;03m    Tries to locate a file in a local folder and repo, downloads and cache it if necessary.\u001b[39;00m\n\u001b[1;32m    261\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    310\u001b[0m \u001b[38;5;124;03m    ```\u001b[39;00m\n\u001b[1;32m    311\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 312\u001b[0m     file \u001b[38;5;241m=\u001b[39m \u001b[43mcached_files\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath_or_repo_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpath_or_repo_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfilenames\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    313\u001b[0m     file \u001b[38;5;241m=\u001b[39m file[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m file\n\u001b[1;32m    314\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m file\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/utils/hub.py:543\u001b[0m, in \u001b[0;36mcached_files\u001b[0;34m(path_or_repo_id, filenames, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, subfolder, repo_type, user_agent, _raise_exceptions_for_gated_repo, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash, **deprecated_kwargs)\u001b[0m\n\u001b[1;32m    540\u001b[0m     \u001b[38;5;66;03m# Here we only raise if both flags for missing entry and connection errors are True (because it can be raised\u001b[39;00m\n\u001b[1;32m    541\u001b[0m     \u001b[38;5;66;03m# even when `local_files_only` is True, in which case raising for connections errors only would not make sense)\u001b[39;00m\n\u001b[1;32m    542\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m _raise_exceptions_for_missing_entries:\n\u001b[0;32m--> 543\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m(\n\u001b[1;32m    544\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWe couldn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt connect to \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mHUGGINGFACE_CO_RESOLVE_ENDPOINT\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m to load the files, and couldn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt find them in the\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    545\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m cached files.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mCheck your internet connection or see how to run the library in offline mode at\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    546\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhttps://huggingface.co/docs/transformers/installation#offline-mode\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    547\u001b[0m         ) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01me\u001b[39;00m\n\u001b[1;32m    548\u001b[0m \u001b[38;5;66;03m# snapshot_download will not raise EntryNotFoundError, but hf_hub_download can. If this is the case, it will be treated\u001b[39;00m\n\u001b[1;32m    549\u001b[0m \u001b[38;5;66;03m# later on anyway and re-raised if needed\u001b[39;00m\n\u001b[1;32m    550\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(e, HTTPError) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(e, EntryNotFoundError):\n",
      "\u001b[0;31mOSError\u001b[0m: We couldn't connect to 'https://huggingface.co' to load the files, and couldn't find them in the cached files.\nCheck your internet connection or see how to run the library in offline mode at 'https://huggingface.co/docs/transformers/installation#offline-mode'."
     ]
    }
   ],
   "source": [
    "clip_model = CLIPModel.from_pretrained(\n",
    "                \"openai/clip-vit-base-patch32\",\n",
    "                use_safetensors=True,\n",
    "                local_files_only=True\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "490c4b10-6b76-4a25-8817-7b2af8ec307a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "998ee170-d00b-40e2-914b-bd3250697723",
   "metadata": {},
   "source": [
    "# 2025-7-6 check and fix loading CLIPModel from local cache on personal \n",
    "#### ins-gl-pt-gpu24-dea5707-gpuprof-3env-j4-1b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "74a27585-b6ae-4bf3-945d-027be96b74cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== CLIP Cache Diagnosis ===\n",
      "\n",
      "Available snapshots: 2\n",
      "  - 3d74acf9a28c67741b2f4f2ea7635f0aaf6f0268\n",
      "    Files: 8\n",
      "      config.json: 4186\n",
      "      merges.txt: 524657\n",
      "      preprocessor_config.json: 316\n",
      "      pytorch_model.bin: 605247071\n",
      "      special_tokens_map.json: 389\n",
      "      tokenizer.json: 2224041\n",
      "      tokenizer_config.json: 592\n",
      "      vocab.json: 862328\n",
      "\n",
      "  - 37000f5cdccec47a3f6f83142bff131370757470\n",
      "    Files: 1\n",
      "      model.safetensors: 605157884\n",
      "\n",
      "refs/ directory contents:\n",
      "  main: 37000f5cdccec47a3f6f83142bff131370757470\n",
      "\n",
      "blobs/ directory: 9 files\n",
      "  182766ce89b439768edadda342519f33802f5364: 862328 bytes\n",
      "  4fdaf6842dd5a725b940c92b6b692490ce59d548: 592 bytes\n",
      "  564c0ebd5ce29c4ee4864004aee693deadd3128c: 2224041 bytes\n",
      "  5a12a1eb250987a4eee0e3e7d7338c4b22724be1: 316 bytes\n",
      "  99d28a652e6ec46629ab7047a0ac82c69b1fe11e0ce672c43af65d3a9a3fc05d: 605157884 bytes\n",
      "  ... and 4 more\n",
      "\n",
      "=== Checking files in 37000f5cdccec47a3f6f83142bff131370757470 ===\n",
      "Present files:\n",
      "  ✓ model.safetensors (605157884 bytes)\n",
      "Missing files:\n",
      "  ✗ config.json\n",
      "  ✗ tokenizer.json\n",
      "  ✗ tokenizer_config.json\n",
      "  ✗ vocab.json\n",
      "  ✗ merges.txt\n",
      "  ✗ preprocessor_config.json\n",
      "\n",
      "Attempting to fix missing files...\n",
      "\n",
      "=== Creating symlinks for 37000f5cdccec47a3f6f83142bff131370757470 ===\n",
      "Created 0 symlinks\n",
      "\n",
      "=== Checking files in 37000f5cdccec47a3f6f83142bff131370757470 ===\n",
      "Present files:\n",
      "  ✓ model.safetensors (605157884 bytes)\n",
      "Missing files:\n",
      "  ✗ config.json\n",
      "  ✗ tokenizer.json\n",
      "  ✗ tokenizer_config.json\n",
      "  ✗ vocab.json\n",
      "  ✗ merges.txt\n",
      "  ✗ preprocessor_config.json\n",
      "\n",
      "✗ Required files still missing. You may need to re-download the model.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "def diagnose_clip_cache():\n",
    "    \"\"\"Comprehensive diagnosis of CLIP cache issues\"\"\"\n",
    "    \n",
    "    cache_base = \"/home/jupyter/.cache/huggingface/hub/models--openai--clip-vit-base-patch32\"\n",
    "    \n",
    "    print(\"=== CLIP Cache Diagnosis ===\\n\")\n",
    "    \n",
    "    # Check snapshots\n",
    "    snapshots_dir = Path(cache_base) / \"snapshots\"\n",
    "    snapshots = [d for d in snapshots_dir.iterdir() if d.is_dir()]\n",
    "    \n",
    "    print(f\"Available snapshots: {len(snapshots)}\")\n",
    "    for snapshot in snapshots:\n",
    "        print(f\"  - {snapshot.name}\")\n",
    "        files = list(snapshot.iterdir())\n",
    "        print(f\"    Files: {len(files)}\")\n",
    "        for file in sorted(files):\n",
    "            size = file.stat().st_size if file.is_file() else \"DIR\"\n",
    "            print(f\"      {file.name}: {size}\")\n",
    "        print()\n",
    "    \n",
    "    # Check refs\n",
    "    refs_dir = Path(cache_base) / \"refs\"\n",
    "    if refs_dir.exists():\n",
    "        print(\"refs/ directory contents:\")\n",
    "        for ref_file in refs_dir.iterdir():\n",
    "            if ref_file.is_file():\n",
    "                content = ref_file.read_text().strip()\n",
    "                print(f\"  {ref_file.name}: {content}\")\n",
    "    else:\n",
    "        print(\"refs/ directory not found\")\n",
    "    \n",
    "    # Check blobs\n",
    "    blobs_dir = Path(cache_base) / \"blobs\"\n",
    "    if blobs_dir.exists():\n",
    "        blobs = list(blobs_dir.iterdir())\n",
    "        print(f\"\\nblobs/ directory: {len(blobs)} files\")\n",
    "        for blob in sorted(blobs)[:5]:  # Show first 5\n",
    "            size = blob.stat().st_size\n",
    "            print(f\"  {blob.name}: {size} bytes\")\n",
    "        if len(blobs) > 5:\n",
    "            print(f\"  ... and {len(blobs) - 5} more\")\n",
    "    else:\n",
    "        print(\"blobs/ directory not found\")\n",
    "    \n",
    "    return snapshots\n",
    "\n",
    "def check_required_files(snapshot_path):\n",
    "    \"\"\"Check if all required files are present in snapshot\"\"\"\n",
    "    snapshot = Path(snapshot_path)\n",
    "    \n",
    "    # Essential files for CLIP model\n",
    "    required_files = [\n",
    "        \"config.json\",\n",
    "        \"tokenizer.json\", \n",
    "        \"tokenizer_config.json\",\n",
    "        \"vocab.json\",\n",
    "        \"merges.txt\",\n",
    "        \"preprocessor_config.json\"\n",
    "    ]\n",
    "    \n",
    "    # Model weight files (at least one should exist)\n",
    "    model_files = [\n",
    "        \"model.safetensors\",\n",
    "        \"pytorch_model.bin\"\n",
    "    ]\n",
    "    \n",
    "    print(f\"\\n=== Checking files in {snapshot.name} ===\")\n",
    "    \n",
    "    missing_files = []\n",
    "    present_files = []\n",
    "    \n",
    "    for file in required_files:\n",
    "        file_path = snapshot / file\n",
    "        if file_path.exists():\n",
    "            size = file_path.stat().st_size\n",
    "            present_files.append(f\"{file} ({size} bytes)\")\n",
    "        else:\n",
    "            missing_files.append(file)\n",
    "    \n",
    "    # Check model files\n",
    "    model_file_found = False\n",
    "    for model_file in model_files:\n",
    "        file_path = snapshot / model_file\n",
    "        if file_path.exists():\n",
    "            size = file_path.stat().st_size\n",
    "            present_files.append(f\"{model_file} ({size} bytes)\")\n",
    "            model_file_found = True\n",
    "            break\n",
    "    \n",
    "    if not model_file_found:\n",
    "        missing_files.extend(model_files)\n",
    "    \n",
    "    print(\"Present files:\")\n",
    "    for file in present_files:\n",
    "        print(f\"  ✓ {file}\")\n",
    "    \n",
    "    if missing_files:\n",
    "        print(\"Missing files:\")\n",
    "        for file in missing_files:\n",
    "            print(f\"  ✗ {file}\")\n",
    "        return False\n",
    "    else:\n",
    "        print(\"✓ All required files present\")\n",
    "        return True\n",
    "\n",
    "def create_symlinks_from_blobs(snapshot_path, cache_base):\n",
    "    \"\"\"Create missing symlinks from blobs directory\"\"\"\n",
    "    snapshot = Path(snapshot_path)\n",
    "    blobs_dir = Path(cache_base) / \"blobs\"\n",
    "    \n",
    "    if not blobs_dir.exists():\n",
    "        print(\"No blobs directory found\")\n",
    "        return False\n",
    "    \n",
    "    print(f\"\\n=== Creating symlinks for {snapshot.name} ===\")\n",
    "    \n",
    "    # Get all blob files\n",
    "    blob_files = {blob.name: blob for blob in blobs_dir.iterdir() if blob.is_file()}\n",
    "    \n",
    "    # Files that should be symlinked from blobs\n",
    "    files_to_link = [\n",
    "        \"config.json\",\n",
    "        \"tokenizer.json\",\n",
    "        \"tokenizer_config.json\", \n",
    "        \"vocab.json\",\n",
    "        \"merges.txt\",\n",
    "        \"preprocessor_config.json\",\n",
    "        \"model.safetensors\",\n",
    "        \"pytorch_model.bin\"\n",
    "    ]\n",
    "    \n",
    "    created_links = 0\n",
    "    for file_name in files_to_link:\n",
    "        target_path = snapshot / file_name\n",
    "        \n",
    "        # Skip if file already exists\n",
    "        if target_path.exists():\n",
    "            continue\n",
    "            \n",
    "        # Find matching blob\n",
    "        matching_blob = None\n",
    "        for blob_name, blob_path in blob_files.items():\n",
    "            # Try to match by checking if it's a JSON file and contains expected content\n",
    "            if file_name.endswith('.json') and blob_name.endswith('.json'):\n",
    "                try:\n",
    "                    content = blob_path.read_text()[:200]  # Read first 200 chars\n",
    "                    if file_name == \"config.json\" and '\"model_type\"' in content and '\"clip\"' in content:\n",
    "                        matching_blob = blob_path\n",
    "                        break\n",
    "                    elif file_name == \"tokenizer_config.json\" and '\"tokenizer_class\"' in content:\n",
    "                        matching_blob = blob_path\n",
    "                        break\n",
    "                    elif file_name == \"preprocessor_config.json\" and '\"image_processor_type\"' in content:\n",
    "                        matching_blob = blob_path\n",
    "                        break\n",
    "                except:\n",
    "                    continue\n",
    "            elif file_name == \"model.safetensors\" and blob_path.stat().st_size > 500_000_000:  # Large file likely model\n",
    "                matching_blob = blob_path\n",
    "                break\n",
    "        \n",
    "        if matching_blob:\n",
    "            try:\n",
    "                target_path.symlink_to(matching_blob)\n",
    "                print(f\"  ✓ Created symlink: {file_name} -> {matching_blob.name}\")\n",
    "                created_links += 1\n",
    "            except Exception as e:\n",
    "                print(f\"  ✗ Failed to create symlink for {file_name}: {e}\")\n",
    "    \n",
    "    print(f\"Created {created_links} symlinks\")\n",
    "    return created_links > 0\n",
    "\n",
    "def force_offline_load():\n",
    "    \"\"\"Set aggressive offline mode\"\"\"\n",
    "    # Set all possible offline environment variables\n",
    "    offline_vars = {\n",
    "        'HF_HUB_OFFLINE': '1',\n",
    "        'TRANSFORMERS_OFFLINE': '1',\n",
    "        'HF_DATASETS_OFFLINE': '1',\n",
    "        'HUGGINGFACE_HUB_CACHE': '/home/jupyter/.cache/huggingface/hub',\n",
    "        'HF_HOME': '/home/jupyter/.cache/huggingface'\n",
    "    }\n",
    "    \n",
    "    for var, value in offline_vars.items():\n",
    "        os.environ[var] = value\n",
    "        print(f\"Set {var}={value}\")\n",
    "\n",
    "def main():\n",
    "    cache_base = \"/home/jupyter/.cache/huggingface/hub/models--openai--clip-vit-base-patch32\"\n",
    "    target_snapshot = \"37000f5cdccec47a3f6f83142bff131370757470\"\n",
    "    \n",
    "    # Step 1: Diagnose current state\n",
    "    snapshots = diagnose_clip_cache()\n",
    "    \n",
    "    # Step 2: Check the target snapshot\n",
    "    target_path = Path(cache_base) / \"snapshots\" / target_snapshot\n",
    "    if not target_path.exists():\n",
    "        print(f\"\\n✗ Target snapshot {target_snapshot} not found!\")\n",
    "        return\n",
    "    \n",
    "    files_ok = check_required_files(target_path)\n",
    "    \n",
    "    # Step 3: Try to fix missing files\n",
    "    if not files_ok:\n",
    "        print(\"\\nAttempting to fix missing files...\")\n",
    "        create_symlinks_from_blobs(target_path, cache_base)\n",
    "        \n",
    "        # Recheck\n",
    "        files_ok = check_required_files(target_path)\n",
    "    \n",
    "    # Step 4: Set offline mode and test\n",
    "    if files_ok:\n",
    "        print(\"\\n=== Testing model load ===\")\n",
    "        force_offline_load()\n",
    "        \n",
    "        try:\n",
    "            from transformers import CLIPModel\n",
    "            model = CLIPModel.from_pretrained(\n",
    "                str(target_path),\n",
    "                use_safetensors=True,\n",
    "                local_files_only=True\n",
    "            )\n",
    "            print(\"✓ Successfully loaded model with direct path\")\n",
    "            \n",
    "            # Now try with model identifier\n",
    "            model2 = CLIPModel.from_pretrained(\n",
    "                \"openai/clip-vit-base-patch32\",\n",
    "                use_safetensors=True,\n",
    "                local_files_only=True\n",
    "            )\n",
    "            print(\"✓ Successfully loaded model with identifier\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"✗ Model loading failed: {e}\")\n",
    "    else:\n",
    "        print(\"\\n✗ Required files still missing. You may need to re-download the model.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "2a370605-52dc-4880-924e-a916877d6a69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 581M\n",
      "-rw-r--r-- 1 root root 4.1K Jul  6 07:02 config.json\n",
      "-rw-r--r-- 1 root root 513K Jul  6 07:02 merges.txt\n",
      "-rw-r--r-- 1 root root  316 Jul  6 07:02 preprocessor_config.json\n",
      "-rw-r--r-- 1 root root 578M Jul  6 07:03 pytorch_model.bin\n",
      "-rw-r--r-- 1 root root  389 Jul  6 07:02 special_tokens_map.json\n",
      "-rw-r--r-- 1 root root 2.2M Jul  6 07:02 tokenizer.json\n",
      "-rw-r--r-- 1 root root  592 Jul  6 07:02 tokenizer_config.json\n",
      "-rw-r--r-- 1 root root 843K Jul  6 07:02 vocab.json\n"
     ]
    }
   ],
   "source": [
    "!ls -lh ~/.cache/huggingface/hub/models--openai--clip-vit-base-patch32/snapshots/3d74acf9a28c67741b2f4f2ea7635f0aaf6f0268"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9e52f51-6c18-4089-86a1-41eeda677877",
   "metadata": {},
   "source": [
    "## copy missing files \n",
    "#### from 3d74acf9a28c67741b2f4f2ea7635f0aaf6f0268\n",
    "#### to 37000f5cdccec47a3f6f83142bff131370757470"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "e23839f5-ae57-4ad9-b293-9d8abde46532",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Copying Missing Configuration Files ===\n",
      "✓ Copied: config.json\n",
      "✓ Copied: tokenizer.json\n",
      "✓ Copied: tokenizer_config.json\n",
      "✓ Copied: vocab.json\n",
      "✓ Copied: merges.txt\n",
      "✓ Copied: preprocessor_config.json\n",
      "✓ Copied: special_tokens_map.json\n",
      "\n",
      "=== Summary ===\n",
      "Successfully copied: 7 files\n",
      "  ✓ config.json\n",
      "  ✓ tokenizer.json\n",
      "  ✓ tokenizer_config.json\n",
      "  ✓ vocab.json\n",
      "  ✓ merges.txt\n",
      "  ✓ preprocessor_config.json\n",
      "  ✓ special_tokens_map.json\n",
      "\n",
      "=== Target Snapshot Contents ===\n",
      "Total files in target: 8\n",
      "  config.json: 4,186 bytes\n",
      "  merges.txt: 524,657 bytes\n",
      "  model.safetensors: 605,157,884 bytes\n",
      "  preprocessor_config.json: 316 bytes\n",
      "  special_tokens_map.json: 389 bytes\n",
      "  tokenizer.json: 2,224,041 bytes\n",
      "  tokenizer_config.json: 592 bytes\n",
      "  vocab.json: 862,328 bytes\n",
      "\n",
      "==================================================\n",
      "\n",
      "=== Testing CLIP Model Loading ===\n",
      "Loading model with identifier...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading processor...\n",
      "✓ Successfully loaded CLIP model and processor!\n",
      "Model type: <class 'transformers.models.clip.modeling_clip.CLIPModel'>\n",
      "Processor type: <class 'transformers.models.clip.processing_clip.CLIPProcessor'>\n",
      "\n",
      "🎉 CLIP model is now ready to use!\n",
      "=== Copying Missing Configuration Files ===\n",
      "ℹ️  File already exists in target: config.json\n",
      "ℹ️  File already exists in target: tokenizer.json\n",
      "ℹ️  File already exists in target: tokenizer_config.json\n",
      "ℹ️  File already exists in target: vocab.json\n",
      "ℹ️  File already exists in target: merges.txt\n",
      "ℹ️  File already exists in target: preprocessor_config.json\n",
      "ℹ️  File already exists in target: special_tokens_map.json\n",
      "\n",
      "=== Summary ===\n",
      "Successfully copied: 0 files\n",
      "\n",
      "=== Target Snapshot Contents ===\n",
      "Total files in target: 8\n",
      "  config.json: 4,186 bytes\n",
      "  merges.txt: 524,657 bytes\n",
      "  model.safetensors: 605,157,884 bytes\n",
      "  preprocessor_config.json: 316 bytes\n",
      "  special_tokens_map.json: 389 bytes\n",
      "  tokenizer.json: 2,224,041 bytes\n",
      "  tokenizer_config.json: 592 bytes\n",
      "  vocab.json: 862,328 bytes\n",
      "\n",
      "=== Testing CLIP Model Loading ===\n",
      "Loading model with identifier...\n",
      "Loading processor...\n",
      "✓ Successfully loaded CLIP model and processor!\n",
      "Model type: <class 'transformers.models.clip.modeling_clip.CLIPModel'>\n",
      "Processor type: <class 'transformers.models.clip.processing_clip.CLIPProcessor'>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(True,\n",
       " CLIPModel(\n",
       "   (text_model): CLIPTextTransformer(\n",
       "     (embeddings): CLIPTextEmbeddings(\n",
       "       (token_embedding): Embedding(49408, 512)\n",
       "       (position_embedding): Embedding(77, 512)\n",
       "     )\n",
       "     (encoder): CLIPEncoder(\n",
       "       (layers): ModuleList(\n",
       "         (0-11): 12 x CLIPEncoderLayer(\n",
       "           (self_attn): CLIPAttention(\n",
       "             (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "             (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "             (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "             (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "           )\n",
       "           (layer_norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "           (mlp): CLIPMLP(\n",
       "             (activation_fn): QuickGELUActivation()\n",
       "             (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "             (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "           )\n",
       "           (layer_norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "         )\n",
       "       )\n",
       "     )\n",
       "     (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "   )\n",
       "   (vision_model): CLIPVisionTransformer(\n",
       "     (embeddings): CLIPVisionEmbeddings(\n",
       "       (patch_embedding): Conv2d(3, 768, kernel_size=(32, 32), stride=(32, 32), bias=False)\n",
       "       (position_embedding): Embedding(50, 768)\n",
       "     )\n",
       "     (pre_layrnorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "     (encoder): CLIPEncoder(\n",
       "       (layers): ModuleList(\n",
       "         (0-11): 12 x CLIPEncoderLayer(\n",
       "           (self_attn): CLIPAttention(\n",
       "             (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "             (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "             (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "             (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "           )\n",
       "           (layer_norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "           (mlp): CLIPMLP(\n",
       "             (activation_fn): QuickGELUActivation()\n",
       "             (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "             (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "           )\n",
       "           (layer_norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "         )\n",
       "       )\n",
       "     )\n",
       "     (post_layernorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "   )\n",
       "   (visual_projection): Linear(in_features=768, out_features=512, bias=False)\n",
       "   (text_projection): Linear(in_features=512, out_features=512, bias=False)\n",
       " ),\n",
       " CLIPProcessor:\n",
       " - image_processor: CLIPImageProcessor {\n",
       "   \"crop_size\": {\n",
       "     \"height\": 224,\n",
       "     \"width\": 224\n",
       "   },\n",
       "   \"do_center_crop\": true,\n",
       "   \"do_convert_rgb\": true,\n",
       "   \"do_normalize\": true,\n",
       "   \"do_rescale\": true,\n",
       "   \"do_resize\": true,\n",
       "   \"image_mean\": [\n",
       "     0.48145466,\n",
       "     0.4578275,\n",
       "     0.40821073\n",
       "   ],\n",
       "   \"image_processor_type\": \"CLIPImageProcessor\",\n",
       "   \"image_std\": [\n",
       "     0.26862954,\n",
       "     0.26130258,\n",
       "     0.27577711\n",
       "   ],\n",
       "   \"resample\": 3,\n",
       "   \"rescale_factor\": 0.00392156862745098,\n",
       "   \"size\": {\n",
       "     \"shortest_edge\": 224\n",
       "   }\n",
       " }\n",
       " \n",
       " - tokenizer: CLIPTokenizerFast(name_or_path='openai/clip-vit-base-patch32', vocab_size=49408, model_max_length=77, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'bos_token': '<|startoftext|>', 'eos_token': '<|endoftext|>', 'unk_token': '<|endoftext|>', 'pad_token': '<|endoftext|>'}, clean_up_tokenization_spaces=False, added_tokens_decoder={\n",
       " \t49406: AddedToken(\"<|startoftext|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n",
       " \t49407: AddedToken(\"<|endoftext|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       " }\n",
       " )\n",
       " \n",
       " {\n",
       "   \"processor_class\": \"CLIPProcessor\"\n",
       " })"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import shutil\n",
    "from pathlib import Path\n",
    "\n",
    "def fix_clip_cache():\n",
    "    \"\"\"Copy missing configuration files from complete snapshot to safetensors snapshot\"\"\"\n",
    "    \n",
    "    # Define paths\n",
    "    source_snapshot = \"/home/jupyter/.cache/huggingface/hub/models--openai--clip-vit-base-patch32/snapshots/3d74acf9a28c67741b2f4f2ea7635f0aaf6f0268\"\n",
    "    target_snapshot = \"/home/jupyter/.cache/huggingface/hub/models--openai--clip-vit-base-patch32/snapshots/37000f5cdccec47a3f6f83142bff131370757470\"\n",
    "    \n",
    "    source_path = Path(source_snapshot)\n",
    "    target_path = Path(target_snapshot)\n",
    "    \n",
    "    # List of configuration files to copy\n",
    "    config_files = [\n",
    "        \"config.json\",\n",
    "        \"tokenizer.json\", \n",
    "        \"tokenizer_config.json\",\n",
    "        \"vocab.json\",\n",
    "        \"merges.txt\",\n",
    "        \"preprocessor_config.json\",\n",
    "        \"special_tokens_map.json\"\n",
    "    ]\n",
    "    \n",
    "    print(\"=== Copying Missing Configuration Files ===\")\n",
    "    \n",
    "    copied_files = []\n",
    "    failed_files = []\n",
    "    \n",
    "    for file_name in config_files:\n",
    "        source_file = source_path / file_name\n",
    "        target_file = target_path / file_name\n",
    "        \n",
    "        # Check if source file exists\n",
    "        if not source_file.exists():\n",
    "            print(f\"⚠️  Source file not found: {file_name}\")\n",
    "            continue\n",
    "            \n",
    "        # Check if target file already exists\n",
    "        if target_file.exists():\n",
    "            print(f\"ℹ️  File already exists in target: {file_name}\")\n",
    "            continue\n",
    "        \n",
    "        try:\n",
    "            # Copy the file\n",
    "            shutil.copy2(source_file, target_file)\n",
    "            copied_files.append(file_name)\n",
    "            print(f\"✓ Copied: {file_name}\")\n",
    "        except Exception as e:\n",
    "            failed_files.append(file_name)\n",
    "            print(f\"✗ Failed to copy {file_name}: {e}\")\n",
    "    \n",
    "    print(f\"\\n=== Summary ===\")\n",
    "    print(f\"Successfully copied: {len(copied_files)} files\")\n",
    "    if copied_files:\n",
    "        for file in copied_files:\n",
    "            print(f\"  ✓ {file}\")\n",
    "    \n",
    "    if failed_files:\n",
    "        print(f\"Failed to copy: {len(failed_files)} files\")\n",
    "        for file in failed_files:\n",
    "            print(f\"  ✗ {file}\")\n",
    "    \n",
    "    # Verify target snapshot contents\n",
    "    print(f\"\\n=== Target Snapshot Contents ===\")\n",
    "    target_files = list(target_path.iterdir())\n",
    "    print(f\"Total files in target: {len(target_files)}\")\n",
    "    \n",
    "    for file in sorted(target_files):\n",
    "        if file.is_file():\n",
    "            size = file.stat().st_size\n",
    "            print(f\"  {file.name}: {size:,} bytes\")\n",
    "    \n",
    "    return len(copied_files) > 0\n",
    "\n",
    "def test_clip_loading():\n",
    "    \"\"\"Test CLIP model loading after fixing cache\"\"\"\n",
    "    print(\"\\n=== Testing CLIP Model Loading ===\")\n",
    "    \n",
    "    # Set offline mode\n",
    "    os.environ['HF_HUB_OFFLINE'] = '1'\n",
    "    os.environ['TRANSFORMERS_OFFLINE'] = '1'\n",
    "    \n",
    "    try:\n",
    "        from transformers import CLIPModel, CLIPProcessor\n",
    "        \n",
    "        # Test with model identifier (should work now)\n",
    "        print(\"Loading model with identifier...\")\n",
    "        clip_model = CLIPModel.from_pretrained(\n",
    "            \"openai/clip-vit-base-patch32\",\n",
    "            use_safetensors=True,\n",
    "            local_files_only=True\n",
    "        )\n",
    "        \n",
    "        print(\"Loading processor...\")\n",
    "        clip_processor = CLIPProcessor.from_pretrained(\n",
    "            \"openai/clip-vit-base-patch32\", \n",
    "            local_files_only=True\n",
    "        )\n",
    "        \n",
    "        print(\"✓ Successfully loaded CLIP model and processor!\")\n",
    "        print(f\"Model type: {type(clip_model)}\")\n",
    "        print(f\"Processor type: {type(clip_processor)}\")\n",
    "        \n",
    "        return True, clip_model, clip_processor\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"✗ Model loading failed: {e}\")\n",
    "        return False, None, None\n",
    "\n",
    "# Main execution\n",
    "if __name__ == \"__main__\":\n",
    "    # Step 1: Fix the cache\n",
    "    success = fix_clip_cache()\n",
    "    \n",
    "    if success:\n",
    "        print(\"\\n\" + \"=\"*50)\n",
    "        # Step 2: Test loading\n",
    "        load_success, model, processor = test_clip_loading()\n",
    "        \n",
    "        if load_success:\n",
    "            print(\"\\n🎉 CLIP model is now ready to use!\")\n",
    "        else:\n",
    "            print(\"\\n❌ Model loading still failed. You may need to re-download.\")\n",
    "    else:\n",
    "        print(\"\\n⚠️  No files were copied. Check if paths are correct.\")\n",
    "\n",
    "# Run the fix\n",
    "fix_clip_cache()\n",
    "test_clip_loading()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "e33bf52f-abb7-4651-a392-27c6e1dfd71d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import CLIPModel, CLIPProcessor\n",
    "\n",
    "# This should now work without network calls\n",
    "model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\", use_safetensors=True, local_files_only=True)\n",
    "processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\", local_files_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "5ac5d5ea-6650-41d9-92af-d3752420d437",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 12K\n",
      "drwxr-xr-x 2 root root 4.0K Jul  6 07:03 blobs\n",
      "drwxr-xr-x 3 root root 4.0K Jul  6 07:02 refs\n",
      "drwxr-xr-x 4 root root 4.0K Jul  6 07:02 snapshots\n"
     ]
    }
   ],
   "source": [
    "!ls -lh ~/.cache/huggingface/hub/models--openai--clip-vit-base-patch32/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "a1891b5c-1c30-4823-91b5-b21ff1ead0d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 8.0K\n",
      "drwxr-xr-x 2 root root 4.0K Jul  6 17:34 37000f5cdccec47a3f6f83142bff131370757470\n",
      "drwxr-xr-x 2 root root 4.0K Jul  6 07:03 3d74acf9a28c67741b2f4f2ea7635f0aaf6f0268\n"
     ]
    }
   ],
   "source": [
    "!ls -lh ~/.cache/huggingface/hub/models--openai--clip-vit-base-patch32/snapshots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "c9d8a6f6-6a18-4fd6-bf90-cd65b25161d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 581M\n",
      "-rw-r--r-- 1 root root 4.1K Jul  6 07:02 config.json\n",
      "-rw-r--r-- 1 root root 513K Jul  6 07:02 merges.txt\n",
      "-rw-r--r-- 1 root root 578M Jul  6 07:03 model.safetensors\n",
      "-rw-r--r-- 1 root root  316 Jul  6 07:02 preprocessor_config.json\n",
      "-rw-r--r-- 1 root root  389 Jul  6 07:02 special_tokens_map.json\n",
      "-rw-r--r-- 1 root root 2.2M Jul  6 07:02 tokenizer.json\n",
      "-rw-r--r-- 1 root root  592 Jul  6 07:02 tokenizer_config.json\n",
      "-rw-r--r-- 1 root root 843K Jul  6 07:02 vocab.json\n"
     ]
    }
   ],
   "source": [
    "!ls -lh ~/.cache/huggingface/hub/models--openai--clip-vit-base-patch32/snapshots/37000f5cdccec47a3f6f83142bff131370757470"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "d74101f5-d524-471b-8484-20c1405bcb6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 581M\n",
      "-rw-r--r-- 1 root root 4.1K Jul  6 07:02 config.json\n",
      "-rw-r--r-- 1 root root 513K Jul  6 07:02 merges.txt\n",
      "-rw-r--r-- 1 root root  316 Jul  6 07:02 preprocessor_config.json\n",
      "-rw-r--r-- 1 root root 578M Jul  6 07:03 pytorch_model.bin\n",
      "-rw-r--r-- 1 root root  389 Jul  6 07:02 special_tokens_map.json\n",
      "-rw-r--r-- 1 root root 2.2M Jul  6 07:02 tokenizer.json\n",
      "-rw-r--r-- 1 root root  592 Jul  6 07:02 tokenizer_config.json\n",
      "-rw-r--r-- 1 root root 843K Jul  6 07:02 vocab.json\n"
     ]
    }
   ],
   "source": [
    "!ls -lh ~/.cache/huggingface/hub/models--openai--clip-vit-base-patch32/snapshots/3d74acf9a28c67741b2f4f2ea7635f0aaf6f0268"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "55f8f71e-832d-4b0f-a518-93d1c0ed13a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 1.2G\n",
      "-rw-r--r-- 1 root root 843K Jul  6 07:02 182766ce89b439768edadda342519f33802f5364\n",
      "-rw-r--r-- 1 root root  592 Jul  6 07:02 4fdaf6842dd5a725b940c92b6b692490ce59d548\n",
      "-rw-r--r-- 1 root root 2.2M Jul  6 07:02 564c0ebd5ce29c4ee4864004aee693deadd3128c\n",
      "-rw-r--r-- 1 root root  316 Jul  6 07:02 5a12a1eb250987a4eee0e3e7d7338c4b22724be1\n",
      "-rw-r--r-- 1 root root 578M Jul  6 07:03 99d28a652e6ec46629ab7047a0ac82c69b1fe11e0ce672c43af65d3a9a3fc05d\n",
      "-rw-r--r-- 1 root root  389 Jul  6 07:02 9bfb42aa97dcd61e89f279ccaee988bccb4fabae\n",
      "-rw-r--r-- 1 root root 4.1K Jul  6 07:02 a2a88b96561196777ca173b15309ea859f4d2ce0\n",
      "-rw-r--r-- 1 root root 578M Jul  6 07:03 a63082132ba4f97a80bea76823f544493bffa8082296d62d71581a4feff1576f\n",
      "-rw-r--r-- 1 root root 513K Jul  6 07:02 bbfec752c9a675946c6dce106def6f35c882dcc2\n"
     ]
    }
   ],
   "source": [
    "!ls -lh ~/.cache/huggingface/hub/models--openai--clip-vit-base-patch32/blobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "c45f9702-6378-4845-a7ac-52d46e8160b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 8.0K\n",
      "-rw-r--r-- 1 root root   40 Jul  6 17:17 main\n",
      "drwxr-xr-x 3 root root 4.0K Jul  6 07:02 refs\n"
     ]
    }
   ],
   "source": [
    "!ls -lh ~/.cache/huggingface/hub/models--openai--clip-vit-base-patch32/refs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "37db577d-42c8-45da-8a92-8e6801e8c51f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-rw-r--r-- 1 root root 40 Jul  6 17:17 /home/jupyter/.cache/huggingface/hub/models--openai--clip-vit-base-patch32/refs/main\n"
     ]
    }
   ],
   "source": [
    "!ls -lh ~/.cache/huggingface/hub/models--openai--clip-vit-base-patch32/refs/main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "8460f37a-8b53-4325-9a72-a888a9af5f83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "37000f5cdccec47a3f6f83142bff131370757470"
     ]
    }
   ],
   "source": [
    "!cat ~/.cache/huggingface/hub/models--openai--clip-vit-base-patch32/refs/main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "38f557f1-6750-47f9-8f12-d0cdb268d173",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 4.0K\n",
      "drwxr-xr-x 2 root root 4.0K Jul  6 07:02 pr\n"
     ]
    }
   ],
   "source": [
    "!ls -lh ~/.cache/huggingface/hub/models--openai--clip-vit-base-patch32/refs/refs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "ad962317-8d45-4c1b-b6f0-642d87c4ecb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 12K\n",
      "drwxr-xr-x 2 root root 4.0K Jul  6 07:02 .\n",
      "drwxr-xr-x 3 root root 4.0K Jul  6 07:02 ..\n",
      "-rw-r--r-- 1 root root   40 Jul  6 07:02 62\n"
     ]
    }
   ],
   "source": [
    "!ls -lha ~/.cache/huggingface/hub/models--openai--clip-vit-base-patch32/refs/refs/pr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "f5aaab24-e287-4549-88d7-d40c819b1e59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "37000f5cdccec47a3f6f83142bff131370757470"
     ]
    }
   ],
   "source": [
    "!cat ~/.cache/huggingface/hub/models--openai--clip-vit-base-patch32/refs/refs/pr/62"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "764cb9c8-7a42-459b-b20f-5064ecd5243c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel) (Local)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
